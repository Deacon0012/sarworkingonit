{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6128dc83",
   "metadata": {},
   "source": [
    "\n",
    "SAR Image Colorization â€” Pix2Pix / U-Net (PyTorch)\n",
    "This is a Jupyter-friendly .py notebook (cells separated by `# %%`).\n",
    "It implements a paired image-to-image translation training pipeline for\n",
    "SAR -> RGB colorization using Lab supervision (predict a/b channels).\n",
    "Features:\n",
    "- PyTorch implementation (generator = U-Net, discriminator = PatchGAN)\n",
    "- Paired dataset loader for SAR (grayscale) and RGB images\n",
    "- Optional SAR contrast stretch (percentile clipping + dB conversion)\n",
    "- Data augmentations (paired random crop, flips, rotations)\n",
    "- Losses: LSGAN adversarial loss + L1 on a,b channels\n",
    "- AMP (automatic mixed precision), checkpointing, TensorBoard logging\n",
    "- Inference utilities to save predicted RGB outputs\n",
    "\n",
    "Edit configuration at the CONFIG cell and run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b570a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0ddaa08",
   "metadata": {},
   "source": [
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00029c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the root dataset folder. The dataset should contain pairs like:\n",
    "# <dataset_root>/<terrain>/s1/<name>.png  (SAR grayscale)\n",
    "# <dataset_root>/<terrain>/s2/<name>.png  (RGB)\n",
    "DATA_ROOT = Path(r\"D:\\My Disk\\coding\\v_2\")  # change to your dataset path\n",
    "\n",
    "# Training hyperparameters\n",
    "DEVICE = 'cuda' if (os.environ.get('CUDA_VISIBLE_DEVICES') or True) and __import__('torch').cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 8\n",
    "PATCH_SIZE = 256\n",
    "NUM_EPOCHS = 50\n",
    "LR = 2e-4\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.999\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LAMBDA_L1 = 100.0\n",
    "CHECKPOINT_DIR = Path('./checkpoints')\n",
    "LOG_DIR = Path('./logs')\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Data splitting\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "# SAR preprocessing options\n",
    "USE_SAR_DB = True\n",
    "SAR_PERCENTILE_CLIP = True\n",
    "CLIP_LO = 2.0\n",
    "CLIP_HI = 98.0\n",
    "\n",
    "# Misc\n",
    "SAVE_SAMPLE_EVERY = 1  # epochs\n",
    "AMP_ENABLED = True\n",
    "PRINT_EVERY_BATCHES = 50\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349e5702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bbbfed",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17924946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.amp import GradScaler\n",
    "from skimage import color\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df361b6",
   "metadata": {},
   "source": [
    "Utilities: Lab conversions & SAR preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd430a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "\n",
    "def rgb_to_lab_tensor(rgb_uint8: np.ndarray):\n",
    "    \"\"\"rgb_uint8: HxWx3 uint8 image (0-255) -> returns L (0-100), a (-128,127), b (-128,127) as float32 arrays\"\"\"\n",
    "    rgb = rgb_uint8.astype('float32') / 255.0\n",
    "    lab = color.rgb2lab(rgb)\n",
    "    return lab.astype('float32')\n",
    "\n",
    "\n",
    "def lab_to_rgb_uint8(lab: np.ndarray):\n",
    "    \"\"\"lab: HxWx3 float32 (L 0-100, a,b around -128..127) -> uint8 RGB 0-255\"\"\"\n",
    "    rgb = color.lab2rgb(lab)\n",
    "    rgb = np.clip(rgb * 255.0, 0, 255).astype('uint8')\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def sar_preprocess(img_pil: Image.Image, use_db=USE_SAR_DB, do_clip=SAR_PERCENTILE_CLIP, lo_pct=CLIP_LO, hi_pct=CLIP_HI):\n",
    "    \"\"\"Take a PIL grayscale SAR image -> return normalized L channel in range [0,1]\n",
    "    Steps:\n",
    "      - convert to float32\n",
    "      - scale to [0,1]\n",
    "      - optionally apply dB: 20*log10(img + eps)\n",
    "      - optionally percentile clip and rescale\n",
    "    \"\"\"\n",
    "    arr = np.array(img_pil).astype('float32')\n",
    "    # If image is 3-ch grayscale repeated, reduce\n",
    "    if arr.ndim == 3:\n",
    "        arr = arr[..., 0]\n",
    "    # normalize naive 0-255 -> 0-1\n",
    "    arr = arr / 255.0\n",
    "    if use_db:\n",
    "        # Convert amplitude-like to dB\n",
    "        arr = 20.0 * np.log10(np.clip(arr, EPS, None))\n",
    "    if do_clip:\n",
    "        lo = np.percentile(arr, lo_pct)\n",
    "        hi = np.percentile(arr, hi_pct)\n",
    "        arr = np.clip(arr, lo, hi)\n",
    "        if hi - lo > 0:\n",
    "            arr = (arr - lo) / (hi - lo)\n",
    "        else:\n",
    "            arr = np.clip(arr, 0, 1)\n",
    "    # final clamp\n",
    "    arr = np.clip(arr, 0.0, 1.0)\n",
    "    return arr.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff44a7",
   "metadata": {},
   "source": [
    "Paired Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "626b9d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedSARDataset(Dataset):\n",
    "    def __init__(self, root: Path, terrains=None, split='train', patch_size=PATCH_SIZE,\n",
    "                 use_db=USE_SAR_DB, clip_sar=SAR_PERCENTILE_CLIP, augment=True):\n",
    "        \"\"\"\n",
    "        root: dataset root\n",
    "        terrains: list of terrain subfolders to include (None => all)\n",
    "        split: 'train'|'val'|'test'\n",
    "        \"\"\"\n",
    "        self.root = Path(root)\n",
    "        all_pairs = []\n",
    "        terrains = terrains or [p.name for p in self.root.iterdir() if p.is_dir()]\n",
    "        for t in terrains:\n",
    "            s1_dir = self.root / t / 's1'\n",
    "            s2_dir = self.root / t / 's2'\n",
    "            if not s1_dir.exists() or not s2_dir.exists():\n",
    "                continue\n",
    "            # find image names common to both\n",
    "            s1_files = sorted([p for p in s1_dir.glob('*.png')])\n",
    "            for p in s1_files:\n",
    "                name = p.name\n",
    "                s2p = s2_dir / name.replace('_s1_', '_s2_') if '_s1_' in name else (s2_dir / name)\n",
    "                # fallback: same basename\n",
    "                if not s2p.exists():\n",
    "                    s2p = s2_dir / name\n",
    "                if s2p.exists():\n",
    "                    all_pairs.append((str(p), str(s2p)))\n",
    "        # shuffle and split\n",
    "        random.shuffle(all_pairs)\n",
    "        n = len(all_pairs)\n",
    "        ntest = int(n * TEST_SPLIT)\n",
    "        nval = int(n * VAL_SPLIT)\n",
    "        if split == 'test':\n",
    "            self.pairs = all_pairs[:ntest]\n",
    "        elif split == 'val':\n",
    "            self.pairs = all_pairs[ntest:ntest + nval]\n",
    "        else:\n",
    "            self.pairs = all_pairs[ntest + nval:]\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.augment = augment\n",
    "        self.use_db = use_db\n",
    "        self.clip_sar = clip_sar\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s1_path, s2_path = self.pairs[idx]\n",
    "        s1_pil = Image.open(s1_path).convert('L')  # SAR grayscale\n",
    "        s2_pil = Image.open(s2_path).convert('RGB')  # RGB\n",
    "\n",
    "        # optionally random crop to patch size\n",
    "        if self.augment:\n",
    "            w, h = s1_pil.size\n",
    "            if w >= self.patch_size and h >= self.patch_size:\n",
    "                x = random.randint(0, w - self.patch_size)\n",
    "                y = random.randint(0, h - self.patch_size)\n",
    "                s1_pil = s1_pil.crop((x, y, x + self.patch_size, y + self.patch_size))\n",
    "                s2_pil = s2_pil.crop((x, y, x + self.patch_size, y + self.patch_size))\n",
    "            else:\n",
    "                # center crop or resize to patch size\n",
    "                s1_pil = s1_pil.resize((self.patch_size, self.patch_size), Image.BILINEAR)\n",
    "                s2_pil = s2_pil.resize((self.patch_size, self.patch_size), Image.BILINEAR)\n",
    "\n",
    "            # random flips and 90-degree rotations\n",
    "            if random.random() > 0.5:\n",
    "                s1_pil = s1_pil.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                s2_pil = s2_pil.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            if random.random() > 0.5:\n",
    "                s1_pil = s1_pil.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                s2_pil = s2_pil.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            if random.random() > 0.5:\n",
    "                s1_pil = s1_pil.transpose(Image.ROTATE_90)\n",
    "                s2_pil = s2_pil.transpose(Image.ROTATE_90)\n",
    "\n",
    "        else:\n",
    "            s1_pil = s1_pil.resize((self.patch_size, self.patch_size), Image.BILINEAR)\n",
    "            s2_pil = s2_pil.resize((self.patch_size, self.patch_size), Image.BILINEAR)\n",
    "\n",
    "        # Preprocess SAR -> L channel normalized to [0,1]\n",
    "        L = sar_preprocess(s1_pil, use_db=self.use_db, do_clip=self.clip_sar)\n",
    "        # RGB -> Lab\n",
    "        rgb = np.array(s2_pil)\n",
    "        lab = rgb_to_lab_tensor(rgb)\n",
    "        # L_ref from RGB may not match SAR L; we will use SAR-derived L as the luminance guide.\n",
    "        # Extract a,b channels and scale to [-1,1]\n",
    "        ab = lab[..., 1:3]\n",
    "        ab = (ab.astype('float32')) / 128.0  # approx scale to [-1,1]\n",
    "\n",
    "        # Convert to tensors\n",
    "        # SAR L (1,H,W)\n",
    "        L_t = torch.from_numpy(L).unsqueeze(0)\n",
    "        # ab (2,H,W)\n",
    "        ab_t = torch.from_numpy(ab).permute(2, 0, 1)\n",
    "        # For visualization/metrics we also return the reference rgb and lab L\n",
    "        rgb_t = torch.from_numpy(rgb.astype('float32') / 255.0).permute(2, 0, 1)\n",
    "        labL_ref = torch.from_numpy(lab[..., :1].astype('float32') / 100.0).permute(2, 0, 1)\n",
    "\n",
    "        return {\n",
    "            'sar_L': L_t,       # [1,H,W], normalized [0,1]\n",
    "            'ab': ab_t,         # [2,H,W], approx [-1,1]\n",
    "            'rgb': rgb_t,       # [3,H,W], [0,1]\n",
    "            'labL_ref': labL_ref  # [1,H,W], [0,1]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f5770",
   "metadata": {},
   "source": [
    "Model definitions: UNet generator and PatchGAN discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7e1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small conv block\n",
    "\n",
    "def conv_block(in_ch, out_ch, norm=True):\n",
    "    layers = [nn.Conv2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "    if norm:\n",
    "        layers.append(nn.InstanceNorm2d(out_ch, affine=True))\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=2, ngf=64, dropout=True):\n",
    "        super().__init__()\n",
    "        # encoder\n",
    "        self.enc1 = nn.Sequential(nn.Conv2d(in_channels, ngf, 4, 2, 1), nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.enc2 = conv_block(ngf, ngf * 2)\n",
    "        self.enc3 = conv_block(ngf * 2, ngf * 4)\n",
    "        self.enc4 = conv_block(ngf * 4, ngf * 8)\n",
    "        self.enc5 = conv_block(ngf * 8, ngf * 8)\n",
    "        self.enc6 = conv_block(ngf * 8, ngf * 8)\n",
    "        self.enc7 = conv_block(ngf * 8, ngf * 8)\n",
    "        self.enc8 = conv_block(ngf * 8, ngf * 8, norm=False)\n",
    "\n",
    "        # decoder (transpose conv)\n",
    "        self.dec1 = nn.Sequential(nn.ConvTranspose2d(ngf * 8, ngf * 8, 4, 2, 1, bias=False),\n",
    "                                   nn.InstanceNorm2d(ngf * 8), nn.ReLU(True))\n",
    "        self.dec2 = nn.Sequential(nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "                                   nn.InstanceNorm2d(ngf * 8), nn.ReLU(True))\n",
    "        self.dec3 = nn.Sequential(nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "                                   nn.InstanceNorm2d(ngf * 8), nn.ReLU(True))\n",
    "        self.dec4 = nn.Sequential(nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "                                   nn.InstanceNorm2d(ngf * 8), nn.ReLU(True))\n",
    "        self.dec5 = nn.Sequential(nn.ConvTranspose2d(ngf * 16, ngf * 4, 4, 2, 1, bias=False),\n",
    "                                   nn.InstanceNorm2d(ngf * 4), nn.ReLU(True))\n",
    "        self.dec6 = nn.Sequential(nn.ConvTranspose2d(ngf * 8, ngf * 2, 4, 2, 1, bias=False),\n",
    "                                   nn.InstanceNorm2d(ngf * 2), nn.ReLU(True))\n",
    "        self.dec7 = nn.Sequential(nn.ConvTranspose2d(ngf * 4, ngf, 4, 2, 1, bias=False),\n",
    "                                   nn.InstanceNorm2d(ngf), nn.ReLU(True))\n",
    "        self.dec8 = nn.Sequential(nn.ConvTranspose2d(ngf * 2, out_channels, 4, 2, 1), nn.Tanh())\n",
    "\n",
    "        # dropout in deeper dec layers\n",
    "        self.dropout = nn.Dropout(0.5) if dropout else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        e5 = self.enc5(e4)\n",
    "        e6 = self.enc6(e5)\n",
    "        e7 = self.enc7(e6)\n",
    "        e8 = self.enc8(e7)\n",
    "\n",
    "        d1 = self.dec1(e8)\n",
    "        d1 = torch.cat([d1, e7], dim=1)\n",
    "        d1 = self.dropout(d1)\n",
    "        d2 = self.dec2(d1)\n",
    "        d2 = torch.cat([d2, e6], dim=1)\n",
    "        d3 = self.dec3(d2)\n",
    "        d3 = torch.cat([d3, e5], dim=1)\n",
    "        d4 = self.dec4(d3)\n",
    "        d4 = torch.cat([d4, e4], dim=1)\n",
    "        d5 = self.dec5(d4)\n",
    "        d5 = torch.cat([d5, e3], dim=1)\n",
    "        d6 = self.dec6(d5)\n",
    "        d6 = torch.cat([d6, e2], dim=1)\n",
    "        d7 = self.dec7(d6)\n",
    "        d7 = torch.cat([d7, e1], dim=1)\n",
    "        out = self.dec8(d7)\n",
    "        return out\n",
    "\n",
    "\n",
    "# PatchGAN Discriminator\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, ndf=64):\n",
    "        # in_channels: number of channels of input (L + ab) -> we will feed SAR L concatenated with predicted/real ab => 1+2 =3\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.utils.spectral_norm(nn.Conv2d(in_channels, ndf, 4, 2, 1)))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        layers.append(nn.utils.spectral_norm(nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)))\n",
    "        layers.append(nn.InstanceNorm2d(ndf * 2, affine=True))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        layers.append(nn.utils.spectral_norm(nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)))\n",
    "        layers.append(nn.InstanceNorm2d(ndf * 4, affine=True))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        layers.append(nn.utils.spectral_norm(nn.Conv2d(ndf * 4, ndf * 8, 4, 1, 1, bias=False)))\n",
    "        layers.append(nn.InstanceNorm2d(ndf * 8, affine=True))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        layers.append(nn.utils.spectral_norm(nn.Conv2d(ndf * 8, 1, 4, 1, 1)))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ae284",
   "metadata": {},
   "source": [
    "Training loop helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e50cd889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename: Path):\n",
    "    torch.save(state, str(filename))\n",
    "\n",
    "\n",
    "def load_checkpoint(filename: Path, device):\n",
    "    return torch.load(str(filename), map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7c2bd",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99b3d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataset_root: Path):\n",
    "    # dataset and loaders\n",
    "    train_ds = PairedSARDataset(dataset_root, split='train', patch_size=PATCH_SIZE, augment=True)\n",
    "    val_ds = PairedSARDataset(dataset_root, split='val', patch_size=PATCH_SIZE, augment=False)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    # models\n",
    "    netG = UNetGenerator(in_channels=1, out_channels=2).to(DEVICE)\n",
    "    netD = PatchDiscriminator(in_channels=3).to(DEVICE)\n",
    "\n",
    "    # losses\n",
    "    criterion_GAN = nn.MSELoss()  # LSGAN\n",
    "    criterion_L1 = nn.L1Loss()\n",
    "\n",
    "    # optimizers\n",
    "    optG = torch.optim.Adam(netG.parameters(), lr=LR, betas=(BETA1, BETA2), weight_decay=WEIGHT_DECAY)\n",
    "    optD = torch.optim.Adam(netD.parameters(), lr=LR, betas=(BETA1, BETA2), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    scaler = torch.amp.GradScaler(device=DEVICE, enabled=AMP_ENABLED)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=str(LOG_DIR))\n",
    "\n",
    "    best_val_l1 = float('inf')\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        netG.train(); netD.train()\n",
    "        running_loss_G = 0.0\n",
    "        running_loss_D = 0.0\n",
    "        running_l1 = 0.0\n",
    "        t0 = time.time()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            sar_L = batch['sar_L'].to(DEVICE)  # [B,1,H,W]\n",
    "            real_ab = batch['ab'].to(DEVICE)   # [B,2,H,W]\n",
    "\n",
    "            # Prepare real and fake labels for LSGAN\n",
    "            valid = torch.ones((sar_L.size(0), 1, 30, 30), device=DEVICE)  # size depends on patch size; discriminator will handle it\n",
    "            fake = torch.zeros_like(valid)\n",
    "\n",
    "            # ------------------\n",
    "            # Train Discriminator\n",
    "            # ------------------\n",
    "            with torch.autocast(device_type=DEVICE,enabled=AMP_ENABLED):\n",
    "                fake_ab = netG(sar_L)\n",
    "                # concatenate L + ab -> 3 channels\n",
    "                real_input_D = torch.cat([sar_L, real_ab], dim=1)\n",
    "                fake_input_D = torch.cat([sar_L, fake_ab.detach()], dim=1)\n",
    "                pred_real = netD(real_input_D)\n",
    "                pred_fake = netD(fake_input_D)\n",
    "                loss_D_real = criterion_GAN(pred_real, valid)\n",
    "                loss_D_fake = criterion_GAN(pred_fake, fake)\n",
    "                loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "            optD.zero_grad()\n",
    "            scaler.scale(loss_D).backward()\n",
    "            scaler.step(optD)\n",
    "\n",
    "            # ------------------\n",
    "            # Train Generator\n",
    "            # ------------------\n",
    "            with torch.autocast(device_type=DEVICE,enabled=AMP_ENABLED):\n",
    "                fake_ab = netG(sar_L)\n",
    "                fake_input_D = torch.cat([sar_L, fake_ab], dim=1)\n",
    "                pred_fake = netD(fake_input_D)\n",
    "                loss_G_GAN = criterion_GAN(pred_fake, valid)\n",
    "                loss_G_L1 = criterion_L1(fake_ab, real_ab) * LAMBDA_L1\n",
    "                loss_G = loss_G_GAN + loss_G_L1\n",
    "\n",
    "            optG.zero_grad()\n",
    "            scaler.scale(loss_G).backward()\n",
    "            scaler.step(optG)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss_G += loss_G.item()\n",
    "            running_loss_D += loss_D.item()\n",
    "            running_l1 += loss_G_L1.item()\n",
    "\n",
    "            if (i + 1) % PRINT_EVERY_BATCHES == 0:\n",
    "                print(f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch [{i+1}/{len(train_loader)}] \"\n",
    "                      f\"LossG: {running_loss_G/(i+1):.4f} LossD: {running_loss_D/(i+1):.4f} L1: {running_l1/(i+1):.4f}\")\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f\"Epoch {epoch} finished in {t1-t0:.1f}s â€” LossG: {running_loss_G/len(train_loader):.4f} LossD: {running_loss_D/len(train_loader):.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        netG.eval(); netD.eval()\n",
    "        val_l1 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for vb, vbatch in enumerate(val_loader):\n",
    "                sar_L = vbatch['sar_L'].to(DEVICE)\n",
    "                real_ab = vbatch['ab'].to(DEVICE)\n",
    "                fake_ab = netG(sar_L)\n",
    "                val_l1 += criterion_L1(fake_ab, real_ab).item()\n",
    "        val_l1 = val_l1 / len(val_loader)\n",
    "        writer.add_scalar('val/L1', val_l1, epoch)\n",
    "\n",
    "        # Save checkpoint\n",
    "        ckpt_path = CHECKPOINT_DIR / f'ckpt_epoch_{epoch}.pth'\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'netG_state': netG.state_dict(),\n",
    "            'netD_state': netD.state_dict(),\n",
    "            'optG': optG.state_dict(),\n",
    "            'optD': optD.state_dict(),\n",
    "            'val_l1': val_l1\n",
    "        }, ckpt_path)\n",
    "\n",
    "        # Save best\n",
    "        if val_l1 < best_val_l1:\n",
    "            best_val_l1 = val_l1\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'netG_state': netG.state_dict(),\n",
    "                'netD_state': netD.state_dict(),\n",
    "                'optG': optG.state_dict(),\n",
    "                'optD': optD.state_dict(),\n",
    "                'val_l1': val_l1\n",
    "            }, CHECKPOINT_DIR / 'best.pth')\n",
    "\n",
    "        # Save sample images (reconstructed RGB)\n",
    "        if epoch % SAVE_SAMPLE_EVERY == 0:\n",
    "            sample_dir = Path('./samples')\n",
    "            sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "            # take first batch from val loader\n",
    "            vbatch = next(iter(val_loader))\n",
    "            sar_L = vbatch['sar_L'].to(DEVICE)\n",
    "            real_rgb = vbatch['rgb']\n",
    "            with torch.no_grad():\n",
    "                fake_ab = netG(sar_L).cpu().numpy()  # [B,2,H,W]\n",
    "            # reconstruct rgb from SAR-derived L + predicted ab\n",
    "            sar_L_cpu = vbatch['sar_L'].cpu().numpy()  # [B,1,H,W]\n",
    "            B = sar_L_cpu.shape[0]\n",
    "            for bi in range(min(B, 8)):\n",
    "                L = (sar_L_cpu[bi, 0] * 100.0).astype('float32')  # [H,W]\n",
    "                ab = (fake_ab[bi].transpose(1, 2, 0) * 128.0).astype('float32')\n",
    "                lab = np.concatenate([L[..., None], ab], axis=2)\n",
    "                rgb_uint8 = lab_to_rgb_uint8(lab)\n",
    "                out_p = sample_dir / f'epoch{epoch}_sample{bi}.png'\n",
    "                Image.fromarray(rgb_uint8).save(out_p)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ea362",
   "metadata": {},
   "source": [
    " Inference util: load best model and run on arbitrary SAR image, save RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "732edf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_on_image(sar_img_path: str, out_path: str, checkpoint_path: Path = CHECKPOINT_DIR / 'best.pth', use_db=USE_SAR_DB, clip_sar=SAR_PERCENTILE_CLIP):\n",
    "    device = DEVICE\n",
    "    ckpt = load_checkpoint(checkpoint_path, device)\n",
    "    netG = UNetGenerator(in_channels=1, out_channels=2).to(device)\n",
    "    netG.load_state_dict(ckpt['netG_state'])\n",
    "    netG.eval()\n",
    "\n",
    "    img = Image.open(sar_img_path).convert('L')\n",
    "    # optionally resize to multiples of 256 or keep original; UNet is flexible but expects sizes divisible by 2^8\n",
    "    w, h = img.size\n",
    "    # ensure sizes divisible by 256? We will pad to nearest multiple of 256\n",
    "    multiple = 256\n",
    "    pad_w = (multiple - (w % multiple)) % multiple\n",
    "    pad_h = (multiple - (h % multiple)) % multiple\n",
    "    if pad_w or pad_h:\n",
    "        new_w = w + pad_w\n",
    "        new_h = h + pad_h\n",
    "        img = img.resize((new_w, new_h), Image.BILINEAR)\n",
    "\n",
    "    L = sar_preprocess(img, use_db=use_db, do_clip=clip_sar)\n",
    "    L_t = torch.from_numpy(L).unsqueeze(0).unsqueeze(0).to(device)  # [1,1,H,W]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_ab = netG(L_t)\n",
    "    fake_ab = fake_ab.cpu().numpy()[0].transpose(1, 2, 0)  # H,W,2\n",
    "    L_np = (L * 100.0).astype('float32')\n",
    "    lab = np.concatenate([L_np[..., None], (fake_ab * 128.0).astype('float32')], axis=2)\n",
    "    rgb_uint8 = lab_to_rgb_uint8(lab)\n",
    "    Image.fromarray(rgb_uint8).save(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd4d7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] Batch [50/1600] LossG: 32.5488 LossD: 0.5623 L1: 30.6623\n",
      "Epoch [1/50] Batch [100/1600] LossG: 23.0459 LossD: 0.3310 L1: 21.5861\n",
      "Epoch [1/50] Batch [150/1600] LossG: 19.1125 LossD: 0.2412 L1: 17.7842\n",
      "Epoch [1/50] Batch [200/1600] LossG: 16.8688 LossD: 0.1959 L1: 15.6348\n",
      "Epoch [1/50] Batch [250/1600] LossG: 15.4417 LossD: 0.1810 L1: 14.2544\n",
      "Epoch [1/50] Batch [300/1600] LossG: 14.4004 LossD: 0.1654 L1: 13.2556\n",
      "Epoch [1/50] Batch [350/1600] LossG: 13.5528 LossD: 0.1491 L1: 12.4363\n",
      "Epoch [1/50] Batch [400/1600] LossG: 12.9516 LossD: 0.1378 L1: 11.8564\n",
      "Epoch [1/50] Batch [450/1600] LossG: 12.4913 LossD: 0.1279 L1: 11.4114\n",
      "Epoch [1/50] Batch [500/1600] LossG: 12.0982 LossD: 0.1188 L1: 11.0300\n",
      "Epoch [1/50] Batch [550/1600] LossG: 11.7612 LossD: 0.1124 L1: 10.7021\n",
      "Epoch [1/50] Batch [600/1600] LossG: 11.4696 LossD: 0.1069 L1: 10.4176\n",
      "Epoch [1/50] Batch [650/1600] LossG: 11.2392 LossD: 0.1029 L1: 10.1922\n",
      "Epoch [1/50] Batch [700/1600] LossG: 11.0199 LossD: 0.1000 L1: 9.9806\n",
      "Epoch [1/50] Batch [750/1600] LossG: 10.8229 LossD: 0.0974 L1: 9.7896\n",
      "Epoch [1/50] Batch [800/1600] LossG: 10.6576 LossD: 0.0949 L1: 9.6301\n",
      "Epoch [1/50] Batch [850/1600] LossG: 10.4999 LossD: 0.0917 L1: 9.4768\n",
      "Epoch [1/50] Batch [900/1600] LossG: 10.3711 LossD: 0.0889 L1: 9.3506\n",
      "Epoch [1/50] Batch [950/1600] LossG: 10.2663 LossD: 0.0862 L1: 9.2481\n",
      "Epoch [1/50] Batch [1000/1600] LossG: 10.1458 LossD: 0.0837 L1: 9.1299\n",
      "Epoch [1/50] Batch [1050/1600] LossG: 10.0347 LossD: 0.0809 L1: 9.0208\n",
      "Epoch [1/50] Batch [1100/1600] LossG: 9.9664 LossD: 0.0787 L1: 8.9536\n",
      "Epoch [1/50] Batch [1150/1600] LossG: 9.8890 LossD: 0.0765 L1: 8.8771\n",
      "Epoch [1/50] Batch [1200/1600] LossG: 9.8149 LossD: 0.0741 L1: 8.8041\n",
      "Epoch [1/50] Batch [1250/1600] LossG: 9.7412 LossD: 0.0721 L1: 8.7311\n",
      "Epoch [1/50] Batch [1300/1600] LossG: 9.6832 LossD: 0.0702 L1: 8.6738\n",
      "Epoch [1/50] Batch [1350/1600] LossG: 9.6273 LossD: 0.0682 L1: 8.6188\n",
      "Epoch [1/50] Batch [1400/1600] LossG: 9.5703 LossD: 0.0665 L1: 8.5625\n",
      "Epoch [1/50] Batch [1450/1600] LossG: 9.5298 LossD: 0.0647 L1: 8.5222\n",
      "Epoch [1/50] Batch [1500/1600] LossG: 9.4691 LossD: 0.0630 L1: 8.4615\n",
      "Epoch [1/50] Batch [1550/1600] LossG: 9.4127 LossD: 0.0613 L1: 8.4055\n",
      "Epoch [1/50] Batch [1600/1600] LossG: 9.3767 LossD: 0.0597 L1: 8.3697\n",
      "Epoch 1 finished in 365.3s â€” LossG: 9.3767 LossD: 0.0597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 10 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 35 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 25 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 15 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 36 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 23 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 5 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 3 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50] Batch [50/1600] LossG: 7.9507 LossD: 0.0140 L1: 6.9608\n",
      "Epoch [2/50] Batch [100/1600] LossG: 7.9316 LossD: 0.0142 L1: 6.9308\n",
      "Epoch [2/50] Batch [150/1600] LossG: 7.9736 LossD: 0.0120 L1: 6.9731\n",
      "Epoch [2/50] Batch [200/1600] LossG: 8.0614 LossD: 0.0108 L1: 7.0603\n",
      "Epoch [2/50] Batch [250/1600] LossG: 8.0578 LossD: 0.0100 L1: 7.0567\n",
      "Epoch [2/50] Batch [300/1600] LossG: 8.0906 LossD: 0.0096 L1: 7.0892\n",
      "Epoch [2/50] Batch [350/1600] LossG: 8.0510 LossD: 0.0097 L1: 7.0496\n",
      "Epoch [2/50] Batch [400/1600] LossG: 8.0484 LossD: 0.0110 L1: 7.0467\n",
      "Epoch [2/50] Batch [450/1600] LossG: 8.0350 LossD: 0.0109 L1: 7.0319\n",
      "Epoch [2/50] Batch [500/1600] LossG: 8.0056 LossD: 0.0105 L1: 7.0030\n",
      "Epoch [2/50] Batch [550/1600] LossG: 7.9934 LossD: 0.0100 L1: 6.9904\n",
      "Epoch [2/50] Batch [600/1600] LossG: 8.0123 LossD: 0.0100 L1: 7.0088\n",
      "Epoch [2/50] Batch [650/1600] LossG: 8.0171 LossD: 0.0101 L1: 7.0134\n",
      "Epoch [2/50] Batch [700/1600] LossG: 7.9772 LossD: 0.0099 L1: 6.9734\n",
      "Epoch [2/50] Batch [750/1600] LossG: 7.9575 LossD: 0.0094 L1: 6.9540\n",
      "Epoch [2/50] Batch [800/1600] LossG: 7.9639 LossD: 0.0091 L1: 6.9605\n",
      "Epoch [2/50] Batch [850/1600] LossG: 7.9686 LossD: 0.0089 L1: 6.9651\n",
      "Epoch [2/50] Batch [900/1600] LossG: 7.9592 LossD: 0.0090 L1: 6.9552\n",
      "Epoch [2/50] Batch [950/1600] LossG: 7.9521 LossD: 0.0087 L1: 6.9482\n",
      "Epoch [2/50] Batch [1000/1600] LossG: 7.9279 LossD: 0.0083 L1: 6.9243\n",
      "Epoch [2/50] Batch [1050/1600] LossG: 7.9289 LossD: 0.0085 L1: 6.9246\n",
      "Epoch [2/50] Batch [1100/1600] LossG: 7.9327 LossD: 0.0085 L1: 6.9282\n",
      "Epoch [2/50] Batch [1150/1600] LossG: 7.9279 LossD: 0.0083 L1: 6.9236\n",
      "Epoch [2/50] Batch [1200/1600] LossG: 7.9112 LossD: 0.0082 L1: 6.9068\n",
      "Epoch [2/50] Batch [1250/1600] LossG: 7.9107 LossD: 0.0079 L1: 6.9064\n",
      "Epoch [2/50] Batch [1300/1600] LossG: 7.9008 LossD: 0.0077 L1: 6.8967\n",
      "Epoch [2/50] Batch [1350/1600] LossG: 7.9019 LossD: 0.0075 L1: 6.8979\n",
      "Epoch [2/50] Batch [1400/1600] LossG: 7.9049 LossD: 0.0075 L1: 6.9007\n",
      "Epoch [2/50] Batch [1450/1600] LossG: 7.8998 LossD: 0.0073 L1: 6.8957\n",
      "Epoch [2/50] Batch [1500/1600] LossG: 7.8968 LossD: 0.0072 L1: 6.8929\n",
      "Epoch [2/50] Batch [1550/1600] LossG: 7.9009 LossD: 0.0076 L1: 6.8966\n",
      "Epoch [2/50] Batch [1600/1600] LossG: 7.8919 LossD: 0.0077 L1: 6.8872\n",
      "Epoch 2 finished in 348.4s â€” LossG: 7.8919 LossD: 0.0077\n",
      "Epoch [3/50] Batch [50/1600] LossG: 8.0731 LossD: 0.0051 L1: 7.0576\n",
      "Epoch [3/50] Batch [100/1600] LossG: 7.9960 LossD: 0.0035 L1: 6.9890\n",
      "Epoch [3/50] Batch [150/1600] LossG: 7.9232 LossD: 0.0027 L1: 6.9186\n",
      "Epoch [3/50] Batch [200/1600] LossG: 7.8010 LossD: 0.0023 L1: 6.7974\n",
      "Epoch [3/50] Batch [250/1600] LossG: 7.8641 LossD: 0.0029 L1: 6.8596\n",
      "Epoch [3/50] Batch [300/1600] LossG: 7.8250 LossD: 0.0028 L1: 6.8210\n",
      "Epoch [3/50] Batch [350/1600] LossG: 7.8036 LossD: 0.0030 L1: 6.7990\n",
      "Epoch [3/50] Batch [400/1600] LossG: 7.8202 LossD: 0.0029 L1: 6.8160\n",
      "Epoch [3/50] Batch [450/1600] LossG: 7.8213 LossD: 0.0037 L1: 6.8164\n",
      "Epoch [3/50] Batch [500/1600] LossG: 7.8173 LossD: 0.0180 L1: 6.8207\n",
      "Epoch [3/50] Batch [550/1600] LossG: 7.8326 LossD: 0.0174 L1: 6.8365\n",
      "Epoch [3/50] Batch [600/1600] LossG: 7.8429 LossD: 0.0166 L1: 6.8462\n",
      "Epoch [3/50] Batch [650/1600] LossG: 7.8368 LossD: 0.0159 L1: 6.8398\n",
      "Epoch [3/50] Batch [700/1600] LossG: 7.8413 LossD: 0.0154 L1: 6.8429\n",
      "Epoch [3/50] Batch [750/1600] LossG: 7.8285 LossD: 0.0148 L1: 6.8295\n",
      "Epoch [3/50] Batch [800/1600] LossG: 7.8021 LossD: 0.0142 L1: 6.8026\n",
      "Epoch [3/50] Batch [850/1600] LossG: 7.7988 LossD: 0.0135 L1: 6.7990\n",
      "Epoch [3/50] Batch [900/1600] LossG: 7.8092 LossD: 0.0129 L1: 6.8093\n",
      "Epoch [3/50] Batch [950/1600] LossG: 7.8100 LossD: 0.0123 L1: 6.8100\n",
      "Epoch [3/50] Batch [1000/1600] LossG: 7.8040 LossD: 0.0117 L1: 6.8039\n",
      "Epoch [3/50] Batch [1050/1600] LossG: 7.8039 LossD: 0.0112 L1: 6.8037\n",
      "Epoch [3/50] Batch [1100/1600] LossG: 7.8043 LossD: 0.0109 L1: 6.8038\n",
      "Epoch [3/50] Batch [1150/1600] LossG: 7.7931 LossD: 0.0105 L1: 6.7926\n",
      "Epoch [3/50] Batch [1200/1600] LossG: 7.7867 LossD: 0.0101 L1: 6.7863\n",
      "Epoch [3/50] Batch [1250/1600] LossG: 7.7806 LossD: 0.0102 L1: 6.7799\n",
      "Epoch [3/50] Batch [1300/1600] LossG: 7.7900 LossD: 0.0105 L1: 6.7886\n",
      "Epoch [3/50] Batch [1350/1600] LossG: 7.8072 LossD: 0.0106 L1: 6.8050\n",
      "Epoch [3/50] Batch [1400/1600] LossG: 7.8180 LossD: 0.0102 L1: 6.8159\n",
      "Epoch [3/50] Batch [1450/1600] LossG: 7.8196 LossD: 0.0099 L1: 6.8175\n",
      "Epoch [3/50] Batch [1500/1600] LossG: 7.8100 LossD: 0.0096 L1: 6.8080\n",
      "Epoch [3/50] Batch [1550/1600] LossG: 7.7982 LossD: 0.0093 L1: 6.7962\n",
      "Epoch [3/50] Batch [1600/1600] LossG: 7.7981 LossD: 0.0091 L1: 6.7962\n",
      "Epoch 3 finished in 346.3s â€” LossG: 7.7981 LossD: 0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 4 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50] Batch [50/1600] LossG: 7.9369 LossD: 0.0023 L1: 6.9329\n",
      "Epoch [4/50] Batch [100/1600] LossG: 8.0159 LossD: 0.0015 L1: 7.0140\n",
      "Epoch [4/50] Batch [150/1600] LossG: 7.9009 LossD: 0.0013 L1: 6.8992\n",
      "Epoch [4/50] Batch [200/1600] LossG: 7.8672 LossD: 0.0017 L1: 6.8647\n",
      "Epoch [4/50] Batch [250/1600] LossG: 7.7641 LossD: 0.0014 L1: 6.7620\n",
      "Epoch [4/50] Batch [300/1600] LossG: 7.6951 LossD: 0.0013 L1: 6.6935\n",
      "Epoch [4/50] Batch [350/1600] LossG: 7.6830 LossD: 0.0020 L1: 6.6796\n",
      "Epoch [4/50] Batch [400/1600] LossG: 7.6725 LossD: 0.0018 L1: 6.6696\n",
      "Epoch [4/50] Batch [450/1600] LossG: 7.6444 LossD: 0.0017 L1: 6.6418\n",
      "Epoch [4/50] Batch [500/1600] LossG: 7.6396 LossD: 0.0015 L1: 6.6374\n",
      "Epoch [4/50] Batch [550/1600] LossG: 7.6312 LossD: 0.0016 L1: 6.6287\n",
      "Epoch [4/50] Batch [600/1600] LossG: 7.6517 LossD: 0.0019 L1: 6.6490\n",
      "Epoch [4/50] Batch [650/1600] LossG: 7.6460 LossD: 0.0028 L1: 6.6418\n",
      "Epoch [4/50] Batch [700/1600] LossG: 7.6687 LossD: 0.0028 L1: 6.6646\n",
      "Epoch [4/50] Batch [750/1600] LossG: 7.6613 LossD: 0.0027 L1: 6.6573\n",
      "Epoch [4/50] Batch [800/1600] LossG: 7.6885 LossD: 0.0026 L1: 6.6846\n",
      "Epoch [4/50] Batch [850/1600] LossG: 7.6926 LossD: 0.0026 L1: 6.6885\n",
      "Epoch [4/50] Batch [900/1600] LossG: 7.7088 LossD: 0.0025 L1: 6.7049\n",
      "Epoch [4/50] Batch [950/1600] LossG: 7.7155 LossD: 0.0024 L1: 6.7119\n",
      "Epoch [4/50] Batch [1000/1600] LossG: 7.7063 LossD: 0.0024 L1: 6.7027\n",
      "Epoch [4/50] Batch [1050/1600] LossG: 7.7092 LossD: 0.0029 L1: 6.7051\n",
      "Epoch [4/50] Batch [1100/1600] LossG: 7.7201 LossD: 0.0029 L1: 6.7161\n",
      "Epoch [4/50] Batch [1150/1600] LossG: 7.7286 LossD: 0.0028 L1: 6.7246\n",
      "Epoch [4/50] Batch [1200/1600] LossG: 7.7297 LossD: 0.0027 L1: 6.7257\n",
      "Epoch [4/50] Batch [1250/1600] LossG: 7.7148 LossD: 0.0026 L1: 6.7109\n",
      "Epoch [4/50] Batch [1300/1600] LossG: 7.7139 LossD: 0.0026 L1: 6.7101\n",
      "Epoch [4/50] Batch [1350/1600] LossG: 7.7126 LossD: 0.0025 L1: 6.7089\n",
      "Epoch [4/50] Batch [1400/1600] LossG: 7.7205 LossD: 0.0025 L1: 6.7168\n",
      "Epoch [4/50] Batch [1450/1600] LossG: 7.7197 LossD: 0.0025 L1: 6.7160\n",
      "Epoch [4/50] Batch [1500/1600] LossG: 7.7093 LossD: 0.0024 L1: 6.7057\n",
      "Epoch [4/50] Batch [1550/1600] LossG: 7.7155 LossD: 0.0023 L1: 6.7120\n",
      "Epoch [4/50] Batch [1600/1600] LossG: 7.7136 LossD: 0.0024 L1: 6.7101\n",
      "Epoch 4 finished in 347.7s â€” LossG: 7.7136 LossD: 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 1 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 2 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50] Batch [50/1600] LossG: 7.6748 LossD: 0.0011 L1: 6.6732\n",
      "Epoch [5/50] Batch [100/1600] LossG: 7.5373 LossD: 0.0021 L1: 6.5351\n",
      "Epoch [5/50] Batch [150/1600] LossG: 7.6551 LossD: 0.0017 L1: 6.6532\n",
      "Epoch [5/50] Batch [200/1600] LossG: 7.6694 LossD: 0.0836 L1: 6.7213\n",
      "Epoch [5/50] Batch [250/1600] LossG: 7.6768 LossD: 0.0854 L1: 6.7622\n",
      "Epoch [5/50] Batch [300/1600] LossG: 7.6736 LossD: 0.0795 L1: 6.7548\n",
      "Epoch [5/50] Batch [350/1600] LossG: 7.7964 LossD: 0.0723 L1: 6.8708\n",
      "Epoch [5/50] Batch [400/1600] LossG: 7.7941 LossD: 0.0659 L1: 6.8620\n",
      "Epoch [5/50] Batch [450/1600] LossG: 7.7748 LossD: 0.0610 L1: 6.8363\n",
      "Epoch [5/50] Batch [500/1600] LossG: 7.7624 LossD: 0.0565 L1: 6.8189\n",
      "Epoch [5/50] Batch [550/1600] LossG: 7.7705 LossD: 0.0523 L1: 6.8223\n",
      "Epoch [5/50] Batch [600/1600] LossG: 7.7967 LossD: 0.0486 L1: 6.8450\n",
      "Epoch [5/50] Batch [650/1600] LossG: 7.7866 LossD: 0.0454 L1: 6.8317\n",
      "Epoch [5/50] Batch [700/1600] LossG: 7.7908 LossD: 0.0426 L1: 6.8334\n",
      "Epoch [5/50] Batch [750/1600] LossG: 7.7937 LossD: 0.0401 L1: 6.8335\n",
      "Epoch [5/50] Batch [800/1600] LossG: 7.7676 LossD: 0.0382 L1: 6.8047\n",
      "Epoch [5/50] Batch [850/1600] LossG: 7.7752 LossD: 0.0363 L1: 6.8104\n",
      "Epoch [5/50] Batch [900/1600] LossG: 7.7330 LossD: 0.0345 L1: 6.7665\n",
      "Epoch [5/50] Batch [950/1600] LossG: 7.7349 LossD: 0.0331 L1: 6.7667\n",
      "Epoch [5/50] Batch [1000/1600] LossG: 7.7294 LossD: 0.0317 L1: 6.7593\n",
      "Epoch [5/50] Batch [1050/1600] LossG: 7.7384 LossD: 0.0304 L1: 6.7667\n",
      "Epoch [5/50] Batch [1100/1600] LossG: 7.7337 LossD: 0.0291 L1: 6.7608\n",
      "Epoch [5/50] Batch [1150/1600] LossG: 7.7273 LossD: 0.0279 L1: 6.7532\n",
      "Epoch [5/50] Batch [1200/1600] LossG: 7.7205 LossD: 0.0268 L1: 6.7454\n",
      "Epoch [5/50] Batch [1250/1600] LossG: 7.7194 LossD: 0.0259 L1: 6.7433\n",
      "Epoch [5/50] Batch [1300/1600] LossG: 7.7014 LossD: 0.0250 L1: 6.7243\n",
      "Epoch [5/50] Batch [1350/1600] LossG: 7.6966 LossD: 0.0241 L1: 6.7187\n",
      "Epoch [5/50] Batch [1400/1600] LossG: 7.6927 LossD: 0.0233 L1: 6.7142\n",
      "Epoch [5/50] Batch [1450/1600] LossG: 7.6916 LossD: 0.0236 L1: 6.7122\n",
      "Epoch [5/50] Batch [1500/1600] LossG: 7.6828 LossD: 0.0230 L1: 6.7029\n",
      "Epoch [5/50] Batch [1550/1600] LossG: 7.6892 LossD: 0.0224 L1: 6.7086\n",
      "Epoch [5/50] Batch [1600/1600] LossG: 7.6969 LossD: 0.0218 L1: 6.7157\n",
      "Epoch 5 finished in 347.2s â€” LossG: 7.6969 LossD: 0.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 12 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 7 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 6 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50] Batch [50/1600] LossG: 7.7413 LossD: 0.0059 L1: 6.7473\n",
      "Epoch [6/50] Batch [100/1600] LossG: 7.6610 LossD: 0.0059 L1: 6.6630\n",
      "Epoch [6/50] Batch [150/1600] LossG: 7.5805 LossD: 0.0048 L1: 6.5809\n",
      "Epoch [6/50] Batch [200/1600] LossG: 7.5105 LossD: 0.0076 L1: 6.5079\n",
      "Epoch [6/50] Batch [250/1600] LossG: 7.5775 LossD: 0.0197 L1: 6.5950\n",
      "Epoch [6/50] Batch [300/1600] LossG: 7.5728 LossD: 0.0193 L1: 6.5893\n",
      "Epoch [6/50] Batch [350/1600] LossG: 7.6191 LossD: 0.0178 L1: 6.6344\n",
      "Epoch [6/50] Batch [400/1600] LossG: 7.6247 LossD: 0.0161 L1: 6.6379\n",
      "Epoch [6/50] Batch [450/1600] LossG: 7.6298 LossD: 0.0146 L1: 6.6417\n",
      "Epoch [6/50] Batch [500/1600] LossG: 7.6242 LossD: 0.0135 L1: 6.6353\n",
      "Epoch [6/50] Batch [550/1600] LossG: 7.6232 LossD: 0.0127 L1: 6.6332\n",
      "Epoch [6/50] Batch [600/1600] LossG: 7.6082 LossD: 0.0119 L1: 6.6175\n",
      "Epoch [6/50] Batch [650/1600] LossG: 7.6047 LossD: 0.0123 L1: 6.6139\n",
      "Epoch [6/50] Batch [700/1600] LossG: 7.6102 LossD: 0.0117 L1: 6.6189\n",
      "Epoch [6/50] Batch [750/1600] LossG: 7.6242 LossD: 0.0112 L1: 6.6324\n",
      "Epoch [6/50] Batch [800/1600] LossG: 7.6403 LossD: 0.0128 L1: 6.6506\n",
      "Epoch [6/50] Batch [850/1600] LossG: 7.6362 LossD: 0.0124 L1: 6.6457\n",
      "Epoch [6/50] Batch [900/1600] LossG: 7.6202 LossD: 0.0119 L1: 6.6294\n",
      "Epoch [6/50] Batch [950/1600] LossG: 7.6251 LossD: 0.0115 L1: 6.6338\n",
      "Epoch [6/50] Batch [1000/1600] LossG: 7.6239 LossD: 0.0110 L1: 6.6323\n",
      "Epoch [6/50] Batch [1050/1600] LossG: 7.6195 LossD: 0.0105 L1: 6.6275\n",
      "Epoch [6/50] Batch [1100/1600] LossG: 7.6212 LossD: 0.0101 L1: 6.6289\n",
      "Epoch [6/50] Batch [1150/1600] LossG: 7.6285 LossD: 0.0097 L1: 6.6358\n",
      "Epoch [6/50] Batch [1200/1600] LossG: 7.6266 LossD: 0.0093 L1: 6.6336\n",
      "Epoch [6/50] Batch [1250/1600] LossG: 7.6008 LossD: 0.0090 L1: 6.6075\n",
      "Epoch [6/50] Batch [1300/1600] LossG: 7.5989 LossD: 0.0087 L1: 6.6054\n",
      "Epoch [6/50] Batch [1350/1600] LossG: 7.5795 LossD: 0.0084 L1: 6.5858\n",
      "Epoch [6/50] Batch [1400/1600] LossG: 7.5900 LossD: 0.0082 L1: 6.5960\n",
      "Epoch [6/50] Batch [1450/1600] LossG: 7.5791 LossD: 0.0079 L1: 6.5850\n",
      "Epoch [6/50] Batch [1500/1600] LossG: 7.5749 LossD: 0.0077 L1: 6.5806\n",
      "Epoch [6/50] Batch [1550/1600] LossG: 7.5717 LossD: 0.0075 L1: 6.5771\n",
      "Epoch [6/50] Batch [1600/1600] LossG: 7.5660 LossD: 0.0073 L1: 6.5713\n",
      "Epoch 6 finished in 349.0s â€” LossG: 7.5660 LossD: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 13 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 8 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50] Batch [50/1600] LossG: 7.5419 LossD: 0.0010 L1: 6.5422\n",
      "Epoch [7/50] Batch [100/1600] LossG: 7.6512 LossD: 0.0009 L1: 6.6516\n",
      "Epoch [7/50] Batch [150/1600] LossG: 7.6099 LossD: 0.0009 L1: 6.6102\n",
      "Epoch [7/50] Batch [200/1600] LossG: 7.5065 LossD: 0.0010 L1: 6.5065\n",
      "Epoch [7/50] Batch [250/1600] LossG: 7.5604 LossD: 0.0010 L1: 6.5606\n",
      "Epoch [7/50] Batch [300/1600] LossG: 7.5936 LossD: 0.0010 L1: 6.5937\n",
      "Epoch [7/50] Batch [350/1600] LossG: 7.5448 LossD: 0.0012 L1: 6.5443\n",
      "Epoch [7/50] Batch [400/1600] LossG: 7.5269 LossD: 0.0015 L1: 6.5268\n",
      "Epoch [7/50] Batch [450/1600] LossG: 7.5248 LossD: 0.0015 L1: 6.5246\n",
      "Epoch [7/50] Batch [500/1600] LossG: 7.5060 LossD: 0.0014 L1: 6.5059\n",
      "Epoch [7/50] Batch [550/1600] LossG: 7.4554 LossD: 0.0013 L1: 6.4552\n",
      "Epoch [7/50] Batch [600/1600] LossG: 7.4543 LossD: 0.0013 L1: 6.4542\n",
      "Epoch [7/50] Batch [650/1600] LossG: 7.4493 LossD: 0.0014 L1: 6.4491\n",
      "Epoch [7/50] Batch [700/1600] LossG: 7.4320 LossD: 0.0013 L1: 6.4319\n",
      "Epoch [7/50] Batch [750/1600] LossG: 7.3919 LossD: 0.0013 L1: 6.3917\n",
      "Epoch [7/50] Batch [800/1600] LossG: 7.3835 LossD: 0.0013 L1: 6.3832\n",
      "Epoch [7/50] Batch [850/1600] LossG: 7.3860 LossD: 0.0013 L1: 6.3856\n",
      "Epoch [7/50] Batch [900/1600] LossG: 7.3693 LossD: 0.0022 L1: 6.3693\n",
      "Epoch [7/50] Batch [950/1600] LossG: 7.3873 LossD: 0.0033 L1: 6.3885\n",
      "Epoch [7/50] Batch [1000/1600] LossG: 7.3715 LossD: 0.0033 L1: 6.3728\n",
      "Epoch [7/50] Batch [1050/1600] LossG: 7.3708 LossD: 0.0032 L1: 6.3720\n",
      "Epoch [7/50] Batch [1100/1600] LossG: 7.3781 LossD: 0.0031 L1: 6.3792\n",
      "Epoch [7/50] Batch [1150/1600] LossG: 7.3619 LossD: 0.0030 L1: 6.3630\n",
      "Epoch [7/50] Batch [1200/1600] LossG: 7.3711 LossD: 0.0029 L1: 6.3722\n",
      "Epoch [7/50] Batch [1250/1600] LossG: 7.3705 LossD: 0.0028 L1: 6.3714\n",
      "Epoch [7/50] Batch [1300/1600] LossG: 7.3604 LossD: 0.0028 L1: 6.3613\n",
      "Epoch [7/50] Batch [1350/1600] LossG: 7.3634 LossD: 0.0027 L1: 6.3643\n",
      "Epoch [7/50] Batch [1400/1600] LossG: 7.3576 LossD: 0.0026 L1: 6.3584\n",
      "Epoch [7/50] Batch [1450/1600] LossG: 7.3574 LossD: 0.0027 L1: 6.3582\n",
      "Epoch [7/50] Batch [1500/1600] LossG: 7.3634 LossD: 0.0026 L1: 6.3641\n",
      "Epoch [7/50] Batch [1550/1600] LossG: 7.3580 LossD: 0.0026 L1: 6.3586\n",
      "Epoch [7/50] Batch [1600/1600] LossG: 7.3527 LossD: 0.0026 L1: 6.3533\n",
      "Epoch 7 finished in 554.9s â€” LossG: 7.3527 LossD: 0.0026\n",
      "Epoch [8/50] Batch [50/1600] LossG: 7.3167 LossD: 0.0011 L1: 6.3147\n",
      "Epoch [8/50] Batch [100/1600] LossG: 7.2593 LossD: 0.0023 L1: 6.2609\n",
      "Epoch [8/50] Batch [150/1600] LossG: 7.2776 LossD: 0.0045 L1: 6.2782\n",
      "Epoch [8/50] Batch [200/1600] LossG: 7.2530 LossD: 0.0038 L1: 6.2530\n",
      "Epoch [8/50] Batch [250/1600] LossG: 7.2637 LossD: 0.0032 L1: 6.2639\n",
      "Epoch [8/50] Batch [300/1600] LossG: 7.2827 LossD: 0.0028 L1: 6.2828\n",
      "Epoch [8/50] Batch [350/1600] LossG: 7.2427 LossD: 0.0024 L1: 6.2428\n",
      "Epoch [8/50] Batch [400/1600] LossG: 7.2100 LossD: 0.0022 L1: 6.2101\n",
      "Epoch [8/50] Batch [450/1600] LossG: 7.1979 LossD: 0.0020 L1: 6.1980\n",
      "Epoch [8/50] Batch [500/1600] LossG: 7.1681 LossD: 0.0019 L1: 6.1682\n",
      "Epoch [8/50] Batch [550/1600] LossG: 7.1688 LossD: 0.0018 L1: 6.1689\n",
      "Epoch [8/50] Batch [600/1600] LossG: 7.1766 LossD: 0.0016 L1: 6.1767\n",
      "Epoch [8/50] Batch [650/1600] LossG: 7.1739 LossD: 0.0016 L1: 6.1739\n",
      "Epoch [8/50] Batch [700/1600] LossG: 7.1727 LossD: 0.0015 L1: 6.1728\n",
      "Epoch [8/50] Batch [750/1600] LossG: 7.1684 LossD: 0.0014 L1: 6.1684\n",
      "Epoch [8/50] Batch [800/1600] LossG: 7.1712 LossD: 0.0014 L1: 6.1711\n",
      "Epoch [8/50] Batch [850/1600] LossG: 7.1829 LossD: 0.0014 L1: 6.1829\n",
      "Epoch [8/50] Batch [900/1600] LossG: 7.1741 LossD: 0.0013 L1: 6.1741\n",
      "Epoch [8/50] Batch [950/1600] LossG: 7.1736 LossD: 0.0015 L1: 6.1735\n",
      "Epoch [8/50] Batch [1000/1600] LossG: 7.1793 LossD: 0.0014 L1: 6.1792\n",
      "Epoch [8/50] Batch [1050/1600] LossG: 7.1700 LossD: 0.0014 L1: 6.1699\n",
      "Epoch [8/50] Batch [1100/1600] LossG: 7.1668 LossD: 0.0013 L1: 6.1666\n",
      "Epoch [8/50] Batch [1150/1600] LossG: 7.1653 LossD: 0.0013 L1: 6.1651\n",
      "Epoch [8/50] Batch [1200/1600] LossG: 7.1706 LossD: 0.0013 L1: 6.1704\n",
      "Epoch [8/50] Batch [1250/1600] LossG: 7.1738 LossD: 0.0013 L1: 6.1736\n",
      "Epoch [8/50] Batch [1300/1600] LossG: 7.1626 LossD: 0.0012 L1: 6.1624\n",
      "Epoch [8/50] Batch [1350/1600] LossG: 7.1574 LossD: 0.0012 L1: 6.1572\n",
      "Epoch [8/50] Batch [1400/1600] LossG: 7.1500 LossD: 0.0012 L1: 6.1497\n",
      "Epoch [8/50] Batch [1450/1600] LossG: 7.1475 LossD: 0.0011 L1: 6.1472\n",
      "Epoch [8/50] Batch [1500/1600] LossG: 7.1430 LossD: 0.0011 L1: 6.1427\n",
      "Epoch [8/50] Batch [1550/1600] LossG: 7.1409 LossD: 0.0023 L1: 6.1414\n",
      "Epoch [8/50] Batch [1600/1600] LossG: 7.1483 LossD: 0.0026 L1: 6.1499\n",
      "Epoch 8 finished in 1439.9s â€” LossG: 7.1483 LossD: 0.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 32 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 24 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 26 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 16 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50] Batch [50/1600] LossG: 7.2573 LossD: 0.0013 L1: 6.2581\n",
      "Epoch [9/50] Batch [100/1600] LossG: 7.2154 LossD: 0.0034 L1: 6.2102\n",
      "Epoch [9/50] Batch [150/1600] LossG: 7.1812 LossD: 0.0030 L1: 6.1766\n",
      "Epoch [9/50] Batch [200/1600] LossG: 7.2106 LossD: 0.0027 L1: 6.2058\n",
      "Epoch [9/50] Batch [250/1600] LossG: 7.1763 LossD: 0.0025 L1: 6.1711\n",
      "Epoch [9/50] Batch [300/1600] LossG: 7.0752 LossD: 0.0022 L1: 6.0709\n",
      "Epoch [9/50] Batch [350/1600] LossG: 7.0802 LossD: 0.0021 L1: 6.0765\n",
      "Epoch [9/50] Batch [400/1600] LossG: 7.0604 LossD: 0.0021 L1: 6.0572\n",
      "Epoch [9/50] Batch [450/1600] LossG: 7.0084 LossD: 0.0019 L1: 6.0056\n",
      "Epoch [9/50] Batch [500/1600] LossG: 7.0238 LossD: 0.0018 L1: 6.0212\n",
      "Epoch [9/50] Batch [550/1600] LossG: 7.0416 LossD: 0.0017 L1: 6.0391\n",
      "Epoch [9/50] Batch [600/1600] LossG: 7.0217 LossD: 0.0016 L1: 6.0194\n",
      "Epoch [9/50] Batch [650/1600] LossG: 7.0055 LossD: 0.0015 L1: 6.0034\n",
      "Epoch [9/50] Batch [700/1600] LossG: 6.9827 LossD: 0.0014 L1: 5.9808\n",
      "Epoch [9/50] Batch [750/1600] LossG: 6.9728 LossD: 0.0014 L1: 5.9711\n",
      "Epoch [9/50] Batch [800/1600] LossG: 6.9756 LossD: 0.0013 L1: 5.9738\n",
      "Epoch [9/50] Batch [850/1600] LossG: 6.9835 LossD: 0.0014 L1: 5.9817\n",
      "Epoch [9/50] Batch [900/1600] LossG: 7.0073 LossD: 0.0013 L1: 6.0056\n",
      "Epoch [9/50] Batch [950/1600] LossG: 7.0137 LossD: 0.0013 L1: 6.0119\n",
      "Epoch [9/50] Batch [1000/1600] LossG: 7.0173 LossD: 0.0012 L1: 6.0156\n",
      "Epoch [9/50] Batch [1050/1600] LossG: 7.0064 LossD: 0.0012 L1: 6.0048\n",
      "Epoch [9/50] Batch [1100/1600] LossG: 7.0063 LossD: 0.0012 L1: 6.0047\n",
      "Epoch [9/50] Batch [1150/1600] LossG: 7.0018 LossD: 0.0011 L1: 6.0003\n",
      "Epoch [9/50] Batch [1200/1600] LossG: 6.9925 LossD: 0.0011 L1: 5.9911\n",
      "Epoch [9/50] Batch [1250/1600] LossG: 7.0015 LossD: 0.0015 L1: 6.0004\n",
      "Epoch [9/50] Batch [1300/1600] LossG: 7.0004 LossD: 0.0014 L1: 5.9993\n",
      "Epoch [9/50] Batch [1350/1600] LossG: 6.9981 LossD: 0.0014 L1: 5.9970\n",
      "Epoch [9/50] Batch [1400/1600] LossG: 6.9959 LossD: 0.0013 L1: 5.9948\n",
      "Epoch [9/50] Batch [1450/1600] LossG: 6.9902 LossD: 0.0013 L1: 5.9892\n",
      "Epoch [9/50] Batch [1500/1600] LossG: 6.9860 LossD: 0.0013 L1: 5.9850\n",
      "Epoch [9/50] Batch [1550/1600] LossG: 6.9881 LossD: 0.0013 L1: 5.9871\n",
      "Epoch [9/50] Batch [1600/1600] LossG: 6.9890 LossD: 0.0012 L1: 5.9880\n",
      "Epoch 9 finished in 489.4s â€” LossG: 6.9890 LossD: 0.0012\n",
      "Epoch [10/50] Batch [50/1600] LossG: 6.8898 LossD: 0.0001 L1: 5.8900\n",
      "Epoch [10/50] Batch [100/1600] LossG: 6.9379 LossD: 0.0001 L1: 5.9382\n",
      "Epoch [10/50] Batch [150/1600] LossG: 7.0356 LossD: 0.0002 L1: 6.0359\n",
      "Epoch [10/50] Batch [200/1600] LossG: 7.0443 LossD: 0.0002 L1: 6.0447\n",
      "Epoch [10/50] Batch [250/1600] LossG: 6.9746 LossD: 0.0002 L1: 5.9750\n",
      "Epoch [10/50] Batch [300/1600] LossG: 6.9041 LossD: 0.0002 L1: 5.9045\n",
      "Epoch [10/50] Batch [350/1600] LossG: 6.8735 LossD: 0.0002 L1: 5.8738\n",
      "Epoch [10/50] Batch [400/1600] LossG: 6.8870 LossD: 0.0002 L1: 5.8873\n",
      "Epoch [10/50] Batch [450/1600] LossG: 6.8697 LossD: 0.0002 L1: 5.8700\n",
      "Epoch [10/50] Batch [500/1600] LossG: 6.8536 LossD: 0.0002 L1: 5.8539\n",
      "Epoch [10/50] Batch [550/1600] LossG: 6.8645 LossD: 0.0002 L1: 5.8647\n",
      "Epoch [10/50] Batch [600/1600] LossG: 6.8505 LossD: 0.0002 L1: 5.8507\n",
      "Epoch [10/50] Batch [650/1600] LossG: 6.8416 LossD: 0.0002 L1: 5.8419\n",
      "Epoch [10/50] Batch [700/1600] LossG: 6.8331 LossD: 0.0002 L1: 5.8334\n",
      "Epoch [10/50] Batch [750/1600] LossG: 6.8233 LossD: 0.0002 L1: 5.8235\n",
      "Epoch [10/50] Batch [800/1600] LossG: 6.8231 LossD: 0.0002 L1: 5.8233\n",
      "Epoch [10/50] Batch [850/1600] LossG: 6.8280 LossD: 0.0002 L1: 5.8282\n",
      "Epoch [10/50] Batch [900/1600] LossG: 6.8288 LossD: 0.0002 L1: 5.8290\n",
      "Epoch [10/50] Batch [950/1600] LossG: 6.8367 LossD: 0.0002 L1: 5.8369\n",
      "Epoch [10/50] Batch [1000/1600] LossG: 6.8350 LossD: 0.0002 L1: 5.8351\n",
      "Epoch [10/50] Batch [1050/1600] LossG: 6.8303 LossD: 0.0002 L1: 5.8305\n",
      "Epoch [10/50] Batch [1100/1600] LossG: 6.8301 LossD: 0.0002 L1: 5.8302\n",
      "Epoch [10/50] Batch [1150/1600] LossG: 6.8266 LossD: 0.0003 L1: 5.8270\n",
      "Epoch [10/50] Batch [1200/1600] LossG: 6.8231 LossD: 0.0022 L1: 5.8271\n",
      "Epoch [10/50] Batch [1250/1600] LossG: 6.8268 LossD: 0.0021 L1: 5.8308\n",
      "Epoch [10/50] Batch [1300/1600] LossG: 6.8181 LossD: 0.0021 L1: 5.8220\n",
      "Epoch [10/50] Batch [1350/1600] LossG: 6.8256 LossD: 0.0020 L1: 5.8294\n",
      "Epoch [10/50] Batch [1400/1600] LossG: 6.8255 LossD: 0.0020 L1: 5.8291\n",
      "Epoch [10/50] Batch [1450/1600] LossG: 6.8283 LossD: 0.0019 L1: 5.8318\n",
      "Epoch [10/50] Batch [1500/1600] LossG: 6.8305 LossD: 0.0023 L1: 5.8335\n",
      "Epoch [10/50] Batch [1550/1600] LossG: 6.8322 LossD: 0.0023 L1: 5.8350\n",
      "Epoch [10/50] Batch [1600/1600] LossG: 6.8343 LossD: 0.0022 L1: 5.8370\n",
      "Epoch 10 finished in 468.6s â€” LossG: 6.8343 LossD: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 39 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50] Batch [50/1600] LossG: 6.4863 LossD: 0.0004 L1: 5.4856\n",
      "Epoch [11/50] Batch [100/1600] LossG: 6.7525 LossD: 0.0004 L1: 5.7519\n",
      "Epoch [11/50] Batch [150/1600] LossG: 6.7957 LossD: 0.0004 L1: 5.7959\n",
      "Epoch [11/50] Batch [200/1600] LossG: 6.7148 LossD: 0.0003 L1: 5.7147\n",
      "Epoch [11/50] Batch [250/1600] LossG: 6.7144 LossD: 0.0003 L1: 5.7144\n",
      "Epoch [11/50] Batch [300/1600] LossG: 6.7251 LossD: 0.0003 L1: 5.7252\n",
      "Epoch [11/50] Batch [350/1600] LossG: 6.7036 LossD: 0.0003 L1: 5.7036\n",
      "Epoch [11/50] Batch [400/1600] LossG: 6.7092 LossD: 0.0003 L1: 5.7093\n",
      "Epoch [11/50] Batch [450/1600] LossG: 6.6688 LossD: 0.0003 L1: 5.6689\n",
      "Epoch [11/50] Batch [500/1600] LossG: 6.6490 LossD: 0.0003 L1: 5.6491\n",
      "Epoch [11/50] Batch [550/1600] LossG: 6.6314 LossD: 0.0003 L1: 5.6314\n",
      "Epoch [11/50] Batch [600/1600] LossG: 6.6325 LossD: 0.0002 L1: 5.6326\n",
      "Epoch [11/50] Batch [650/1600] LossG: 6.6317 LossD: 0.0002 L1: 5.6318\n",
      "Epoch [11/50] Batch [700/1600] LossG: 6.6591 LossD: 0.0002 L1: 5.6592\n",
      "Epoch [11/50] Batch [750/1600] LossG: 6.6653 LossD: 0.0002 L1: 5.6654\n",
      "Epoch [11/50] Batch [800/1600] LossG: 6.6485 LossD: 0.0002 L1: 5.6486\n",
      "Epoch [11/50] Batch [850/1600] LossG: 6.6476 LossD: 0.0002 L1: 5.6477\n",
      "Epoch [11/50] Batch [900/1600] LossG: 6.6545 LossD: 0.0002 L1: 5.6545\n",
      "Epoch [11/50] Batch [950/1600] LossG: 6.6554 LossD: 0.0002 L1: 5.6555\n",
      "Epoch [11/50] Batch [1000/1600] LossG: 6.6518 LossD: 0.0002 L1: 5.6519\n",
      "Epoch [11/50] Batch [1050/1600] LossG: 6.6544 LossD: 0.0002 L1: 5.6545\n",
      "Epoch [11/50] Batch [1100/1600] LossG: 6.6536 LossD: 0.0002 L1: 5.6537\n",
      "Epoch [11/50] Batch [1150/1600] LossG: 6.6510 LossD: 0.0002 L1: 5.6510\n",
      "Epoch [11/50] Batch [1200/1600] LossG: 6.6512 LossD: 0.0002 L1: 5.6512\n",
      "Epoch [11/50] Batch [1250/1600] LossG: 6.6487 LossD: 0.0002 L1: 5.6487\n",
      "Epoch [11/50] Batch [1300/1600] LossG: 6.6481 LossD: 0.0002 L1: 5.6482\n",
      "Epoch [11/50] Batch [1350/1600] LossG: 6.6442 LossD: 0.0002 L1: 5.6443\n",
      "Epoch [11/50] Batch [1400/1600] LossG: 6.6453 LossD: 0.0002 L1: 5.6454\n",
      "Epoch [11/50] Batch [1450/1600] LossG: 6.6425 LossD: 0.0002 L1: 5.6426\n",
      "Epoch [11/50] Batch [1500/1600] LossG: 6.6445 LossD: 0.0002 L1: 5.6446\n",
      "Epoch [11/50] Batch [1550/1600] LossG: 6.6390 LossD: 0.0002 L1: 5.6391\n",
      "Epoch [11/50] Batch [1600/1600] LossG: 6.6379 LossD: 0.0002 L1: 5.6379\n",
      "Epoch 11 finished in 464.7s â€” LossG: 6.6379 LossD: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 22 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50] Batch [50/1600] LossG: 6.7070 LossD: 0.0003 L1: 5.7055\n",
      "Epoch [12/50] Batch [100/1600] LossG: 6.6981 LossD: 0.0002 L1: 5.6974\n",
      "Epoch [12/50] Batch [150/1600] LossG: 6.6988 LossD: 0.0002 L1: 5.6983\n",
      "Epoch [12/50] Batch [200/1600] LossG: 6.6486 LossD: 0.0002 L1: 5.6483\n",
      "Epoch [12/50] Batch [250/1600] LossG: 6.6108 LossD: 0.0002 L1: 5.6104\n",
      "Epoch [12/50] Batch [300/1600] LossG: 6.6207 LossD: 0.0002 L1: 5.6205\n",
      "Epoch [12/50] Batch [350/1600] LossG: 6.6289 LossD: 0.0002 L1: 5.6287\n",
      "Epoch [12/50] Batch [400/1600] LossG: 6.6008 LossD: 0.0003 L1: 5.6007\n",
      "Epoch [12/50] Batch [450/1600] LossG: 6.5797 LossD: 0.0002 L1: 5.5796\n",
      "Epoch [12/50] Batch [500/1600] LossG: 6.5947 LossD: 0.0002 L1: 5.5946\n",
      "Epoch [12/50] Batch [550/1600] LossG: 6.5818 LossD: 0.0002 L1: 5.5818\n",
      "Epoch [12/50] Batch [600/1600] LossG: 6.5692 LossD: 0.0002 L1: 5.5692\n",
      "Epoch [12/50] Batch [650/1600] LossG: 6.5686 LossD: 0.0002 L1: 5.5686\n",
      "Epoch [12/50] Batch [700/1600] LossG: 6.5701 LossD: 0.0002 L1: 5.5702\n",
      "Epoch [12/50] Batch [750/1600] LossG: 6.5569 LossD: 0.0002 L1: 5.5569\n",
      "Epoch [12/50] Batch [800/1600] LossG: 6.5616 LossD: 0.0002 L1: 5.5617\n",
      "Epoch [12/50] Batch [850/1600] LossG: 6.5568 LossD: 0.0002 L1: 5.5568\n",
      "Epoch [12/50] Batch [900/1600] LossG: 6.5505 LossD: 0.0002 L1: 5.5505\n",
      "Epoch [12/50] Batch [950/1600] LossG: 6.5570 LossD: 0.0002 L1: 5.5570\n",
      "Epoch [12/50] Batch [1000/1600] LossG: 6.5622 LossD: 0.0002 L1: 5.5623\n",
      "Epoch [12/50] Batch [1050/1600] LossG: 6.5580 LossD: 0.0011 L1: 5.5598\n",
      "Epoch [12/50] Batch [1100/1600] LossG: 6.5543 LossD: 0.0011 L1: 5.5561\n",
      "Epoch [12/50] Batch [1150/1600] LossG: 6.5558 LossD: 0.0010 L1: 5.5576\n",
      "Epoch [12/50] Batch [1200/1600] LossG: 6.5551 LossD: 0.0010 L1: 5.5569\n",
      "Epoch [12/50] Batch [1250/1600] LossG: 6.5666 LossD: 0.0010 L1: 5.5679\n",
      "Epoch [12/50] Batch [1300/1600] LossG: 6.5674 LossD: 0.0009 L1: 5.5687\n",
      "Epoch [12/50] Batch [1350/1600] LossG: 6.5689 LossD: 0.0009 L1: 5.5702\n",
      "Epoch [12/50] Batch [1400/1600] LossG: 6.5656 LossD: 0.0009 L1: 5.5669\n",
      "Epoch [12/50] Batch [1450/1600] LossG: 6.5624 LossD: 0.0008 L1: 5.5636\n",
      "Epoch [12/50] Batch [1500/1600] LossG: 6.5642 LossD: 0.0008 L1: 5.5653\n",
      "Epoch [12/50] Batch [1550/1600] LossG: 6.5636 LossD: 0.0008 L1: 5.5648\n",
      "Epoch [12/50] Batch [1600/1600] LossG: 6.5653 LossD: 0.0008 L1: 5.5664\n",
      "Epoch 12 finished in 456.1s â€” LossG: 6.5653 LossD: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 9 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50] Batch [50/1600] LossG: 6.6917 LossD: 0.0001 L1: 5.6918\n",
      "Epoch [13/50] Batch [100/1600] LossG: 6.5831 LossD: 0.0001 L1: 5.5832\n",
      "Epoch [13/50] Batch [150/1600] LossG: 6.5998 LossD: 0.0001 L1: 5.5999\n",
      "Epoch [13/50] Batch [200/1600] LossG: 6.5895 LossD: 0.0001 L1: 5.5896\n",
      "Epoch [13/50] Batch [250/1600] LossG: 6.5781 LossD: 0.0001 L1: 5.5783\n",
      "Epoch [13/50] Batch [300/1600] LossG: 6.5582 LossD: 0.0001 L1: 5.5583\n",
      "Epoch [13/50] Batch [350/1600] LossG: 6.5698 LossD: 0.0001 L1: 5.5700\n",
      "Epoch [13/50] Batch [400/1600] LossG: 6.5732 LossD: 0.0001 L1: 5.5733\n",
      "Epoch [13/50] Batch [450/1600] LossG: 6.5739 LossD: 0.0001 L1: 5.5741\n",
      "Epoch [13/50] Batch [500/1600] LossG: 6.5628 LossD: 0.0001 L1: 5.5629\n",
      "Epoch [13/50] Batch [550/1600] LossG: 6.5575 LossD: 0.0001 L1: 5.5575\n",
      "Epoch [13/50] Batch [600/1600] LossG: 6.5516 LossD: 0.0001 L1: 5.5516\n",
      "Epoch [13/50] Batch [650/1600] LossG: 6.5521 LossD: 0.0001 L1: 5.5522\n",
      "Epoch [13/50] Batch [700/1600] LossG: 6.5488 LossD: 0.0001 L1: 5.5487\n",
      "Epoch [13/50] Batch [750/1600] LossG: 6.5300 LossD: 0.0001 L1: 5.5299\n",
      "Epoch [13/50] Batch [800/1600] LossG: 6.5307 LossD: 0.0001 L1: 5.5307\n",
      "Epoch [13/50] Batch [850/1600] LossG: 6.5314 LossD: 0.0001 L1: 5.5314\n",
      "Epoch [13/50] Batch [900/1600] LossG: 6.5325 LossD: 0.0001 L1: 5.5325\n",
      "Epoch [13/50] Batch [950/1600] LossG: 6.5260 LossD: 0.0001 L1: 5.5260\n",
      "Epoch [13/50] Batch [1000/1600] LossG: 6.5336 LossD: 0.0001 L1: 5.5336\n",
      "Epoch [13/50] Batch [1050/1600] LossG: 6.5270 LossD: 0.0001 L1: 5.5270\n",
      "Epoch [13/50] Batch [1100/1600] LossG: 6.5269 LossD: 0.0001 L1: 5.5269\n",
      "Epoch [13/50] Batch [1150/1600] LossG: 6.5334 LossD: 0.0001 L1: 5.5334\n",
      "Epoch [13/50] Batch [1200/1600] LossG: 6.5247 LossD: 0.0001 L1: 5.5247\n",
      "Epoch [13/50] Batch [1250/1600] LossG: 6.5139 LossD: 0.0001 L1: 5.5139\n",
      "Epoch [13/50] Batch [1300/1600] LossG: 6.5128 LossD: 0.0001 L1: 5.5128\n",
      "Epoch [13/50] Batch [1350/1600] LossG: 6.5028 LossD: 0.0001 L1: 5.5028\n",
      "Epoch [13/50] Batch [1400/1600] LossG: 6.4949 LossD: 0.0001 L1: 5.4949\n",
      "Epoch [13/50] Batch [1450/1600] LossG: 6.4994 LossD: 0.0001 L1: 5.4994\n",
      "Epoch [13/50] Batch [1500/1600] LossG: 6.4944 LossD: 0.0001 L1: 5.4944\n",
      "Epoch [13/50] Batch [1550/1600] LossG: 6.4931 LossD: 0.0001 L1: 5.4932\n",
      "Epoch [13/50] Batch [1600/1600] LossG: 6.4860 LossD: 0.0001 L1: 5.4860\n",
      "Epoch 13 finished in 462.8s â€” LossG: 6.4860 LossD: 0.0001\n",
      "Epoch [14/50] Batch [50/1600] LossG: 6.5403 LossD: 0.0002 L1: 5.5399\n",
      "Epoch [14/50] Batch [100/1600] LossG: 6.5448 LossD: 0.0001 L1: 5.5447\n",
      "Epoch [14/50] Batch [150/1600] LossG: 6.4764 LossD: 0.0001 L1: 5.4763\n",
      "Epoch [14/50] Batch [200/1600] LossG: 6.4586 LossD: 0.0001 L1: 5.4586\n",
      "Epoch [14/50] Batch [250/1600] LossG: 6.4596 LossD: 0.0001 L1: 5.4596\n",
      "Epoch [14/50] Batch [300/1600] LossG: 6.4773 LossD: 0.0001 L1: 5.4773\n",
      "Epoch [14/50] Batch [350/1600] LossG: 6.4871 LossD: 0.0001 L1: 5.4871\n",
      "Epoch [14/50] Batch [400/1600] LossG: 6.4698 LossD: 0.0001 L1: 5.4699\n",
      "Epoch [14/50] Batch [450/1600] LossG: 6.4717 LossD: 0.0001 L1: 5.4717\n",
      "Epoch [14/50] Batch [500/1600] LossG: 6.4815 LossD: 0.0001 L1: 5.4815\n",
      "Epoch [14/50] Batch [550/1600] LossG: 6.4799 LossD: 0.0001 L1: 5.4799\n",
      "Epoch [14/50] Batch [600/1600] LossG: 6.4718 LossD: 0.0001 L1: 5.4718\n",
      "Epoch [14/50] Batch [650/1600] LossG: 6.4539 LossD: 0.0001 L1: 5.4539\n",
      "Epoch [14/50] Batch [700/1600] LossG: 6.4617 LossD: 0.0001 L1: 5.4617\n",
      "Epoch [14/50] Batch [750/1600] LossG: 6.4565 LossD: 0.0001 L1: 5.4566\n",
      "Epoch [14/50] Batch [800/1600] LossG: 6.4524 LossD: 0.0001 L1: 5.4524\n",
      "Epoch [14/50] Batch [850/1600] LossG: 6.4373 LossD: 0.0001 L1: 5.4373\n",
      "Epoch [14/50] Batch [900/1600] LossG: 6.4270 LossD: 0.0001 L1: 5.4270\n",
      "Epoch [14/50] Batch [950/1600] LossG: 6.4100 LossD: 0.0001 L1: 5.4100\n",
      "Epoch [14/50] Batch [1000/1600] LossG: 6.4015 LossD: 0.0001 L1: 5.4015\n",
      "Epoch [14/50] Batch [1050/1600] LossG: 6.4031 LossD: 0.0001 L1: 5.4031\n",
      "Epoch [14/50] Batch [1100/1600] LossG: 6.3835 LossD: 0.0075 L1: 5.4039\n",
      "Epoch [14/50] Batch [1150/1600] LossG: 6.3919 LossD: 0.0074 L1: 5.4126\n",
      "Epoch [14/50] Batch [1200/1600] LossG: 6.4056 LossD: 0.0071 L1: 5.4254\n",
      "Epoch [14/50] Batch [1250/1600] LossG: 6.4208 LossD: 0.0069 L1: 5.4398\n",
      "Epoch [14/50] Batch [1300/1600] LossG: 6.4289 LossD: 0.0066 L1: 5.4472\n",
      "Epoch [14/50] Batch [1350/1600] LossG: 6.4301 LossD: 0.0064 L1: 5.4477\n",
      "Epoch [14/50] Batch [1400/1600] LossG: 6.4379 LossD: 0.0062 L1: 5.4549\n",
      "Epoch [14/50] Batch [1450/1600] LossG: 6.4480 LossD: 0.0060 L1: 5.4645\n",
      "Epoch [14/50] Batch [1500/1600] LossG: 6.4468 LossD: 0.0058 L1: 5.4627\n",
      "Epoch [14/50] Batch [1550/1600] LossG: 6.4425 LossD: 0.0056 L1: 5.4579\n",
      "Epoch [14/50] Batch [1600/1600] LossG: 6.4471 LossD: 0.0054 L1: 5.4621\n",
      "Epoch 14 finished in 421.4s â€” LossG: 6.4471 LossD: 0.0054\n",
      "Epoch [15/50] Batch [50/1600] LossG: 6.4521 LossD: 0.0004 L1: 5.4520\n",
      "Epoch [15/50] Batch [100/1600] LossG: 6.0746 LossD: 0.1038 L1: 5.3271\n",
      "Epoch [15/50] Batch [150/1600] LossG: 6.1040 LossD: 0.0875 L1: 5.3229\n",
      "Epoch [15/50] Batch [200/1600] LossG: 6.2991 LossD: 0.0667 L1: 5.4629\n",
      "Epoch [15/50] Batch [250/1600] LossG: 6.3048 LossD: 0.0541 L1: 5.4341\n",
      "Epoch [15/50] Batch [300/1600] LossG: 6.3499 LossD: 0.0458 L1: 5.4564\n",
      "Epoch [15/50] Batch [350/1600] LossG: 6.3706 LossD: 0.0396 L1: 5.4600\n",
      "Epoch [15/50] Batch [400/1600] LossG: 6.3897 LossD: 0.0349 L1: 5.4665\n",
      "Epoch [15/50] Batch [450/1600] LossG: 6.3992 LossD: 0.0312 L1: 5.4669\n",
      "Epoch [15/50] Batch [500/1600] LossG: 6.4114 LossD: 0.0296 L1: 5.4734\n",
      "Epoch [15/50] Batch [550/1600] LossG: 6.4293 LossD: 0.0273 L1: 5.4856\n",
      "Epoch [15/50] Batch [600/1600] LossG: 6.4341 LossD: 0.0253 L1: 5.4861\n",
      "Epoch [15/50] Batch [650/1600] LossG: 6.4505 LossD: 0.0234 L1: 5.4981\n",
      "Epoch [15/50] Batch [700/1600] LossG: 6.4416 LossD: 0.0218 L1: 5.4857\n",
      "Epoch [15/50] Batch [750/1600] LossG: 6.4307 LossD: 0.0205 L1: 5.4720\n",
      "Epoch [15/50] Batch [800/1600] LossG: 6.4297 LossD: 0.0193 L1: 5.4685\n",
      "Epoch [15/50] Batch [850/1600] LossG: 6.4256 LossD: 0.0182 L1: 5.4620\n",
      "Epoch [15/50] Batch [900/1600] LossG: 6.4284 LossD: 0.0173 L1: 5.4628\n",
      "Epoch [15/50] Batch [950/1600] LossG: 6.4219 LossD: 0.0165 L1: 5.4545\n",
      "Epoch [15/50] Batch [1000/1600] LossG: 6.4054 LossD: 0.0157 L1: 5.4363\n",
      "Epoch [15/50] Batch [1050/1600] LossG: 6.3999 LossD: 0.0150 L1: 5.4294\n",
      "Epoch [15/50] Batch [1100/1600] LossG: 6.3985 LossD: 0.0143 L1: 5.4267\n",
      "Epoch [15/50] Batch [1150/1600] LossG: 6.3888 LossD: 0.0137 L1: 5.4159\n",
      "Epoch [15/50] Batch [1200/1600] LossG: 6.3837 LossD: 0.0132 L1: 5.4095\n",
      "Epoch [15/50] Batch [1250/1600] LossG: 6.3910 LossD: 0.0127 L1: 5.4158\n",
      "Epoch [15/50] Batch [1300/1600] LossG: 6.3825 LossD: 0.0122 L1: 5.4063\n",
      "Epoch [15/50] Batch [1350/1600] LossG: 6.3795 LossD: 0.0119 L1: 5.4025\n",
      "Epoch [15/50] Batch [1400/1600] LossG: 6.3794 LossD: 0.0115 L1: 5.4016\n",
      "Epoch [15/50] Batch [1450/1600] LossG: 6.3753 LossD: 0.0111 L1: 5.3967\n",
      "Epoch [15/50] Batch [1500/1600] LossG: 6.3815 LossD: 0.0108 L1: 5.4021\n",
      "Epoch [15/50] Batch [1550/1600] LossG: 6.3787 LossD: 0.0107 L1: 5.3992\n",
      "Epoch [15/50] Batch [1600/1600] LossG: 6.3793 LossD: 0.0104 L1: 5.3991\n",
      "Epoch 15 finished in 387.5s â€” LossG: 6.3793 LossD: 0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 128 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 98 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50] Batch [50/1600] LossG: 6.4002 LossD: 0.0004 L1: 5.4001\n",
      "Epoch [16/50] Batch [100/1600] LossG: 6.4039 LossD: 0.0004 L1: 5.4036\n",
      "Epoch [16/50] Batch [150/1600] LossG: 6.3424 LossD: 0.0003 L1: 5.3426\n",
      "Epoch [16/50] Batch [200/1600] LossG: 6.3907 LossD: 0.0003 L1: 5.3910\n",
      "Epoch [16/50] Batch [250/1600] LossG: 6.3373 LossD: 0.0003 L1: 5.3374\n",
      "Epoch [16/50] Batch [300/1600] LossG: 6.3355 LossD: 0.0003 L1: 5.3359\n",
      "Epoch [16/50] Batch [350/1600] LossG: 6.3409 LossD: 0.0017 L1: 5.3427\n",
      "Epoch [16/50] Batch [400/1600] LossG: 6.3550 LossD: 0.0018 L1: 5.3572\n",
      "Epoch [16/50] Batch [450/1600] LossG: 6.3850 LossD: 0.0017 L1: 5.3866\n",
      "Epoch [16/50] Batch [500/1600] LossG: 6.3845 LossD: 0.0015 L1: 5.3859\n",
      "Epoch [16/50] Batch [550/1600] LossG: 6.3841 LossD: 0.0015 L1: 5.3856\n",
      "Epoch [16/50] Batch [600/1600] LossG: 6.3799 LossD: 0.0014 L1: 5.3813\n",
      "Epoch [16/50] Batch [650/1600] LossG: 6.3633 LossD: 0.0013 L1: 5.3645\n",
      "Epoch [16/50] Batch [700/1600] LossG: 6.3624 LossD: 0.0012 L1: 5.3634\n",
      "Epoch [16/50] Batch [750/1600] LossG: 6.3675 LossD: 0.0012 L1: 5.3683\n",
      "Epoch [16/50] Batch [800/1600] LossG: 6.3466 LossD: 0.0011 L1: 5.3473\n",
      "Epoch [16/50] Batch [850/1600] LossG: 6.3457 LossD: 0.0011 L1: 5.3465\n",
      "Epoch [16/50] Batch [900/1600] LossG: 6.3488 LossD: 0.0010 L1: 5.3494\n",
      "Epoch [16/50] Batch [950/1600] LossG: 6.3575 LossD: 0.0010 L1: 5.3581\n",
      "Epoch [16/50] Batch [1000/1600] LossG: 6.3595 LossD: 0.0010 L1: 5.3600\n",
      "Epoch [16/50] Batch [1050/1600] LossG: 6.3568 LossD: 0.0010 L1: 5.3570\n",
      "Epoch [16/50] Batch [1100/1600] LossG: 6.3500 LossD: 0.0009 L1: 5.3503\n",
      "Epoch [16/50] Batch [1150/1600] LossG: 6.3536 LossD: 0.0010 L1: 5.3538\n",
      "Epoch [16/50] Batch [1200/1600] LossG: 6.3507 LossD: 0.0009 L1: 5.3506\n",
      "Epoch [16/50] Batch [1250/1600] LossG: 6.3607 LossD: 0.0009 L1: 5.3605\n",
      "Epoch [16/50] Batch [1300/1600] LossG: 6.3636 LossD: 0.0009 L1: 5.3635\n",
      "Epoch [16/50] Batch [1350/1600] LossG: 6.3601 LossD: 0.0009 L1: 5.3599\n",
      "Epoch [16/50] Batch [1400/1600] LossG: 6.3655 LossD: 0.0008 L1: 5.3653\n",
      "Epoch [16/50] Batch [1450/1600] LossG: 6.3758 LossD: 0.0019 L1: 5.3775\n",
      "Epoch [16/50] Batch [1500/1600] LossG: 6.3851 LossD: 0.0020 L1: 5.3862\n",
      "Epoch [16/50] Batch [1550/1600] LossG: 6.3838 LossD: 0.0020 L1: 5.3843\n",
      "Epoch [16/50] Batch [1600/1600] LossG: 6.3816 LossD: 0.0020 L1: 5.3819\n",
      "Epoch 16 finished in 382.7s â€” LossG: 6.3816 LossD: 0.0020\n",
      "Epoch [17/50] Batch [50/1600] LossG: 6.1521 LossD: 0.0012 L1: 5.1471\n",
      "Epoch [17/50] Batch [100/1600] LossG: 6.4260 LossD: 0.0016 L1: 5.4221\n",
      "Epoch [17/50] Batch [150/1600] LossG: 6.3918 LossD: 0.0018 L1: 5.3870\n",
      "Epoch [17/50] Batch [200/1600] LossG: 6.3391 LossD: 0.0015 L1: 5.3348\n",
      "Epoch [17/50] Batch [250/1600] LossG: 6.3069 LossD: 0.0013 L1: 5.3034\n",
      "Epoch [17/50] Batch [300/1600] LossG: 6.3156 LossD: 0.0011 L1: 5.3127\n",
      "Epoch [17/50] Batch [350/1600] LossG: 6.3032 LossD: 0.0010 L1: 5.3007\n",
      "Epoch [17/50] Batch [400/1600] LossG: 6.2768 LossD: 0.0009 L1: 5.2744\n",
      "Epoch [17/50] Batch [450/1600] LossG: 6.2685 LossD: 0.0008 L1: 5.2663\n",
      "Epoch [17/50] Batch [500/1600] LossG: 6.2697 LossD: 0.0008 L1: 5.2677\n",
      "Epoch [17/50] Batch [550/1600] LossG: 6.2688 LossD: 0.0007 L1: 5.2669\n",
      "Epoch [17/50] Batch [600/1600] LossG: 6.2735 LossD: 0.0007 L1: 5.2718\n",
      "Epoch [17/50] Batch [650/1600] LossG: 6.2960 LossD: 0.0007 L1: 5.2943\n",
      "Epoch [17/50] Batch [700/1600] LossG: 6.2945 LossD: 0.0007 L1: 5.2928\n",
      "Epoch [17/50] Batch [750/1600] LossG: 6.2840 LossD: 0.0007 L1: 5.2822\n",
      "Epoch [17/50] Batch [800/1600] LossG: 6.2905 LossD: 0.0007 L1: 5.2887\n",
      "Epoch [17/50] Batch [850/1600] LossG: 6.2882 LossD: 0.0006 L1: 5.2864\n",
      "Epoch [17/50] Batch [900/1600] LossG: 6.2839 LossD: 0.0006 L1: 5.2822\n",
      "Epoch [17/50] Batch [950/1600] LossG: 6.2835 LossD: 0.0025 L1: 5.2835\n",
      "Epoch [17/50] Batch [1000/1600] LossG: 6.2868 LossD: 0.0026 L1: 5.2862\n",
      "Epoch [17/50] Batch [1050/1600] LossG: 6.2967 LossD: 0.0025 L1: 5.2961\n",
      "Epoch [17/50] Batch [1100/1600] LossG: 6.2947 LossD: 0.0025 L1: 5.2940\n",
      "Epoch [17/50] Batch [1150/1600] LossG: 6.2921 LossD: 0.0024 L1: 5.2914\n",
      "Epoch [17/50] Batch [1200/1600] LossG: 6.2887 LossD: 0.0024 L1: 5.2882\n",
      "Epoch [17/50] Batch [1250/1600] LossG: 6.2880 LossD: 0.0023 L1: 5.2873\n",
      "Epoch [17/50] Batch [1300/1600] LossG: 6.2940 LossD: 0.0022 L1: 5.2930\n",
      "Epoch [17/50] Batch [1350/1600] LossG: 6.2906 LossD: 0.0022 L1: 5.2895\n",
      "Epoch [17/50] Batch [1400/1600] LossG: 6.2917 LossD: 0.0021 L1: 5.2908\n",
      "Epoch [17/50] Batch [1450/1600] LossG: 6.2938 LossD: 0.0021 L1: 5.2929\n",
      "Epoch [17/50] Batch [1500/1600] LossG: 6.2888 LossD: 0.0021 L1: 5.2878\n",
      "Epoch [17/50] Batch [1550/1600] LossG: 6.2907 LossD: 0.0020 L1: 5.2897\n",
      "Epoch [17/50] Batch [1600/1600] LossG: 6.2872 LossD: 0.0020 L1: 5.2862\n",
      "Epoch 17 finished in 808.1s â€” LossG: 6.2872 LossD: 0.0020\n",
      "Epoch [18/50] Batch [50/1600] LossG: 6.2106 LossD: 0.0005 L1: 5.2101\n",
      "Epoch [18/50] Batch [100/1600] LossG: 6.2565 LossD: 0.0006 L1: 5.2559\n",
      "Epoch [18/50] Batch [150/1600] LossG: 6.2191 LossD: 0.0006 L1: 5.2190\n",
      "Epoch [18/50] Batch [200/1600] LossG: 6.2215 LossD: 0.0006 L1: 5.2205\n",
      "Epoch [18/50] Batch [250/1600] LossG: 6.2310 LossD: 0.0006 L1: 5.2302\n",
      "Epoch [18/50] Batch [300/1600] LossG: 6.1985 LossD: 0.0011 L1: 5.1993\n",
      "Epoch [18/50] Batch [350/1600] LossG: 6.2119 LossD: 0.0013 L1: 5.2126\n",
      "Epoch [18/50] Batch [400/1600] LossG: 6.2512 LossD: 0.0013 L1: 5.2520\n",
      "Epoch [18/50] Batch [450/1600] LossG: 6.2526 LossD: 0.0013 L1: 5.2532\n",
      "Epoch [18/50] Batch [500/1600] LossG: 6.2755 LossD: 0.0012 L1: 5.2759\n",
      "Epoch [18/50] Batch [550/1600] LossG: 6.2652 LossD: 0.0012 L1: 5.2648\n",
      "Epoch [18/50] Batch [600/1600] LossG: 6.2525 LossD: 0.0012 L1: 5.2518\n",
      "Epoch [18/50] Batch [650/1600] LossG: 6.2562 LossD: 0.0011 L1: 5.2554\n",
      "Epoch [18/50] Batch [700/1600] LossG: 6.2474 LossD: 0.0010 L1: 5.2465\n",
      "Epoch [18/50] Batch [750/1600] LossG: 6.2580 LossD: 0.0010 L1: 5.2572\n",
      "Epoch [18/50] Batch [800/1600] LossG: 6.2565 LossD: 0.0009 L1: 5.2556\n",
      "Epoch [18/50] Batch [850/1600] LossG: 6.2516 LossD: 0.0009 L1: 5.2508\n",
      "Epoch [18/50] Batch [900/1600] LossG: 6.2488 LossD: 0.0008 L1: 5.2481\n",
      "Epoch [18/50] Batch [950/1600] LossG: 6.2522 LossD: 0.0008 L1: 5.2515\n",
      "Epoch [18/50] Batch [1000/1600] LossG: 6.2467 LossD: 0.0008 L1: 5.2461\n",
      "Epoch [18/50] Batch [1050/1600] LossG: 6.2453 LossD: 0.0008 L1: 5.2446\n",
      "Epoch [18/50] Batch [1100/1600] LossG: 6.2517 LossD: 0.0007 L1: 5.2510\n",
      "Epoch [18/50] Batch [1150/1600] LossG: 6.2459 LossD: 0.0007 L1: 5.2453\n",
      "Epoch [18/50] Batch [1200/1600] LossG: 6.2473 LossD: 0.0008 L1: 5.2467\n",
      "Epoch [18/50] Batch [1250/1600] LossG: 6.2474 LossD: 0.0008 L1: 5.2465\n",
      "Epoch [18/50] Batch [1300/1600] LossG: 6.2531 LossD: 0.0008 L1: 5.2523\n",
      "Epoch [18/50] Batch [1350/1600] LossG: 6.2619 LossD: 0.0008 L1: 5.2610\n",
      "Epoch [18/50] Batch [1400/1600] LossG: 6.2570 LossD: 0.0007 L1: 5.2560\n",
      "Epoch [18/50] Batch [1450/1600] LossG: 6.2621 LossD: 0.0007 L1: 5.2611\n",
      "Epoch [18/50] Batch [1500/1600] LossG: 6.2631 LossD: 0.0007 L1: 5.2621\n",
      "Epoch [18/50] Batch [1550/1600] LossG: 6.2623 LossD: 0.0007 L1: 5.2613\n",
      "Epoch [18/50] Batch [1600/1600] LossG: 6.2670 LossD: 0.0007 L1: 5.2661\n",
      "Epoch 18 finished in 518.5s â€” LossG: 6.2670 LossD: 0.0007\n",
      "Epoch [19/50] Batch [50/1600] LossG: 6.2793 LossD: 0.0002 L1: 5.2789\n",
      "Epoch [19/50] Batch [100/1600] LossG: 6.1511 LossD: 0.0001 L1: 5.1508\n",
      "Epoch [19/50] Batch [150/1600] LossG: 6.1031 LossD: 0.0001 L1: 5.1028\n",
      "Epoch [19/50] Batch [200/1600] LossG: 6.1283 LossD: 0.0001 L1: 5.1281\n",
      "Epoch [19/50] Batch [250/1600] LossG: 6.1762 LossD: 0.0001 L1: 5.1760\n",
      "Epoch [19/50] Batch [300/1600] LossG: 6.1962 LossD: 0.0001 L1: 5.1960\n",
      "Epoch [19/50] Batch [350/1600] LossG: 6.1806 LossD: 0.0001 L1: 5.1804\n",
      "Epoch [19/50] Batch [400/1600] LossG: 6.1766 LossD: 0.0001 L1: 5.1764\n",
      "Epoch [19/50] Batch [450/1600] LossG: 6.1721 LossD: 0.0001 L1: 5.1719\n",
      "Epoch [19/50] Batch [500/1600] LossG: 6.1960 LossD: 0.0002 L1: 5.1955\n",
      "Epoch [19/50] Batch [550/1600] LossG: 6.1897 LossD: 0.0004 L1: 5.1889\n",
      "Epoch [19/50] Batch [600/1600] LossG: 6.1949 LossD: 0.0015 L1: 5.1940\n",
      "Epoch [19/50] Batch [650/1600] LossG: 6.1923 LossD: 0.0015 L1: 5.1906\n",
      "Epoch [19/50] Batch [700/1600] LossG: 6.2303 LossD: 0.0017 L1: 5.2292\n",
      "Epoch [19/50] Batch [750/1600] LossG: 6.2489 LossD: 0.0017 L1: 5.2476\n",
      "Epoch [19/50] Batch [800/1600] LossG: 6.2485 LossD: 0.0016 L1: 5.2468\n",
      "Epoch [19/50] Batch [850/1600] LossG: 6.2507 LossD: 0.0015 L1: 5.2491\n",
      "Epoch [19/50] Batch [900/1600] LossG: 6.2535 LossD: 0.0015 L1: 5.2520\n",
      "Epoch [19/50] Batch [950/1600] LossG: 6.2576 LossD: 0.0014 L1: 5.2560\n",
      "Epoch [19/50] Batch [1000/1600] LossG: 6.2554 LossD: 0.0014 L1: 5.2539\n",
      "Epoch [19/50] Batch [1050/1600] LossG: 6.2390 LossD: 0.0013 L1: 5.2376\n",
      "Epoch [19/50] Batch [1100/1600] LossG: 6.2383 LossD: 0.0012 L1: 5.2369\n",
      "Epoch [19/50] Batch [1150/1600] LossG: 6.2400 LossD: 0.0012 L1: 5.2387\n",
      "Epoch [19/50] Batch [1200/1600] LossG: 6.2426 LossD: 0.0012 L1: 5.2413\n",
      "Epoch [19/50] Batch [1250/1600] LossG: 6.2433 LossD: 0.0011 L1: 5.2421\n",
      "Epoch [19/50] Batch [1300/1600] LossG: 6.2414 LossD: 0.0011 L1: 5.2402\n",
      "Epoch [19/50] Batch [1350/1600] LossG: 6.2384 LossD: 0.0010 L1: 5.2373\n",
      "Epoch [19/50] Batch [1400/1600] LossG: 6.2402 LossD: 0.0010 L1: 5.2390\n",
      "Epoch [19/50] Batch [1450/1600] LossG: 6.2366 LossD: 0.0010 L1: 5.2355\n",
      "Epoch [19/50] Batch [1500/1600] LossG: 6.2348 LossD: 0.0010 L1: 5.2338\n",
      "Epoch [19/50] Batch [1550/1600] LossG: 6.2366 LossD: 0.0010 L1: 5.2356\n",
      "Epoch [19/50] Batch [1600/1600] LossG: 6.2375 LossD: 0.0010 L1: 5.2362\n",
      "Epoch 19 finished in 368.0s â€” LossG: 6.2375 LossD: 0.0010\n",
      "Epoch [20/50] Batch [50/1600] LossG: 6.1066 LossD: 0.0003 L1: 5.1032\n",
      "Epoch [20/50] Batch [100/1600] LossG: 6.1500 LossD: 0.0002 L1: 5.1476\n",
      "Epoch [20/50] Batch [150/1600] LossG: 6.1691 LossD: 0.0002 L1: 5.1673\n",
      "Epoch [20/50] Batch [200/1600] LossG: 6.1625 LossD: 0.0003 L1: 5.1607\n",
      "Epoch [20/50] Batch [250/1600] LossG: 6.1508 LossD: 0.0003 L1: 5.1492\n",
      "Epoch [20/50] Batch [300/1600] LossG: 6.1573 LossD: 0.0002 L1: 5.1558\n",
      "Epoch [20/50] Batch [350/1600] LossG: 6.1276 LossD: 0.0002 L1: 5.1262\n",
      "Epoch [20/50] Batch [400/1600] LossG: 6.1408 LossD: 0.0002 L1: 5.1396\n",
      "Epoch [20/50] Batch [450/1600] LossG: 6.1494 LossD: 0.0002 L1: 5.1483\n",
      "Epoch [20/50] Batch [500/1600] LossG: 6.1749 LossD: 0.0002 L1: 5.1739\n",
      "Epoch [20/50] Batch [550/1600] LossG: 6.1781 LossD: 0.0002 L1: 5.1772\n",
      "Epoch [20/50] Batch [600/1600] LossG: 6.1882 LossD: 0.0002 L1: 5.1874\n",
      "Epoch [20/50] Batch [650/1600] LossG: 6.1886 LossD: 0.0001 L1: 5.1878\n",
      "Epoch [20/50] Batch [700/1600] LossG: 6.1694 LossD: 0.0117 L1: 5.1957\n",
      "Epoch [20/50] Batch [750/1600] LossG: 6.1456 LossD: 0.0214 L1: 5.1994\n",
      "Epoch [20/50] Batch [800/1600] LossG: 6.1491 LossD: 0.0211 L1: 5.2019\n",
      "Epoch [20/50] Batch [850/1600] LossG: 6.1713 LossD: 0.0200 L1: 5.2201\n",
      "Epoch [20/50] Batch [900/1600] LossG: 6.1804 LossD: 0.0191 L1: 5.2264\n",
      "Epoch [20/50] Batch [950/1600] LossG: 6.1847 LossD: 0.0183 L1: 5.2276\n",
      "Epoch [20/50] Batch [1000/1600] LossG: 6.1862 LossD: 0.0175 L1: 5.2266\n",
      "Epoch [20/50] Batch [1050/1600] LossG: 6.1904 LossD: 0.0168 L1: 5.2285\n",
      "Epoch [20/50] Batch [1100/1600] LossG: 6.1839 LossD: 0.0161 L1: 5.2203\n",
      "Epoch [20/50] Batch [1150/1600] LossG: 6.1880 LossD: 0.0155 L1: 5.2227\n",
      "Epoch [20/50] Batch [1200/1600] LossG: 6.1763 LossD: 0.0149 L1: 5.2095\n",
      "Epoch [20/50] Batch [1250/1600] LossG: 6.1755 LossD: 0.0143 L1: 5.2074\n",
      "Epoch [20/50] Batch [1300/1600] LossG: 6.1764 LossD: 0.0138 L1: 5.2068\n",
      "Epoch [20/50] Batch [1350/1600] LossG: 6.1741 LossD: 0.0134 L1: 5.2033\n",
      "Epoch [20/50] Batch [1400/1600] LossG: 6.1824 LossD: 0.0129 L1: 5.2105\n",
      "Epoch [20/50] Batch [1450/1600] LossG: 6.1825 LossD: 0.0125 L1: 5.2097\n",
      "Epoch [20/50] Batch [1500/1600] LossG: 6.1779 LossD: 0.0121 L1: 5.2041\n",
      "Epoch [20/50] Batch [1550/1600] LossG: 6.1840 LossD: 0.0117 L1: 5.2094\n",
      "Epoch [20/50] Batch [1600/1600] LossG: 6.1883 LossD: 0.0114 L1: 5.2129\n",
      "Epoch 20 finished in 346.2s â€” LossG: 6.1883 LossD: 0.0114\n",
      "Epoch [21/50] Batch [50/1600] LossG: 6.0898 LossD: 0.0007 L1: 5.0856\n",
      "Epoch [21/50] Batch [100/1600] LossG: 6.0888 LossD: 0.0009 L1: 5.0856\n",
      "Epoch [21/50] Batch [150/1600] LossG: 6.1095 LossD: 0.0010 L1: 5.1052\n",
      "Epoch [21/50] Batch [200/1600] LossG: 6.1456 LossD: 0.0010 L1: 5.1407\n",
      "Epoch [21/50] Batch [250/1600] LossG: 6.1011 LossD: 0.0009 L1: 5.0977\n",
      "Epoch [21/50] Batch [300/1600] LossG: 6.0776 LossD: 0.0010 L1: 5.0740\n",
      "Epoch [21/50] Batch [350/1600] LossG: 6.1056 LossD: 0.0009 L1: 5.1023\n",
      "Epoch [21/50] Batch [400/1600] LossG: 6.1062 LossD: 0.0015 L1: 5.1051\n",
      "Epoch [21/50] Batch [450/1600] LossG: 6.1141 LossD: 0.0018 L1: 5.1121\n",
      "Epoch [21/50] Batch [500/1600] LossG: 6.1132 LossD: 0.0018 L1: 5.1110\n",
      "Epoch [21/50] Batch [550/1600] LossG: 6.1145 LossD: 0.0016 L1: 5.1128\n",
      "Epoch [21/50] Batch [600/1600] LossG: 6.1316 LossD: 0.0016 L1: 5.1296\n",
      "Epoch [21/50] Batch [650/1600] LossG: 6.1410 LossD: 0.0015 L1: 5.1390\n",
      "Epoch [21/50] Batch [700/1600] LossG: 6.1545 LossD: 0.0014 L1: 5.1526\n",
      "Epoch [21/50] Batch [750/1600] LossG: 6.1528 LossD: 0.0015 L1: 5.1508\n",
      "Epoch [21/50] Batch [800/1600] LossG: 6.1579 LossD: 0.0023 L1: 5.1555\n",
      "Epoch [21/50] Batch [850/1600] LossG: 6.1689 LossD: 0.0023 L1: 5.1663\n",
      "Epoch [21/50] Batch [900/1600] LossG: 6.1598 LossD: 0.0023 L1: 5.1570\n",
      "Epoch [21/50] Batch [950/1600] LossG: 6.1616 LossD: 0.0022 L1: 5.1589\n",
      "Epoch [21/50] Batch [1000/1600] LossG: 6.1800 LossD: 0.0022 L1: 5.1767\n",
      "Epoch [21/50] Batch [1050/1600] LossG: 6.1739 LossD: 0.0021 L1: 5.1708\n",
      "Epoch [21/50] Batch [1100/1600] LossG: 6.1652 LossD: 0.0020 L1: 5.1622\n",
      "Epoch [21/50] Batch [1150/1600] LossG: 6.1615 LossD: 0.0019 L1: 5.1586\n",
      "Epoch [21/50] Batch [1200/1600] LossG: 6.1605 LossD: 0.0019 L1: 5.1577\n",
      "Epoch [21/50] Batch [1250/1600] LossG: 6.1620 LossD: 0.0018 L1: 5.1593\n",
      "Epoch [21/50] Batch [1300/1600] LossG: 6.1603 LossD: 0.0018 L1: 5.1577\n",
      "Epoch [21/50] Batch [1350/1600] LossG: 6.1561 LossD: 0.0017 L1: 5.1536\n",
      "Epoch [21/50] Batch [1400/1600] LossG: 6.1577 LossD: 0.0017 L1: 5.1554\n",
      "Epoch [21/50] Batch [1450/1600] LossG: 6.1569 LossD: 0.0016 L1: 5.1545\n",
      "Epoch [21/50] Batch [1500/1600] LossG: 6.1570 LossD: 0.0016 L1: 5.1547\n",
      "Epoch [21/50] Batch [1550/1600] LossG: 6.1603 LossD: 0.0015 L1: 5.1581\n",
      "Epoch [21/50] Batch [1600/1600] LossG: 6.1668 LossD: 0.0015 L1: 5.1647\n",
      "Epoch 21 finished in 348.0s â€” LossG: 6.1668 LossD: 0.0015\n",
      "Epoch [22/50] Batch [50/1600] LossG: 6.2760 LossD: 0.0001 L1: 5.2762\n",
      "Epoch [22/50] Batch [100/1600] LossG: 6.2713 LossD: 0.0001 L1: 5.2715\n",
      "Epoch [22/50] Batch [150/1600] LossG: 6.1982 LossD: 0.0001 L1: 5.1984\n",
      "Epoch [22/50] Batch [200/1600] LossG: 6.1940 LossD: 0.0001 L1: 5.1942\n",
      "Epoch [22/50] Batch [250/1600] LossG: 6.1725 LossD: 0.0001 L1: 5.1726\n",
      "Epoch [22/50] Batch [300/1600] LossG: 6.1824 LossD: 0.0001 L1: 5.1825\n",
      "Epoch [22/50] Batch [350/1600] LossG: 6.2001 LossD: 0.0002 L1: 5.1995\n",
      "Epoch [22/50] Batch [400/1600] LossG: 6.1998 LossD: 0.0002 L1: 5.1994\n",
      "Epoch [22/50] Batch [450/1600] LossG: 6.2030 LossD: 0.0006 L1: 5.2032\n",
      "Epoch [22/50] Batch [500/1600] LossG: 6.2125 LossD: 0.0023 L1: 5.2109\n",
      "Epoch [22/50] Batch [550/1600] LossG: 6.2233 LossD: 0.0022 L1: 5.2214\n",
      "Epoch [22/50] Batch [600/1600] LossG: 6.2163 LossD: 0.0021 L1: 5.2145\n",
      "Epoch [22/50] Batch [650/1600] LossG: 6.2058 LossD: 0.0020 L1: 5.2040\n",
      "Epoch [22/50] Batch [700/1600] LossG: 6.2028 LossD: 0.0019 L1: 5.2012\n",
      "Epoch [22/50] Batch [750/1600] LossG: 6.2007 LossD: 0.0018 L1: 5.1993\n",
      "Epoch [22/50] Batch [800/1600] LossG: 6.2077 LossD: 0.0017 L1: 5.2063\n",
      "Epoch [22/50] Batch [850/1600] LossG: 6.2023 LossD: 0.0016 L1: 5.2010\n",
      "Epoch [22/50] Batch [900/1600] LossG: 6.1853 LossD: 0.0015 L1: 5.1841\n",
      "Epoch [22/50] Batch [950/1600] LossG: 6.1771 LossD: 0.0014 L1: 5.1758\n",
      "Epoch [22/50] Batch [1000/1600] LossG: 6.1654 LossD: 0.0014 L1: 5.1643\n",
      "Epoch [22/50] Batch [1050/1600] LossG: 6.1639 LossD: 0.0013 L1: 5.1628\n",
      "Epoch [22/50] Batch [1100/1600] LossG: 6.1660 LossD: 0.0013 L1: 5.1650\n",
      "Epoch [22/50] Batch [1150/1600] LossG: 6.1773 LossD: 0.0012 L1: 5.1762\n",
      "Epoch [22/50] Batch [1200/1600] LossG: 6.1715 LossD: 0.0012 L1: 5.1704\n",
      "Epoch [22/50] Batch [1250/1600] LossG: 6.1688 LossD: 0.0011 L1: 5.1678\n",
      "Epoch [22/50] Batch [1300/1600] LossG: 6.1617 LossD: 0.0011 L1: 5.1608\n",
      "Epoch [22/50] Batch [1350/1600] LossG: 6.1542 LossD: 0.0011 L1: 5.1533\n",
      "Epoch [22/50] Batch [1400/1600] LossG: 6.1537 LossD: 0.0010 L1: 5.1527\n",
      "Epoch [22/50] Batch [1450/1600] LossG: 6.1538 LossD: 0.0010 L1: 5.1529\n",
      "Epoch [22/50] Batch [1500/1600] LossG: 6.1488 LossD: 0.0010 L1: 5.1480\n",
      "Epoch [22/50] Batch [1550/1600] LossG: 6.1458 LossD: 0.0009 L1: 5.1450\n",
      "Epoch [22/50] Batch [1600/1600] LossG: 6.1470 LossD: 0.0009 L1: 5.1462\n",
      "Epoch 22 finished in 349.5s â€” LossG: 6.1470 LossD: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 27 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50] Batch [50/1600] LossG: 6.0513 LossD: 0.0005 L1: 5.0505\n",
      "Epoch [23/50] Batch [100/1600] LossG: 6.1825 LossD: 0.0005 L1: 5.1806\n",
      "Epoch [23/50] Batch [150/1600] LossG: 6.2438 LossD: 0.0004 L1: 5.2423\n",
      "Epoch [23/50] Batch [200/1600] LossG: 6.2191 LossD: 0.0003 L1: 5.2181\n",
      "Epoch [23/50] Batch [250/1600] LossG: 6.1842 LossD: 0.0003 L1: 5.1834\n",
      "Epoch [23/50] Batch [300/1600] LossG: 6.1797 LossD: 0.0003 L1: 5.1791\n",
      "Epoch [23/50] Batch [350/1600] LossG: 6.1841 LossD: 0.0003 L1: 5.1831\n",
      "Epoch [23/50] Batch [400/1600] LossG: 6.1855 LossD: 0.0004 L1: 5.1841\n",
      "Epoch [23/50] Batch [450/1600] LossG: 6.1721 LossD: 0.0003 L1: 5.1709\n",
      "Epoch [23/50] Batch [500/1600] LossG: 6.1801 LossD: 0.0003 L1: 5.1790\n",
      "Epoch [23/50] Batch [550/1600] LossG: 6.1760 LossD: 0.0003 L1: 5.1751\n",
      "Epoch [23/50] Batch [600/1600] LossG: 6.1661 LossD: 0.0003 L1: 5.1651\n",
      "Epoch [23/50] Batch [650/1600] LossG: 6.1552 LossD: 0.0003 L1: 5.1542\n",
      "Epoch [23/50] Batch [700/1600] LossG: 6.1428 LossD: 0.0003 L1: 5.1418\n",
      "Epoch [23/50] Batch [750/1600] LossG: 6.1439 LossD: 0.0003 L1: 5.1430\n",
      "Epoch [23/50] Batch [800/1600] LossG: 6.1476 LossD: 0.0003 L1: 5.1468\n",
      "Epoch [23/50] Batch [850/1600] LossG: 6.1531 LossD: 0.0003 L1: 5.1523\n",
      "Epoch [23/50] Batch [900/1600] LossG: 6.1526 LossD: 0.0003 L1: 5.1519\n",
      "Epoch [23/50] Batch [950/1600] LossG: 6.1496 LossD: 0.0002 L1: 5.1489\n",
      "Epoch [23/50] Batch [1000/1600] LossG: 6.1534 LossD: 0.0002 L1: 5.1528\n",
      "Epoch [23/50] Batch [1050/1600] LossG: 6.1500 LossD: 0.0002 L1: 5.1494\n",
      "Epoch [23/50] Batch [1100/1600] LossG: 6.1373 LossD: 0.0002 L1: 5.1367\n",
      "Epoch [23/50] Batch [1150/1600] LossG: 6.1446 LossD: 0.0002 L1: 5.1440\n",
      "Epoch [23/50] Batch [1200/1600] LossG: 6.1341 LossD: 0.0002 L1: 5.1336\n",
      "Epoch [23/50] Batch [1250/1600] LossG: 6.1354 LossD: 0.0002 L1: 5.1349\n",
      "Epoch [23/50] Batch [1300/1600] LossG: 6.1293 LossD: 0.0002 L1: 5.1288\n",
      "Epoch [23/50] Batch [1350/1600] LossG: 6.1312 LossD: 0.0002 L1: 5.1307\n",
      "Epoch [23/50] Batch [1400/1600] LossG: 6.1238 LossD: 0.0002 L1: 5.1233\n",
      "Epoch [23/50] Batch [1450/1600] LossG: 6.1220 LossD: 0.0002 L1: 5.1215\n",
      "Epoch [23/50] Batch [1500/1600] LossG: 6.1195 LossD: 0.0002 L1: 5.1191\n",
      "Epoch [23/50] Batch [1550/1600] LossG: 6.1209 LossD: 0.0002 L1: 5.1205\n",
      "Epoch [23/50] Batch [1600/1600] LossG: 6.1223 LossD: 0.0002 L1: 5.1216\n",
      "Epoch 23 finished in 350.6s â€” LossG: 6.1223 LossD: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 38 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50] Batch [50/1600] LossG: 5.9929 LossD: 0.0003 L1: 4.9915\n",
      "Epoch [24/50] Batch [100/1600] LossG: 6.0123 LossD: 0.0028 L1: 5.0039\n",
      "Epoch [24/50] Batch [150/1600] LossG: 6.0199 LossD: 0.0020 L1: 5.0130\n",
      "Epoch [24/50] Batch [200/1600] LossG: 6.0915 LossD: 0.0017 L1: 5.0856\n",
      "Epoch [24/50] Batch [250/1600] LossG: 6.1296 LossD: 0.0014 L1: 5.1244\n",
      "Epoch [24/50] Batch [300/1600] LossG: 6.1264 LossD: 0.0013 L1: 5.1216\n",
      "Epoch [24/50] Batch [350/1600] LossG: 6.1563 LossD: 0.0022 L1: 5.1512\n",
      "Epoch [24/50] Batch [400/1600] LossG: 6.1549 LossD: 0.0020 L1: 5.1492\n",
      "Epoch [24/50] Batch [450/1600] LossG: 6.1599 LossD: 0.0019 L1: 5.1544\n",
      "Epoch [24/50] Batch [500/1600] LossG: 6.1552 LossD: 0.0018 L1: 5.1501\n",
      "Epoch [24/50] Batch [550/1600] LossG: 6.1548 LossD: 0.0016 L1: 5.1500\n",
      "Epoch [24/50] Batch [600/1600] LossG: 6.1552 LossD: 0.0015 L1: 5.1508\n",
      "Epoch [24/50] Batch [650/1600] LossG: 6.1540 LossD: 0.0014 L1: 5.1499\n",
      "Epoch [24/50] Batch [700/1600] LossG: 6.1433 LossD: 0.0013 L1: 5.1394\n",
      "Epoch [24/50] Batch [750/1600] LossG: 6.1385 LossD: 0.0012 L1: 5.1348\n",
      "Epoch [24/50] Batch [800/1600] LossG: 6.1347 LossD: 0.0012 L1: 5.1312\n",
      "Epoch [24/50] Batch [850/1600] LossG: 6.1472 LossD: 0.0011 L1: 5.1439\n",
      "Epoch [24/50] Batch [900/1600] LossG: 6.1464 LossD: 0.0010 L1: 5.1434\n",
      "Epoch [24/50] Batch [950/1600] LossG: 6.1440 LossD: 0.0010 L1: 5.1412\n",
      "Epoch [24/50] Batch [1000/1600] LossG: 6.1329 LossD: 0.0009 L1: 5.1301\n",
      "Epoch [24/50] Batch [1050/1600] LossG: 6.1388 LossD: 0.0009 L1: 5.1362\n",
      "Epoch [24/50] Batch [1100/1600] LossG: 6.1398 LossD: 0.0009 L1: 5.1373\n",
      "Epoch [24/50] Batch [1150/1600] LossG: 6.1369 LossD: 0.0008 L1: 5.1344\n",
      "Epoch [24/50] Batch [1200/1600] LossG: 6.1352 LossD: 0.0008 L1: 5.1329\n",
      "Epoch [24/50] Batch [1250/1600] LossG: 6.1400 LossD: 0.0008 L1: 5.1377\n",
      "Epoch [24/50] Batch [1300/1600] LossG: 6.1355 LossD: 0.0007 L1: 5.1333\n",
      "Epoch [24/50] Batch [1350/1600] LossG: 6.1328 LossD: 0.0007 L1: 5.1306\n",
      "Epoch [24/50] Batch [1400/1600] LossG: 6.1343 LossD: 0.0007 L1: 5.1321\n",
      "Epoch [24/50] Batch [1450/1600] LossG: 6.1273 LossD: 0.0007 L1: 5.1252\n",
      "Epoch [24/50] Batch [1500/1600] LossG: 6.1230 LossD: 0.0007 L1: 5.1209\n",
      "Epoch [24/50] Batch [1550/1600] LossG: 6.1201 LossD: 0.0007 L1: 5.1182\n",
      "Epoch [24/50] Batch [1600/1600] LossG: 6.1163 LossD: 0.0006 L1: 5.1144\n",
      "Epoch 24 finished in 354.6s â€” LossG: 6.1163 LossD: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 20 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50] Batch [50/1600] LossG: 6.0604 LossD: 0.0001 L1: 5.0600\n",
      "Epoch [25/50] Batch [100/1600] LossG: 6.0703 LossD: 0.0002 L1: 5.0693\n",
      "Epoch [25/50] Batch [150/1600] LossG: 6.1538 LossD: 0.0002 L1: 5.1530\n",
      "Epoch [25/50] Batch [200/1600] LossG: 6.1544 LossD: 0.0001 L1: 5.1536\n",
      "Epoch [25/50] Batch [250/1600] LossG: 6.1157 LossD: 0.0001 L1: 5.1151\n",
      "Epoch [25/50] Batch [300/1600] LossG: 6.0893 LossD: 0.0001 L1: 5.0888\n",
      "Epoch [25/50] Batch [350/1600] LossG: 6.0818 LossD: 0.0001 L1: 5.0815\n",
      "Epoch [25/50] Batch [400/1600] LossG: 6.1076 LossD: 0.0001 L1: 5.1072\n",
      "Epoch [25/50] Batch [450/1600] LossG: 6.1064 LossD: 0.0003 L1: 5.1058\n",
      "Epoch [25/50] Batch [500/1600] LossG: 6.1317 LossD: 0.0003 L1: 5.1311\n",
      "Epoch [25/50] Batch [550/1600] LossG: 6.1231 LossD: 0.0003 L1: 5.1225\n",
      "Epoch [25/50] Batch [600/1600] LossG: 6.1060 LossD: 0.0003 L1: 5.1055\n",
      "Epoch [25/50] Batch [650/1600] LossG: 6.1007 LossD: 0.0003 L1: 5.0998\n",
      "Epoch [25/50] Batch [700/1600] LossG: 6.1005 LossD: 0.0003 L1: 5.0994\n",
      "Epoch [25/50] Batch [750/1600] LossG: 6.0995 LossD: 0.0003 L1: 5.0984\n",
      "Epoch [25/50] Batch [800/1600] LossG: 6.0869 LossD: 0.0003 L1: 5.0858\n",
      "Epoch [25/50] Batch [850/1600] LossG: 6.0849 LossD: 0.0003 L1: 5.0839\n",
      "Epoch [25/50] Batch [900/1600] LossG: 6.0953 LossD: 0.0009 L1: 5.0949\n",
      "Epoch [25/50] Batch [950/1600] LossG: 6.0961 LossD: 0.0009 L1: 5.0954\n",
      "Epoch [25/50] Batch [1000/1600] LossG: 6.0929 LossD: 0.0009 L1: 5.0922\n",
      "Epoch [25/50] Batch [1050/1600] LossG: 6.0987 LossD: 0.0009 L1: 5.0980\n",
      "Epoch [25/50] Batch [1100/1600] LossG: 6.0872 LossD: 0.0009 L1: 5.0864\n",
      "Epoch [25/50] Batch [1150/1600] LossG: 6.0964 LossD: 0.0009 L1: 5.0956\n",
      "Epoch [25/50] Batch [1200/1600] LossG: 6.0941 LossD: 0.0008 L1: 5.0933\n",
      "Epoch [25/50] Batch [1250/1600] LossG: 6.0907 LossD: 0.0008 L1: 5.0899\n",
      "Epoch [25/50] Batch [1300/1600] LossG: 6.0896 LossD: 0.0008 L1: 5.0887\n",
      "Epoch [25/50] Batch [1350/1600] LossG: 6.0914 LossD: 0.0008 L1: 5.0906\n",
      "Epoch [25/50] Batch [1400/1600] LossG: 6.0876 LossD: 0.0007 L1: 5.0868\n",
      "Epoch [25/50] Batch [1450/1600] LossG: 6.0852 LossD: 0.0007 L1: 5.0845\n",
      "Epoch [25/50] Batch [1500/1600] LossG: 6.0871 LossD: 0.0007 L1: 5.0864\n",
      "Epoch [25/50] Batch [1550/1600] LossG: 6.0899 LossD: 0.0007 L1: 5.0892\n",
      "Epoch [25/50] Batch [1600/1600] LossG: 6.0828 LossD: 0.0006 L1: 5.0821\n",
      "Epoch 25 finished in 349.0s â€” LossG: 6.0828 LossD: 0.0006\n",
      "Epoch [26/50] Batch [50/1600] LossG: 6.2232 LossD: 0.0001 L1: 5.2225\n",
      "Epoch [26/50] Batch [100/1600] LossG: 6.2694 LossD: 0.0001 L1: 5.2691\n",
      "Epoch [26/50] Batch [150/1600] LossG: 6.2073 LossD: 0.0001 L1: 5.2071\n",
      "Epoch [26/50] Batch [200/1600] LossG: 6.1229 LossD: 0.0001 L1: 5.1228\n",
      "Epoch [26/50] Batch [250/1600] LossG: 6.1030 LossD: 0.0001 L1: 5.1029\n",
      "Epoch [26/50] Batch [300/1600] LossG: 6.0959 LossD: 0.0001 L1: 5.0957\n",
      "Epoch [26/50] Batch [350/1600] LossG: 6.1008 LossD: 0.0001 L1: 5.1007\n",
      "Epoch [26/50] Batch [400/1600] LossG: 6.0838 LossD: 0.0001 L1: 5.0836\n",
      "Epoch [26/50] Batch [450/1600] LossG: 6.0770 LossD: 0.0001 L1: 5.0768\n",
      "Epoch [26/50] Batch [500/1600] LossG: 6.0642 LossD: 0.0001 L1: 5.0640\n",
      "Epoch [26/50] Batch [550/1600] LossG: 6.0759 LossD: 0.0001 L1: 5.0757\n",
      "Epoch [26/50] Batch [600/1600] LossG: 6.0723 LossD: 0.0001 L1: 5.0721\n",
      "Epoch [26/50] Batch [650/1600] LossG: 6.0732 LossD: 0.0001 L1: 5.0729\n",
      "Epoch [26/50] Batch [700/1600] LossG: 6.0787 LossD: 0.0001 L1: 5.0785\n",
      "Epoch [26/50] Batch [750/1600] LossG: 6.0572 LossD: 0.0001 L1: 5.0570\n",
      "Epoch [26/50] Batch [800/1600] LossG: 6.0690 LossD: 0.0001 L1: 5.0688\n",
      "Epoch [26/50] Batch [850/1600] LossG: 6.0701 LossD: 0.0001 L1: 5.0699\n",
      "Epoch [26/50] Batch [900/1600] LossG: 6.0648 LossD: 0.0001 L1: 5.0645\n",
      "Epoch [26/50] Batch [950/1600] LossG: 6.0679 LossD: 0.0001 L1: 5.0677\n",
      "Epoch [26/50] Batch [1000/1600] LossG: 6.0642 LossD: 0.0001 L1: 5.0639\n",
      "Epoch [26/50] Batch [1050/1600] LossG: 6.0528 LossD: 0.0001 L1: 5.0526\n",
      "Epoch [26/50] Batch [1100/1600] LossG: 6.0549 LossD: 0.0001 L1: 5.0547\n",
      "Epoch [26/50] Batch [1150/1600] LossG: 6.0549 LossD: 0.0001 L1: 5.0546\n",
      "Epoch [26/50] Batch [1200/1600] LossG: 6.0582 LossD: 0.0001 L1: 5.0580\n",
      "Epoch [26/50] Batch [1250/1600] LossG: 6.0595 LossD: 0.0001 L1: 5.0592\n",
      "Epoch [26/50] Batch [1300/1600] LossG: 6.0545 LossD: 0.0001 L1: 5.0542\n",
      "Epoch [26/50] Batch [1350/1600] LossG: 6.0568 LossD: 0.0001 L1: 5.0566\n",
      "Epoch [26/50] Batch [1400/1600] LossG: 6.0562 LossD: 0.0001 L1: 5.0560\n",
      "Epoch [26/50] Batch [1450/1600] LossG: 6.0570 LossD: 0.0001 L1: 5.0567\n",
      "Epoch [26/50] Batch [1500/1600] LossG: 6.0646 LossD: 0.0002 L1: 5.0642\n",
      "Epoch [26/50] Batch [1550/1600] LossG: 6.0648 LossD: 0.0002 L1: 5.0643\n",
      "Epoch [26/50] Batch [1600/1600] LossG: 6.0651 LossD: 0.0002 L1: 5.0644\n",
      "Epoch 26 finished in 348.8s â€” LossG: 6.0651 LossD: 0.0002\n",
      "Epoch [27/50] Batch [50/1600] LossG: 6.1585 LossD: 0.0006 L1: 5.1559\n",
      "Epoch [27/50] Batch [100/1600] LossG: 6.0877 LossD: 0.0013 L1: 5.0851\n",
      "Epoch [27/50] Batch [150/1600] LossG: 6.0601 LossD: 0.0018 L1: 5.0556\n",
      "Epoch [27/50] Batch [200/1600] LossG: 6.0383 LossD: 0.0015 L1: 5.0342\n",
      "Epoch [27/50] Batch [250/1600] LossG: 6.0178 LossD: 0.0012 L1: 5.0146\n",
      "Epoch [27/50] Batch [300/1600] LossG: 6.0627 LossD: 0.0010 L1: 5.0602\n",
      "Epoch [27/50] Batch [350/1600] LossG: 6.0364 LossD: 0.0009 L1: 5.0343\n",
      "Epoch [27/50] Batch [400/1600] LossG: 6.0342 LossD: 0.0008 L1: 5.0324\n",
      "Epoch [27/50] Batch [450/1600] LossG: 6.0360 LossD: 0.0007 L1: 5.0343\n",
      "Epoch [27/50] Batch [500/1600] LossG: 6.0454 LossD: 0.0006 L1: 5.0439\n",
      "Epoch [27/50] Batch [550/1600] LossG: 6.0469 LossD: 0.0006 L1: 5.0455\n",
      "Epoch [27/50] Batch [600/1600] LossG: 6.0523 LossD: 0.0005 L1: 5.0511\n",
      "Epoch [27/50] Batch [650/1600] LossG: 6.0512 LossD: 0.0005 L1: 5.0500\n",
      "Epoch [27/50] Batch [700/1600] LossG: 6.0553 LossD: 0.0005 L1: 5.0541\n",
      "Epoch [27/50] Batch [750/1600] LossG: 6.0532 LossD: 0.0004 L1: 5.0521\n",
      "Epoch [27/50] Batch [800/1600] LossG: 6.0571 LossD: 0.0004 L1: 5.0562\n",
      "Epoch [27/50] Batch [850/1600] LossG: 6.0534 LossD: 0.0004 L1: 5.0524\n",
      "Epoch [27/50] Batch [900/1600] LossG: 6.0547 LossD: 0.0004 L1: 5.0538\n",
      "Epoch [27/50] Batch [950/1600] LossG: 6.0553 LossD: 0.0004 L1: 5.0545\n",
      "Epoch [27/50] Batch [1000/1600] LossG: 6.0478 LossD: 0.0003 L1: 5.0470\n",
      "Epoch [27/50] Batch [1050/1600] LossG: 6.0539 LossD: 0.0003 L1: 5.0531\n",
      "Epoch [27/50] Batch [1100/1600] LossG: 6.0504 LossD: 0.0003 L1: 5.0497\n",
      "Epoch [27/50] Batch [1150/1600] LossG: 6.0425 LossD: 0.0003 L1: 5.0418\n",
      "Epoch [27/50] Batch [1200/1600] LossG: 6.0467 LossD: 0.0003 L1: 5.0460\n",
      "Epoch [27/50] Batch [1250/1600] LossG: 6.0433 LossD: 0.0003 L1: 5.0426\n",
      "Epoch [27/50] Batch [1300/1600] LossG: 6.0394 LossD: 0.0003 L1: 5.0388\n",
      "Epoch [27/50] Batch [1350/1600] LossG: 6.0403 LossD: 0.0003 L1: 5.0396\n",
      "Epoch [27/50] Batch [1400/1600] LossG: 6.0347 LossD: 0.0003 L1: 5.0340\n",
      "Epoch [27/50] Batch [1450/1600] LossG: 6.0339 LossD: 0.0003 L1: 5.0333\n",
      "Epoch [27/50] Batch [1500/1600] LossG: 6.0360 LossD: 0.0003 L1: 5.0353\n",
      "Epoch [27/50] Batch [1550/1600] LossG: 6.0336 LossD: 0.0002 L1: 5.0330\n",
      "Epoch [27/50] Batch [1600/1600] LossG: 6.0348 LossD: 0.0002 L1: 5.0342\n",
      "Epoch 27 finished in 350.3s â€” LossG: 6.0348 LossD: 0.0002\n",
      "Epoch [28/50] Batch [50/1600] LossG: 6.0475 LossD: 0.0001 L1: 5.0470\n",
      "Epoch [28/50] Batch [100/1600] LossG: 6.0293 LossD: 0.0001 L1: 5.0291\n",
      "Epoch [28/50] Batch [150/1600] LossG: 6.0373 LossD: 0.0001 L1: 5.0372\n",
      "Epoch [28/50] Batch [200/1600] LossG: 6.0164 LossD: 0.0001 L1: 5.0162\n",
      "Epoch [28/50] Batch [250/1600] LossG: 5.9955 LossD: 0.0001 L1: 4.9951\n",
      "Epoch [28/50] Batch [300/1600] LossG: 6.0301 LossD: 0.0001 L1: 5.0297\n",
      "Epoch [28/50] Batch [350/1600] LossG: 6.0046 LossD: 0.0001 L1: 5.0043\n",
      "Epoch [28/50] Batch [400/1600] LossG: 6.0057 LossD: 0.0001 L1: 5.0054\n",
      "Epoch [28/50] Batch [450/1600] LossG: 5.9838 LossD: 0.0001 L1: 4.9836\n",
      "Epoch [28/50] Batch [500/1600] LossG: 5.9858 LossD: 0.0001 L1: 4.9855\n",
      "Epoch [28/50] Batch [550/1600] LossG: 5.9839 LossD: 0.0001 L1: 4.9836\n",
      "Epoch [28/50] Batch [600/1600] LossG: 5.9848 LossD: 0.0001 L1: 4.9845\n",
      "Epoch [28/50] Batch [650/1600] LossG: 5.9814 LossD: 0.0001 L1: 4.9812\n",
      "Epoch [28/50] Batch [700/1600] LossG: 5.9814 LossD: 0.0001 L1: 4.9811\n",
      "Epoch [28/50] Batch [750/1600] LossG: 5.9899 LossD: 0.0001 L1: 4.9896\n",
      "Epoch [28/50] Batch [800/1600] LossG: 6.0020 LossD: 0.0003 L1: 5.0013\n",
      "Epoch [28/50] Batch [850/1600] LossG: 6.0038 LossD: 0.0003 L1: 5.0032\n",
      "Epoch [28/50] Batch [900/1600] LossG: 6.0176 LossD: 0.0004 L1: 5.0172\n",
      "Epoch [28/50] Batch [950/1600] LossG: 6.0222 LossD: 0.0004 L1: 5.0217\n",
      "Epoch [28/50] Batch [1000/1600] LossG: 6.0228 LossD: 0.0003 L1: 5.0223\n",
      "Epoch [28/50] Batch [1050/1600] LossG: 6.0206 LossD: 0.0003 L1: 5.0200\n",
      "Epoch [28/50] Batch [1100/1600] LossG: 6.0287 LossD: 0.0003 L1: 5.0282\n",
      "Epoch [28/50] Batch [1150/1600] LossG: 6.0315 LossD: 0.0003 L1: 5.0310\n",
      "Epoch [28/50] Batch [1200/1600] LossG: 6.0386 LossD: 0.0003 L1: 5.0380\n",
      "Epoch [28/50] Batch [1250/1600] LossG: 6.0391 LossD: 0.0003 L1: 5.0386\n",
      "Epoch [28/50] Batch [1300/1600] LossG: 6.0284 LossD: 0.0003 L1: 5.0279\n",
      "Epoch [28/50] Batch [1350/1600] LossG: 6.0248 LossD: 0.0003 L1: 5.0243\n",
      "Epoch [28/50] Batch [1400/1600] LossG: 6.0282 LossD: 0.0003 L1: 5.0277\n",
      "Epoch [28/50] Batch [1450/1600] LossG: 6.0318 LossD: 0.0003 L1: 5.0314\n",
      "Epoch [28/50] Batch [1500/1600] LossG: 6.0305 LossD: 0.0003 L1: 5.0301\n",
      "Epoch [28/50] Batch [1550/1600] LossG: 6.0310 LossD: 0.0002 L1: 5.0306\n",
      "Epoch [28/50] Batch [1600/1600] LossG: 6.0299 LossD: 0.0002 L1: 5.0295\n",
      "Epoch 28 finished in 355.4s â€” LossG: 6.0299 LossD: 0.0002\n",
      "Epoch [29/50] Batch [50/1600] LossG: 6.0835 LossD: 0.0000 L1: 5.0837\n",
      "Epoch [29/50] Batch [100/1600] LossG: 6.0516 LossD: 0.0000 L1: 5.0511\n",
      "Epoch [29/50] Batch [150/1600] LossG: 6.1452 LossD: 0.0000 L1: 5.1450\n",
      "Epoch [29/50] Batch [200/1600] LossG: 6.0779 LossD: 0.0000 L1: 5.0777\n",
      "Epoch [29/50] Batch [250/1600] LossG: 6.0481 LossD: 0.0000 L1: 5.0479\n",
      "Epoch [29/50] Batch [300/1600] LossG: 6.0449 LossD: 0.0000 L1: 5.0448\n",
      "Epoch [29/50] Batch [350/1600] LossG: 6.0240 LossD: 0.0000 L1: 5.0239\n",
      "Epoch [29/50] Batch [400/1600] LossG: 6.0201 LossD: 0.0000 L1: 5.0201\n",
      "Epoch [29/50] Batch [450/1600] LossG: 6.0427 LossD: 0.0000 L1: 5.0427\n",
      "Epoch [29/50] Batch [500/1600] LossG: 6.0304 LossD: 0.0000 L1: 5.0303\n",
      "Epoch [29/50] Batch [550/1600] LossG: 6.0304 LossD: 0.0000 L1: 5.0303\n",
      "Epoch [29/50] Batch [600/1600] LossG: 6.0198 LossD: 0.0000 L1: 5.0198\n",
      "Epoch [29/50] Batch [650/1600] LossG: 6.0172 LossD: 0.0000 L1: 5.0171\n",
      "Epoch [29/50] Batch [700/1600] LossG: 6.0226 LossD: 0.0000 L1: 5.0226\n",
      "Epoch [29/50] Batch [750/1600] LossG: 6.0256 LossD: 0.0001 L1: 5.0254\n",
      "Epoch [29/50] Batch [800/1600] LossG: 6.0209 LossD: 0.0000 L1: 5.0207\n",
      "Epoch [29/50] Batch [850/1600] LossG: 6.0262 LossD: 0.0001 L1: 5.0261\n",
      "Epoch [29/50] Batch [900/1600] LossG: 6.0162 LossD: 0.0001 L1: 5.0161\n",
      "Epoch [29/50] Batch [950/1600] LossG: 6.0165 LossD: 0.0001 L1: 5.0166\n",
      "Epoch [29/50] Batch [1000/1600] LossG: 6.0230 LossD: 0.0008 L1: 5.0249\n",
      "Epoch [29/50] Batch [1050/1600] LossG: 6.0278 LossD: 0.0008 L1: 5.0288\n",
      "Epoch [29/50] Batch [1100/1600] LossG: 6.0342 LossD: 0.0008 L1: 5.0347\n",
      "Epoch [29/50] Batch [1150/1600] LossG: 6.0481 LossD: 0.0008 L1: 5.0485\n",
      "Epoch [29/50] Batch [1200/1600] LossG: 6.0566 LossD: 0.0008 L1: 5.0572\n",
      "Epoch [29/50] Batch [1250/1600] LossG: 6.0554 LossD: 0.0007 L1: 5.0559\n",
      "Epoch [29/50] Batch [1300/1600] LossG: 6.0655 LossD: 0.0007 L1: 5.0661\n",
      "Epoch [29/50] Batch [1350/1600] LossG: 6.0679 LossD: 0.0007 L1: 5.0685\n",
      "Epoch [29/50] Batch [1400/1600] LossG: 6.0693 LossD: 0.0007 L1: 5.0699\n",
      "Epoch [29/50] Batch [1450/1600] LossG: 6.0711 LossD: 0.0007 L1: 5.0717\n",
      "Epoch [29/50] Batch [1500/1600] LossG: 6.0685 LossD: 0.0007 L1: 5.0691\n",
      "Epoch [29/50] Batch [1550/1600] LossG: 6.0707 LossD: 0.0006 L1: 5.0713\n",
      "Epoch [29/50] Batch [1600/1600] LossG: 6.0738 LossD: 0.0006 L1: 5.0744\n",
      "Epoch 29 finished in 354.2s â€” LossG: 6.0738 LossD: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 42 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 18 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 29 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50] Batch [50/1600] LossG: 6.1103 LossD: 0.0001 L1: 5.1111\n",
      "Epoch [30/50] Batch [100/1600] LossG: 6.1318 LossD: 0.0003 L1: 5.1320\n",
      "Epoch [30/50] Batch [150/1600] LossG: 6.1359 LossD: 0.0002 L1: 5.1361\n",
      "Epoch [30/50] Batch [200/1600] LossG: 6.1262 LossD: 0.0002 L1: 5.1265\n",
      "Epoch [30/50] Batch [250/1600] LossG: 6.0816 LossD: 0.0002 L1: 5.0822\n",
      "Epoch [30/50] Batch [300/1600] LossG: 6.0558 LossD: 0.0024 L1: 5.0571\n",
      "Epoch [30/50] Batch [350/1600] LossG: 6.0425 LossD: 0.0039 L1: 5.0433\n",
      "Epoch [30/50] Batch [400/1600] LossG: 6.0379 LossD: 0.0038 L1: 5.0388\n",
      "Epoch [30/50] Batch [450/1600] LossG: 6.0635 LossD: 0.0035 L1: 5.0626\n",
      "Epoch [30/50] Batch [500/1600] LossG: 6.0597 LossD: 0.0033 L1: 5.0585\n",
      "Epoch [30/50] Batch [550/1600] LossG: 6.0415 LossD: 0.0032 L1: 5.0399\n",
      "Epoch [30/50] Batch [600/1600] LossG: 6.0344 LossD: 0.0030 L1: 5.0326\n",
      "Epoch [30/50] Batch [650/1600] LossG: 6.0288 LossD: 0.0028 L1: 5.0269\n",
      "Epoch [30/50] Batch [700/1600] LossG: 6.0222 LossD: 0.0026 L1: 5.0204\n",
      "Epoch [30/50] Batch [750/1600] LossG: 6.0349 LossD: 0.0025 L1: 5.0332\n",
      "Epoch [30/50] Batch [800/1600] LossG: 6.0259 LossD: 0.0023 L1: 5.0244\n",
      "Epoch [30/50] Batch [850/1600] LossG: 6.0282 LossD: 0.0022 L1: 5.0265\n",
      "Epoch [30/50] Batch [900/1600] LossG: 6.0391 LossD: 0.0021 L1: 5.0375\n",
      "Epoch [30/50] Batch [950/1600] LossG: 6.0304 LossD: 0.0020 L1: 5.0289\n",
      "Epoch [30/50] Batch [1000/1600] LossG: 6.0246 LossD: 0.0020 L1: 5.0231\n",
      "Epoch [30/50] Batch [1050/1600] LossG: 6.0289 LossD: 0.0019 L1: 5.0275\n",
      "Epoch [30/50] Batch [1100/1600] LossG: 6.0295 LossD: 0.0018 L1: 5.0279\n",
      "Epoch [30/50] Batch [1150/1600] LossG: 6.0216 LossD: 0.0017 L1: 5.0202\n",
      "Epoch [30/50] Batch [1200/1600] LossG: 6.0203 LossD: 0.0017 L1: 5.0189\n",
      "Epoch [30/50] Batch [1250/1600] LossG: 6.0162 LossD: 0.0016 L1: 5.0149\n",
      "Epoch [30/50] Batch [1300/1600] LossG: 6.0142 LossD: 0.0015 L1: 5.0130\n",
      "Epoch [30/50] Batch [1350/1600] LossG: 6.0113 LossD: 0.0015 L1: 5.0102\n",
      "Epoch [30/50] Batch [1400/1600] LossG: 6.0120 LossD: 0.0014 L1: 5.0108\n",
      "Epoch [30/50] Batch [1450/1600] LossG: 6.0159 LossD: 0.0014 L1: 5.0147\n",
      "Epoch [30/50] Batch [1500/1600] LossG: 6.0177 LossD: 0.0014 L1: 5.0165\n",
      "Epoch [30/50] Batch [1550/1600] LossG: 6.0264 LossD: 0.0013 L1: 5.0253\n",
      "Epoch [30/50] Batch [1600/1600] LossG: 6.0299 LossD: 0.0013 L1: 5.0289\n",
      "Epoch 30 finished in 356.8s â€” LossG: 6.0299 LossD: 0.0013\n",
      "Epoch [31/50] Batch [50/1600] LossG: 5.9353 LossD: 0.0000 L1: 4.9355\n",
      "Epoch [31/50] Batch [100/1600] LossG: 6.0198 LossD: 0.0000 L1: 5.0200\n",
      "Epoch [31/50] Batch [150/1600] LossG: 6.0572 LossD: 0.0000 L1: 5.0572\n",
      "Epoch [31/50] Batch [200/1600] LossG: 5.9719 LossD: 0.0000 L1: 4.9719\n",
      "Epoch [31/50] Batch [250/1600] LossG: 5.9522 LossD: 0.0000 L1: 4.9522\n",
      "Epoch [31/50] Batch [300/1600] LossG: 5.9732 LossD: 0.0000 L1: 4.9732\n",
      "Epoch [31/50] Batch [350/1600] LossG: 5.9635 LossD: 0.0000 L1: 4.9635\n",
      "Epoch [31/50] Batch [400/1600] LossG: 5.9726 LossD: 0.0000 L1: 4.9726\n",
      "Epoch [31/50] Batch [450/1600] LossG: 5.9734 LossD: 0.0000 L1: 4.9734\n",
      "Epoch [31/50] Batch [500/1600] LossG: 5.9638 LossD: 0.0000 L1: 4.9638\n",
      "Epoch [31/50] Batch [550/1600] LossG: 5.9652 LossD: 0.0000 L1: 4.9653\n",
      "Epoch [31/50] Batch [600/1600] LossG: 5.9806 LossD: 0.0000 L1: 4.9806\n",
      "Epoch [31/50] Batch [650/1600] LossG: 5.9783 LossD: 0.0000 L1: 4.9784\n",
      "Epoch [31/50] Batch [700/1600] LossG: 5.9688 LossD: 0.0000 L1: 4.9688\n",
      "Epoch [31/50] Batch [750/1600] LossG: 5.9623 LossD: 0.0000 L1: 4.9622\n",
      "Epoch [31/50] Batch [800/1600] LossG: 5.9577 LossD: 0.0000 L1: 4.9577\n",
      "Epoch [31/50] Batch [850/1600] LossG: 5.9598 LossD: 0.0000 L1: 4.9597\n",
      "Epoch [31/50] Batch [900/1600] LossG: 5.9693 LossD: 0.0000 L1: 4.9692\n",
      "Epoch [31/50] Batch [950/1600] LossG: 5.9739 LossD: 0.0000 L1: 4.9738\n",
      "Epoch [31/50] Batch [1000/1600] LossG: 5.9671 LossD: 0.0000 L1: 4.9670\n",
      "Epoch [31/50] Batch [1050/1600] LossG: 5.9706 LossD: 0.0000 L1: 4.9705\n",
      "Epoch [31/50] Batch [1100/1600] LossG: 5.9626 LossD: 0.0000 L1: 4.9625\n",
      "Epoch [31/50] Batch [1150/1600] LossG: 5.9688 LossD: 0.0000 L1: 4.9687\n",
      "Epoch [31/50] Batch [1200/1600] LossG: 5.9667 LossD: 0.0000 L1: 4.9666\n",
      "Epoch [31/50] Batch [1250/1600] LossG: 5.9617 LossD: 0.0001 L1: 4.9616\n",
      "Epoch [31/50] Batch [1300/1600] LossG: 5.9625 LossD: 0.0001 L1: 4.9624\n",
      "Epoch [31/50] Batch [1350/1600] LossG: 5.9651 LossD: 0.0001 L1: 4.9650\n",
      "Epoch [31/50] Batch [1400/1600] LossG: 5.9689 LossD: 0.0001 L1: 4.9687\n",
      "Epoch [31/50] Batch [1450/1600] LossG: 5.9660 LossD: 0.0001 L1: 4.9658\n",
      "Epoch [31/50] Batch [1500/1600] LossG: 5.9659 LossD: 0.0001 L1: 4.9656\n",
      "Epoch [31/50] Batch [1550/1600] LossG: 5.9714 LossD: 0.0001 L1: 4.9711\n",
      "Epoch [31/50] Batch [1600/1600] LossG: 5.9778 LossD: 0.0001 L1: 4.9776\n",
      "Epoch 31 finished in 361.1s â€” LossG: 5.9778 LossD: 0.0001\n",
      "Epoch [32/50] Batch [50/1600] LossG: 5.9352 LossD: 0.0001 L1: 4.9344\n",
      "Epoch [32/50] Batch [100/1600] LossG: 5.9227 LossD: 0.0001 L1: 4.9224\n",
      "Epoch [32/50] Batch [150/1600] LossG: 5.9397 LossD: 0.0001 L1: 4.9394\n",
      "Epoch [32/50] Batch [200/1600] LossG: 5.9944 LossD: 0.0001 L1: 4.9940\n",
      "Epoch [32/50] Batch [250/1600] LossG: 5.9968 LossD: 0.0002 L1: 4.9960\n",
      "Epoch [32/50] Batch [300/1600] LossG: 6.0097 LossD: 0.0001 L1: 5.0090\n",
      "Epoch [32/50] Batch [350/1600] LossG: 5.9913 LossD: 0.0001 L1: 4.9905\n",
      "Epoch [32/50] Batch [400/1600] LossG: 5.9895 LossD: 0.0001 L1: 4.9887\n",
      "Epoch [32/50] Batch [450/1600] LossG: 5.9731 LossD: 0.0001 L1: 4.9724\n",
      "Epoch [32/50] Batch [500/1600] LossG: 5.9702 LossD: 0.0001 L1: 4.9695\n",
      "Epoch [32/50] Batch [550/1600] LossG: 5.9795 LossD: 0.0001 L1: 4.9789\n",
      "Epoch [32/50] Batch [600/1600] LossG: 5.9745 LossD: 0.0001 L1: 4.9739\n",
      "Epoch [32/50] Batch [650/1600] LossG: 5.9861 LossD: 0.0002 L1: 4.9856\n",
      "Epoch [32/50] Batch [700/1600] LossG: 5.9760 LossD: 0.0002 L1: 4.9751\n",
      "Epoch [32/50] Batch [750/1600] LossG: 5.9677 LossD: 0.0002 L1: 4.9667\n",
      "Epoch [32/50] Batch [800/1600] LossG: 5.9682 LossD: 0.0002 L1: 4.9672\n",
      "Epoch [32/50] Batch [850/1600] LossG: 5.9943 LossD: 0.0003 L1: 4.9932\n",
      "Epoch [32/50] Batch [900/1600] LossG: 5.9988 LossD: 0.0003 L1: 4.9973\n",
      "Epoch [32/50] Batch [950/1600] LossG: 6.0003 LossD: 0.0003 L1: 4.9987\n",
      "Epoch [32/50] Batch [1000/1600] LossG: 5.9988 LossD: 0.0003 L1: 4.9972\n",
      "Epoch [32/50] Batch [1050/1600] LossG: 5.9940 LossD: 0.0002 L1: 4.9924\n",
      "Epoch [32/50] Batch [1100/1600] LossG: 5.9871 LossD: 0.0004 L1: 4.9855\n",
      "Epoch [32/50] Batch [1150/1600] LossG: 5.9837 LossD: 0.0004 L1: 4.9820\n",
      "Epoch [32/50] Batch [1200/1600] LossG: 5.9862 LossD: 0.0004 L1: 4.9847\n",
      "Epoch [32/50] Batch [1250/1600] LossG: 5.9873 LossD: 0.0004 L1: 4.9857\n",
      "Epoch [32/50] Batch [1300/1600] LossG: 5.9898 LossD: 0.0004 L1: 4.9883\n",
      "Epoch [32/50] Batch [1350/1600] LossG: 5.9874 LossD: 0.0004 L1: 4.9860\n",
      "Epoch [32/50] Batch [1400/1600] LossG: 5.9854 LossD: 0.0003 L1: 4.9840\n",
      "Epoch [32/50] Batch [1450/1600] LossG: 5.9906 LossD: 0.0003 L1: 4.9893\n",
      "Epoch [32/50] Batch [1500/1600] LossG: 5.9911 LossD: 0.0003 L1: 4.9898\n",
      "Epoch [32/50] Batch [1550/1600] LossG: 5.9933 LossD: 0.0003 L1: 4.9920\n",
      "Epoch [32/50] Batch [1600/1600] LossG: 5.9880 LossD: 0.0003 L1: 4.9868\n",
      "Epoch 32 finished in 355.4s â€” LossG: 5.9880 LossD: 0.0003\n",
      "Epoch [33/50] Batch [50/1600] LossG: 5.6507 LossD: 0.0000 L1: 4.6506\n",
      "Epoch [33/50] Batch [100/1600] LossG: 5.7157 LossD: 0.0000 L1: 4.7156\n",
      "Epoch [33/50] Batch [150/1600] LossG: 5.7855 LossD: 0.0000 L1: 4.7855\n",
      "Epoch [33/50] Batch [200/1600] LossG: 5.8534 LossD: 0.0000 L1: 4.8534\n",
      "Epoch [33/50] Batch [250/1600] LossG: 5.9304 LossD: 0.0000 L1: 4.9304\n",
      "Epoch [33/50] Batch [300/1600] LossG: 5.9574 LossD: 0.0000 L1: 4.9574\n",
      "Epoch [33/50] Batch [350/1600] LossG: 5.9300 LossD: 0.0000 L1: 4.9300\n",
      "Epoch [33/50] Batch [400/1600] LossG: 5.9289 LossD: 0.0000 L1: 4.9289\n",
      "Epoch [33/50] Batch [450/1600] LossG: 5.9210 LossD: 0.0000 L1: 4.9210\n",
      "Epoch [33/50] Batch [500/1600] LossG: 5.9137 LossD: 0.0000 L1: 4.9136\n",
      "Epoch [33/50] Batch [550/1600] LossG: 5.9181 LossD: 0.0000 L1: 4.9180\n",
      "Epoch [33/50] Batch [600/1600] LossG: 5.9214 LossD: 0.0000 L1: 4.9214\n",
      "Epoch [33/50] Batch [650/1600] LossG: 5.9195 LossD: 0.0000 L1: 4.9195\n",
      "Epoch [33/50] Batch [700/1600] LossG: 5.9018 LossD: 0.0000 L1: 4.9018\n",
      "Epoch [33/50] Batch [750/1600] LossG: 5.9131 LossD: 0.0000 L1: 4.9131\n",
      "Epoch [33/50] Batch [800/1600] LossG: 5.9240 LossD: 0.0000 L1: 4.9239\n",
      "Epoch [33/50] Batch [850/1600] LossG: 5.9286 LossD: 0.0000 L1: 4.9285\n",
      "Epoch [33/50] Batch [900/1600] LossG: 5.9408 LossD: 0.0000 L1: 4.9408\n",
      "Epoch [33/50] Batch [950/1600] LossG: 5.9388 LossD: 0.0000 L1: 4.9387\n",
      "Epoch [33/50] Batch [1000/1600] LossG: 5.9284 LossD: 0.0000 L1: 4.9283\n",
      "Epoch [33/50] Batch [1050/1600] LossG: 5.9275 LossD: 0.0000 L1: 4.9274\n",
      "Epoch [33/50] Batch [1100/1600] LossG: 5.9292 LossD: 0.0000 L1: 4.9291\n",
      "Epoch [33/50] Batch [1150/1600] LossG: 5.9356 LossD: 0.0000 L1: 4.9356\n",
      "Epoch [33/50] Batch [1200/1600] LossG: 5.9366 LossD: 0.0000 L1: 4.9365\n",
      "Epoch [33/50] Batch [1250/1600] LossG: 5.9460 LossD: 0.0000 L1: 4.9460\n",
      "Epoch [33/50] Batch [1300/1600] LossG: 5.9448 LossD: 0.0000 L1: 4.9447\n",
      "Epoch [33/50] Batch [1350/1600] LossG: 5.9411 LossD: 0.0000 L1: 4.9411\n",
      "Epoch [33/50] Batch [1400/1600] LossG: 5.9374 LossD: 0.0000 L1: 4.9374\n",
      "Epoch [33/50] Batch [1450/1600] LossG: 5.9309 LossD: 0.0000 L1: 4.9309\n",
      "Epoch [33/50] Batch [1500/1600] LossG: 5.9353 LossD: 0.0000 L1: 4.9353\n",
      "Epoch [33/50] Batch [1550/1600] LossG: 5.9387 LossD: 0.0000 L1: 4.9387\n",
      "Epoch [33/50] Batch [1600/1600] LossG: 5.9403 LossD: 0.0000 L1: 4.9403\n",
      "Epoch 33 finished in 353.9s â€” LossG: 5.9403 LossD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 85 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50] Batch [50/1600] LossG: 5.8611 LossD: 0.0000 L1: 4.8611\n",
      "Epoch [34/50] Batch [100/1600] LossG: 5.8613 LossD: 0.0000 L1: 4.8614\n",
      "Epoch [34/50] Batch [150/1600] LossG: 5.8992 LossD: 0.0000 L1: 4.8992\n",
      "Epoch [34/50] Batch [200/1600] LossG: 5.8987 LossD: 0.0000 L1: 4.8986\n",
      "Epoch [34/50] Batch [250/1600] LossG: 5.9233 LossD: 0.0000 L1: 4.9233\n",
      "Epoch [34/50] Batch [300/1600] LossG: 5.9204 LossD: 0.0000 L1: 4.9204\n",
      "Epoch [34/50] Batch [350/1600] LossG: 5.9143 LossD: 0.0000 L1: 4.9143\n",
      "Epoch [34/50] Batch [400/1600] LossG: 5.9138 LossD: 0.0000 L1: 4.9138\n",
      "Epoch [34/50] Batch [450/1600] LossG: 5.9169 LossD: 0.0001 L1: 4.9168\n",
      "Epoch [34/50] Batch [500/1600] LossG: 5.9106 LossD: 0.0001 L1: 4.9105\n",
      "Epoch [34/50] Batch [550/1600] LossG: 5.9114 LossD: 0.0001 L1: 4.9113\n",
      "Epoch [34/50] Batch [600/1600] LossG: 5.9222 LossD: 0.0001 L1: 4.9221\n",
      "Epoch [34/50] Batch [650/1600] LossG: 5.9142 LossD: 0.0001 L1: 4.9141\n",
      "Epoch [34/50] Batch [700/1600] LossG: 5.9186 LossD: 0.0001 L1: 4.9185\n",
      "Epoch [34/50] Batch [750/1600] LossG: 5.9245 LossD: 0.0001 L1: 4.9243\n",
      "Epoch [34/50] Batch [800/1600] LossG: 5.9415 LossD: 0.0002 L1: 4.9424\n",
      "Epoch [34/50] Batch [850/1600] LossG: 5.9460 LossD: 0.0002 L1: 4.9464\n",
      "Epoch [34/50] Batch [900/1600] LossG: 5.9665 LossD: 0.0002 L1: 4.9668\n",
      "Epoch [34/50] Batch [950/1600] LossG: 5.9636 LossD: 0.0002 L1: 4.9637\n",
      "Epoch [34/50] Batch [1000/1600] LossG: 5.9659 LossD: 0.0002 L1: 4.9661\n",
      "Epoch [34/50] Batch [1050/1600] LossG: 5.9685 LossD: 0.0002 L1: 4.9685\n",
      "Epoch [34/50] Batch [1100/1600] LossG: 5.9633 LossD: 0.0011 L1: 4.9644\n",
      "Epoch [34/50] Batch [1150/1600] LossG: 5.9676 LossD: 0.0011 L1: 4.9686\n",
      "Epoch [34/50] Batch [1200/1600] LossG: 5.9696 LossD: 0.0011 L1: 4.9706\n",
      "Epoch [34/50] Batch [1250/1600] LossG: 5.9736 LossD: 0.0011 L1: 4.9744\n",
      "Epoch [34/50] Batch [1300/1600] LossG: 5.9700 LossD: 0.0011 L1: 4.9705\n",
      "Epoch [34/50] Batch [1350/1600] LossG: 5.9613 LossD: 0.0010 L1: 4.9617\n",
      "Epoch [34/50] Batch [1400/1600] LossG: 5.9633 LossD: 0.0010 L1: 4.9637\n",
      "Epoch [34/50] Batch [1450/1600] LossG: 5.9609 LossD: 0.0010 L1: 4.9612\n",
      "Epoch [34/50] Batch [1500/1600] LossG: 5.9582 LossD: 0.0010 L1: 4.9585\n",
      "Epoch [34/50] Batch [1550/1600] LossG: 5.9621 LossD: 0.0009 L1: 4.9623\n",
      "Epoch [34/50] Batch [1600/1600] LossG: 5.9618 LossD: 0.0009 L1: 4.9620\n",
      "Epoch 34 finished in 352.5s â€” LossG: 5.9618 LossD: 0.0009\n",
      "Epoch [35/50] Batch [50/1600] LossG: 5.8581 LossD: 0.0001 L1: 4.8581\n",
      "Epoch [35/50] Batch [100/1600] LossG: 5.8741 LossD: 0.0000 L1: 4.8740\n",
      "Epoch [35/50] Batch [150/1600] LossG: 5.8585 LossD: 0.0000 L1: 4.8585\n",
      "Epoch [35/50] Batch [200/1600] LossG: 5.9077 LossD: 0.0000 L1: 4.9077\n",
      "Epoch [35/50] Batch [250/1600] LossG: 5.9304 LossD: 0.0000 L1: 4.9304\n",
      "Epoch [35/50] Batch [300/1600] LossG: 5.9411 LossD: 0.0000 L1: 4.9411\n",
      "Epoch [35/50] Batch [350/1600] LossG: 5.9014 LossD: 0.0001 L1: 4.9011\n",
      "Epoch [35/50] Batch [400/1600] LossG: 5.9030 LossD: 0.0001 L1: 4.9027\n",
      "Epoch [35/50] Batch [450/1600] LossG: 5.9175 LossD: 0.0001 L1: 4.9172\n",
      "Epoch [35/50] Batch [500/1600] LossG: 5.9098 LossD: 0.0001 L1: 4.9095\n",
      "Epoch [35/50] Batch [550/1600] LossG: 5.9229 LossD: 0.0001 L1: 4.9226\n",
      "Epoch [35/50] Batch [600/1600] LossG: 5.9269 LossD: 0.0001 L1: 4.9266\n",
      "Epoch [35/50] Batch [650/1600] LossG: 5.9189 LossD: 0.0001 L1: 4.9186\n",
      "Epoch [35/50] Batch [700/1600] LossG: 5.9124 LossD: 0.0001 L1: 4.9121\n",
      "Epoch [35/50] Batch [750/1600] LossG: 5.9039 LossD: 0.0001 L1: 4.9036\n",
      "Epoch [35/50] Batch [800/1600] LossG: 5.9046 LossD: 0.0001 L1: 4.9044\n",
      "Epoch [35/50] Batch [850/1600] LossG: 5.9046 LossD: 0.0001 L1: 4.9044\n",
      "Epoch [35/50] Batch [900/1600] LossG: 5.9081 LossD: 0.0001 L1: 4.9079\n",
      "Epoch [35/50] Batch [950/1600] LossG: 5.9085 LossD: 0.0001 L1: 4.9084\n",
      "Epoch [35/50] Batch [1000/1600] LossG: 5.9048 LossD: 0.0001 L1: 4.9046\n",
      "Epoch [35/50] Batch [1050/1600] LossG: 5.9091 LossD: 0.0001 L1: 4.9090\n",
      "Epoch [35/50] Batch [1100/1600] LossG: 5.9163 LossD: 0.0001 L1: 4.9162\n",
      "Epoch [35/50] Batch [1150/1600] LossG: 5.9199 LossD: 0.0001 L1: 4.9198\n",
      "Epoch [35/50] Batch [1200/1600] LossG: 5.9337 LossD: 0.0001 L1: 4.9336\n",
      "Epoch [35/50] Batch [1250/1600] LossG: 5.9437 LossD: 0.0001 L1: 4.9436\n",
      "Epoch [35/50] Batch [1300/1600] LossG: 5.9457 LossD: 0.0001 L1: 4.9457\n",
      "Epoch [35/50] Batch [1350/1600] LossG: 5.9505 LossD: 0.0001 L1: 4.9504\n",
      "Epoch [35/50] Batch [1400/1600] LossG: 5.9540 LossD: 0.0001 L1: 4.9540\n",
      "Epoch [35/50] Batch [1450/1600] LossG: 5.9500 LossD: 0.0001 L1: 4.9500\n",
      "Epoch [35/50] Batch [1500/1600] LossG: 5.9454 LossD: 0.0001 L1: 4.9454\n",
      "Epoch [35/50] Batch [1550/1600] LossG: 5.9508 LossD: 0.0001 L1: 4.9508\n",
      "Epoch [35/50] Batch [1600/1600] LossG: 5.9479 LossD: 0.0001 L1: 4.9479\n",
      "Epoch 35 finished in 356.5s â€” LossG: 5.9479 LossD: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 19 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50] Batch [50/1600] LossG: 5.9953 LossD: 0.0000 L1: 4.9956\n",
      "Epoch [36/50] Batch [100/1600] LossG: 5.9322 LossD: 0.0000 L1: 4.9323\n",
      "Epoch [36/50] Batch [150/1600] LossG: 5.9402 LossD: 0.0000 L1: 4.9404\n",
      "Epoch [36/50] Batch [200/1600] LossG: 5.9287 LossD: 0.0000 L1: 4.9288\n",
      "Epoch [36/50] Batch [250/1600] LossG: 5.9351 LossD: 0.0000 L1: 4.9352\n",
      "Epoch [36/50] Batch [300/1600] LossG: 5.9269 LossD: 0.0000 L1: 4.9270\n",
      "Epoch [36/50] Batch [350/1600] LossG: 5.9491 LossD: 0.0000 L1: 4.9492\n",
      "Epoch [36/50] Batch [400/1600] LossG: 5.9429 LossD: 0.0000 L1: 4.9430\n",
      "Epoch [36/50] Batch [450/1600] LossG: 5.9543 LossD: 0.0000 L1: 4.9544\n",
      "Epoch [36/50] Batch [500/1600] LossG: 5.9669 LossD: 0.0000 L1: 4.9669\n",
      "Epoch [36/50] Batch [550/1600] LossG: 5.9561 LossD: 0.0000 L1: 4.9561\n",
      "Epoch [36/50] Batch [600/1600] LossG: 5.9566 LossD: 0.0000 L1: 4.9567\n",
      "Epoch [36/50] Batch [650/1600] LossG: 5.9512 LossD: 0.0000 L1: 4.9513\n",
      "Epoch [36/50] Batch [700/1600] LossG: 5.9416 LossD: 0.0000 L1: 4.9417\n",
      "Epoch [36/50] Batch [750/1600] LossG: 5.9465 LossD: 0.0000 L1: 4.9466\n",
      "Epoch [36/50] Batch [800/1600] LossG: 5.9292 LossD: 0.0000 L1: 4.9293\n",
      "Epoch [36/50] Batch [850/1600] LossG: 5.9337 LossD: 0.0000 L1: 4.9337\n",
      "Epoch [36/50] Batch [900/1600] LossG: 5.9405 LossD: 0.0000 L1: 4.9405\n",
      "Epoch [36/50] Batch [950/1600] LossG: 5.9391 LossD: 0.0000 L1: 4.9391\n",
      "Epoch [36/50] Batch [1000/1600] LossG: 5.9473 LossD: 0.0001 L1: 4.9473\n",
      "Epoch [36/50] Batch [1050/1600] LossG: 5.9478 LossD: 0.0000 L1: 4.9478\n",
      "Epoch [36/50] Batch [1100/1600] LossG: 5.9511 LossD: 0.0000 L1: 4.9511\n",
      "Epoch [36/50] Batch [1150/1600] LossG: 5.9554 LossD: 0.0000 L1: 4.9554\n",
      "Epoch [36/50] Batch [1200/1600] LossG: 5.9552 LossD: 0.0000 L1: 4.9552\n",
      "Epoch [36/50] Batch [1250/1600] LossG: 5.9503 LossD: 0.0000 L1: 4.9503\n",
      "Epoch [36/50] Batch [1300/1600] LossG: 5.9419 LossD: 0.0000 L1: 4.9420\n",
      "Epoch [36/50] Batch [1350/1600] LossG: 5.9400 LossD: 0.0000 L1: 4.9400\n",
      "Epoch [36/50] Batch [1400/1600] LossG: 5.9382 LossD: 0.0000 L1: 4.9383\n",
      "Epoch [36/50] Batch [1450/1600] LossG: 5.9387 LossD: 0.0000 L1: 4.9387\n",
      "Epoch [36/50] Batch [1500/1600] LossG: 5.9374 LossD: 0.0000 L1: 4.9374\n",
      "Epoch [36/50] Batch [1550/1600] LossG: 5.9381 LossD: 0.0000 L1: 4.9381\n",
      "Epoch [36/50] Batch [1600/1600] LossG: 5.9349 LossD: 0.0000 L1: 4.9349\n",
      "Epoch 36 finished in 351.3s â€” LossG: 5.9349 LossD: 0.0000\n",
      "Epoch [37/50] Batch [50/1600] LossG: 5.9995 LossD: 0.0000 L1: 4.9996\n",
      "Epoch [37/50] Batch [100/1600] LossG: 5.8973 LossD: 0.0000 L1: 4.8974\n",
      "Epoch [37/50] Batch [150/1600] LossG: 5.9075 LossD: 0.0000 L1: 4.9075\n",
      "Epoch [37/50] Batch [200/1600] LossG: 5.8961 LossD: 0.0000 L1: 4.8962\n",
      "Epoch [37/50] Batch [250/1600] LossG: 5.9032 LossD: 0.0000 L1: 4.9033\n",
      "Epoch [37/50] Batch [300/1600] LossG: 5.9122 LossD: 0.0000 L1: 4.9123\n",
      "Epoch [37/50] Batch [350/1600] LossG: 5.9309 LossD: 0.0000 L1: 4.9309\n",
      "Epoch [37/50] Batch [400/1600] LossG: 5.9401 LossD: 0.0000 L1: 4.9401\n",
      "Epoch [37/50] Batch [450/1600] LossG: 5.9438 LossD: 0.0000 L1: 4.9438\n",
      "Epoch [37/50] Batch [500/1600] LossG: 5.9557 LossD: 0.0000 L1: 4.9558\n",
      "Epoch [37/50] Batch [550/1600] LossG: 5.9603 LossD: 0.0000 L1: 4.9604\n",
      "Epoch [37/50] Batch [600/1600] LossG: 5.9592 LossD: 0.0000 L1: 4.9592\n",
      "Epoch [37/50] Batch [650/1600] LossG: 5.9570 LossD: 0.0000 L1: 4.9570\n",
      "Epoch [37/50] Batch [700/1600] LossG: 5.9477 LossD: 0.0000 L1: 4.9478\n",
      "Epoch [37/50] Batch [750/1600] LossG: 5.9442 LossD: 0.0000 L1: 4.9442\n",
      "Epoch [37/50] Batch [800/1600] LossG: 5.9443 LossD: 0.0000 L1: 4.9444\n",
      "Epoch [37/50] Batch [850/1600] LossG: 5.9480 LossD: 0.0000 L1: 4.9480\n",
      "Epoch [37/50] Batch [900/1600] LossG: 5.9481 LossD: 0.0000 L1: 4.9482\n",
      "Epoch [37/50] Batch [950/1600] LossG: 5.9524 LossD: 0.0000 L1: 4.9525\n",
      "Epoch [37/50] Batch [1000/1600] LossG: 5.9497 LossD: 0.0000 L1: 4.9497\n",
      "Epoch [37/50] Batch [1050/1600] LossG: 5.9466 LossD: 0.0000 L1: 4.9467\n",
      "Epoch [37/50] Batch [1100/1600] LossG: 5.9448 LossD: 0.0000 L1: 4.9449\n",
      "Epoch [37/50] Batch [1150/1600] LossG: 5.9426 LossD: 0.0000 L1: 4.9426\n",
      "Epoch [37/50] Batch [1200/1600] LossG: 5.9407 LossD: 0.0000 L1: 4.9407\n",
      "Epoch [37/50] Batch [1250/1600] LossG: 5.9397 LossD: 0.0000 L1: 4.9397\n",
      "Epoch [37/50] Batch [1300/1600] LossG: 5.9353 LossD: 0.0000 L1: 4.9353\n",
      "Epoch [37/50] Batch [1350/1600] LossG: 5.9315 LossD: 0.0000 L1: 4.9315\n",
      "Epoch [37/50] Batch [1400/1600] LossG: 5.9327 LossD: 0.0000 L1: 4.9327\n",
      "Epoch [37/50] Batch [1450/1600] LossG: 5.9365 LossD: 0.0007 L1: 4.9384\n",
      "Epoch [37/50] Batch [1500/1600] LossG: 5.9364 LossD: 0.0010 L1: 4.9384\n",
      "Epoch [37/50] Batch [1550/1600] LossG: 5.9397 LossD: 0.0011 L1: 4.9419\n",
      "Epoch [37/50] Batch [1600/1600] LossG: 5.9436 LossD: 0.0011 L1: 4.9458\n",
      "Epoch 37 finished in 337.3s â€” LossG: 5.9436 LossD: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 11 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/50] Batch [50/1600] LossG: 6.1052 LossD: 0.0007 L1: 5.1026\n",
      "Epoch [38/50] Batch [100/1600] LossG: 6.0779 LossD: 0.0007 L1: 5.0742\n",
      "Epoch [38/50] Batch [150/1600] LossG: 6.0258 LossD: 0.0008 L1: 5.0226\n",
      "Epoch [38/50] Batch [200/1600] LossG: 5.9988 LossD: 0.0007 L1: 4.9946\n",
      "Epoch [38/50] Batch [250/1600] LossG: 5.9905 LossD: 0.0029 L1: 4.9893\n",
      "Epoch [38/50] Batch [300/1600] LossG: 6.0133 LossD: 0.0028 L1: 5.0097\n",
      "Epoch [38/50] Batch [350/1600] LossG: 5.9988 LossD: 0.0025 L1: 4.9943\n",
      "Epoch [38/50] Batch [400/1600] LossG: 5.9654 LossD: 0.0022 L1: 4.9612\n",
      "Epoch [38/50] Batch [450/1600] LossG: 5.9523 LossD: 0.0020 L1: 4.9481\n",
      "Epoch [38/50] Batch [500/1600] LossG: 5.9381 LossD: 0.0019 L1: 4.9344\n",
      "Epoch [38/50] Batch [550/1600] LossG: 5.9333 LossD: 0.0017 L1: 4.9298\n",
      "Epoch [38/50] Batch [600/1600] LossG: 5.9339 LossD: 0.0016 L1: 4.9306\n",
      "Epoch [38/50] Batch [650/1600] LossG: 5.9452 LossD: 0.0015 L1: 4.9422\n",
      "Epoch [38/50] Batch [700/1600] LossG: 5.9265 LossD: 0.0014 L1: 4.9236\n",
      "Epoch [38/50] Batch [750/1600] LossG: 5.9188 LossD: 0.0013 L1: 4.9161\n",
      "Epoch [38/50] Batch [800/1600] LossG: 5.9176 LossD: 0.0012 L1: 4.9151\n",
      "Epoch [38/50] Batch [850/1600] LossG: 5.9164 LossD: 0.0011 L1: 4.9141\n",
      "Epoch [38/50] Batch [900/1600] LossG: 5.9093 LossD: 0.0011 L1: 4.9071\n",
      "Epoch [38/50] Batch [950/1600] LossG: 5.9144 LossD: 0.0010 L1: 4.9123\n",
      "Epoch [38/50] Batch [1000/1600] LossG: 5.9122 LossD: 0.0010 L1: 4.9102\n",
      "Epoch [38/50] Batch [1050/1600] LossG: 5.9107 LossD: 0.0009 L1: 4.9087\n",
      "Epoch [38/50] Batch [1100/1600] LossG: 5.9024 LossD: 0.0009 L1: 4.9006\n",
      "Epoch [38/50] Batch [1150/1600] LossG: 5.9074 LossD: 0.0009 L1: 4.9056\n",
      "Epoch [38/50] Batch [1200/1600] LossG: 5.9035 LossD: 0.0008 L1: 4.9018\n",
      "Epoch [38/50] Batch [1250/1600] LossG: 5.9013 LossD: 0.0008 L1: 4.8997\n",
      "Epoch [38/50] Batch [1300/1600] LossG: 5.9027 LossD: 0.0008 L1: 4.9011\n",
      "Epoch [38/50] Batch [1350/1600] LossG: 5.8995 LossD: 0.0007 L1: 4.8980\n",
      "Epoch [38/50] Batch [1400/1600] LossG: 5.9044 LossD: 0.0007 L1: 4.9029\n",
      "Epoch [38/50] Batch [1450/1600] LossG: 5.9026 LossD: 0.0007 L1: 4.9012\n",
      "Epoch [38/50] Batch [1500/1600] LossG: 5.9035 LossD: 0.0007 L1: 4.9021\n",
      "Epoch [38/50] Batch [1550/1600] LossG: 5.9064 LossD: 0.0007 L1: 4.9051\n",
      "Epoch [38/50] Batch [1600/1600] LossG: 5.9049 LossD: 0.0006 L1: 4.9036\n",
      "Epoch 38 finished in 331.1s â€” LossG: 5.9049 LossD: 0.0006\n",
      "Epoch [39/50] Batch [50/1600] LossG: 5.8309 LossD: 0.0000 L1: 4.8310\n",
      "Epoch [39/50] Batch [100/1600] LossG: 5.8469 LossD: 0.0000 L1: 4.8471\n",
      "Epoch [39/50] Batch [150/1600] LossG: 5.8914 LossD: 0.0000 L1: 4.8914\n",
      "Epoch [39/50] Batch [200/1600] LossG: 5.9127 LossD: 0.0000 L1: 4.9125\n",
      "Epoch [39/50] Batch [250/1600] LossG: 5.8927 LossD: 0.0000 L1: 4.8926\n",
      "Epoch [39/50] Batch [300/1600] LossG: 5.8849 LossD: 0.0000 L1: 4.8848\n",
      "Epoch [39/50] Batch [350/1600] LossG: 5.8821 LossD: 0.0000 L1: 4.8820\n",
      "Epoch [39/50] Batch [400/1600] LossG: 5.8643 LossD: 0.0000 L1: 4.8642\n",
      "Epoch [39/50] Batch [450/1600] LossG: 5.8476 LossD: 0.0000 L1: 4.8476\n",
      "Epoch [39/50] Batch [500/1600] LossG: 5.8619 LossD: 0.0000 L1: 4.8619\n",
      "Epoch [39/50] Batch [550/1600] LossG: 5.8413 LossD: 0.0000 L1: 4.8413\n",
      "Epoch [39/50] Batch [600/1600] LossG: 5.8387 LossD: 0.0000 L1: 4.8387\n",
      "Epoch [39/50] Batch [650/1600] LossG: 5.8404 LossD: 0.0000 L1: 4.8403\n",
      "Epoch [39/50] Batch [700/1600] LossG: 5.8354 LossD: 0.0000 L1: 4.8354\n",
      "Epoch [39/50] Batch [750/1600] LossG: 5.8503 LossD: 0.0000 L1: 4.8503\n",
      "Epoch [39/50] Batch [800/1600] LossG: 5.8610 LossD: 0.0000 L1: 4.8610\n",
      "Epoch [39/50] Batch [850/1600] LossG: 5.8619 LossD: 0.0000 L1: 4.8619\n",
      "Epoch [39/50] Batch [900/1600] LossG: 5.8753 LossD: 0.0000 L1: 4.8752\n",
      "Epoch [39/50] Batch [950/1600] LossG: 5.8771 LossD: 0.0000 L1: 4.8771\n",
      "Epoch [39/50] Batch [1000/1600] LossG: 5.8738 LossD: 0.0000 L1: 4.8738\n",
      "Epoch [39/50] Batch [1050/1600] LossG: 5.8677 LossD: 0.0000 L1: 4.8677\n",
      "Epoch [39/50] Batch [1100/1600] LossG: 5.8673 LossD: 0.0000 L1: 4.8672\n",
      "Epoch [39/50] Batch [1150/1600] LossG: 5.8634 LossD: 0.0000 L1: 4.8634\n",
      "Epoch [39/50] Batch [1200/1600] LossG: 5.8670 LossD: 0.0000 L1: 4.8670\n",
      "Epoch [39/50] Batch [1250/1600] LossG: 5.8668 LossD: 0.0000 L1: 4.8668\n",
      "Epoch [39/50] Batch [1300/1600] LossG: 5.8679 LossD: 0.0000 L1: 4.8679\n",
      "Epoch [39/50] Batch [1350/1600] LossG: 5.8683 LossD: 0.0000 L1: 4.8683\n",
      "Epoch [39/50] Batch [1400/1600] LossG: 5.8722 LossD: 0.0000 L1: 4.8722\n",
      "Epoch [39/50] Batch [1450/1600] LossG: 5.8735 LossD: 0.0000 L1: 4.8735\n",
      "Epoch [39/50] Batch [1500/1600] LossG: 5.8783 LossD: 0.0000 L1: 4.8783\n",
      "Epoch [39/50] Batch [1550/1600] LossG: 5.8814 LossD: 0.0001 L1: 4.8813\n",
      "Epoch [39/50] Batch [1600/1600] LossG: 5.8831 LossD: 0.0001 L1: 4.8831\n",
      "Epoch 39 finished in 329.0s â€” LossG: 5.8831 LossD: 0.0001\n",
      "Epoch [40/50] Batch [50/1600] LossG: 5.9460 LossD: 0.0000 L1: 4.9462\n",
      "Epoch [40/50] Batch [100/1600] LossG: 5.8321 LossD: 0.0000 L1: 4.8322\n",
      "Epoch [40/50] Batch [150/1600] LossG: 5.8742 LossD: 0.0000 L1: 4.8743\n",
      "Epoch [40/50] Batch [200/1600] LossG: 5.9088 LossD: 0.0000 L1: 4.9089\n",
      "Epoch [40/50] Batch [250/1600] LossG: 5.9101 LossD: 0.0000 L1: 4.9102\n",
      "Epoch [40/50] Batch [300/1600] LossG: 5.8883 LossD: 0.0000 L1: 4.8884\n",
      "Epoch [40/50] Batch [350/1600] LossG: 5.8948 LossD: 0.0000 L1: 4.8949\n",
      "Epoch [40/50] Batch [400/1600] LossG: 5.8795 LossD: 0.0000 L1: 4.8794\n",
      "Epoch [40/50] Batch [450/1600] LossG: 5.8832 LossD: 0.0000 L1: 4.8831\n",
      "Epoch [40/50] Batch [500/1600] LossG: 5.8868 LossD: 0.0000 L1: 4.8868\n",
      "Epoch [40/50] Batch [550/1600] LossG: 5.8767 LossD: 0.0000 L1: 4.8767\n",
      "Epoch [40/50] Batch [600/1600] LossG: 5.8784 LossD: 0.0000 L1: 4.8782\n",
      "Epoch [40/50] Batch [650/1600] LossG: 5.8751 LossD: 0.0000 L1: 4.8750\n",
      "Epoch [40/50] Batch [700/1600] LossG: 5.8736 LossD: 0.0000 L1: 4.8735\n",
      "Epoch [40/50] Batch [750/1600] LossG: 5.8739 LossD: 0.0000 L1: 4.8738\n",
      "Epoch [40/50] Batch [800/1600] LossG: 5.8667 LossD: 0.0000 L1: 4.8666\n",
      "Epoch [40/50] Batch [850/1600] LossG: 5.8707 LossD: 0.0000 L1: 4.8706\n",
      "Epoch [40/50] Batch [900/1600] LossG: 5.8689 LossD: 0.0000 L1: 4.8688\n",
      "Epoch [40/50] Batch [950/1600] LossG: 5.8581 LossD: 0.0000 L1: 4.8580\n",
      "Epoch [40/50] Batch [1000/1600] LossG: 5.8591 LossD: 0.0000 L1: 4.8590\n",
      "Epoch [40/50] Batch [1050/1600] LossG: 5.8589 LossD: 0.0000 L1: 4.8587\n",
      "Epoch [40/50] Batch [1100/1600] LossG: 5.8673 LossD: 0.0000 L1: 4.8671\n",
      "Epoch [40/50] Batch [1150/1600] LossG: 5.8651 LossD: 0.0000 L1: 4.8650\n",
      "Epoch [40/50] Batch [1200/1600] LossG: 5.8660 LossD: 0.0000 L1: 4.8659\n",
      "Epoch [40/50] Batch [1250/1600] LossG: 5.8602 LossD: 0.0000 L1: 4.8601\n",
      "Epoch [40/50] Batch [1300/1600] LossG: 5.8630 LossD: 0.0000 L1: 4.8629\n",
      "Epoch [40/50] Batch [1350/1600] LossG: 5.8603 LossD: 0.0000 L1: 4.8603\n",
      "Epoch [40/50] Batch [1400/1600] LossG: 5.8620 LossD: 0.0000 L1: 4.8619\n",
      "Epoch [40/50] Batch [1450/1600] LossG: 5.8642 LossD: 0.0000 L1: 4.8641\n",
      "Epoch [40/50] Batch [1500/1600] LossG: 5.8678 LossD: 0.0000 L1: 4.8677\n",
      "Epoch [40/50] Batch [1550/1600] LossG: 5.8709 LossD: 0.0000 L1: 4.8708\n",
      "Epoch [40/50] Batch [1600/1600] LossG: 5.8723 LossD: 0.0000 L1: 4.8722\n",
      "Epoch 40 finished in 333.2s â€” LossG: 5.8723 LossD: 0.0000\n",
      "Epoch [41/50] Batch [50/1600] LossG: 5.7179 LossD: 0.0000 L1: 4.7180\n",
      "Epoch [41/50] Batch [100/1600] LossG: 5.6654 LossD: 0.0000 L1: 4.6655\n",
      "Epoch [41/50] Batch [150/1600] LossG: 5.6747 LossD: 0.0000 L1: 4.6745\n",
      "Epoch [41/50] Batch [200/1600] LossG: 5.7337 LossD: 0.0000 L1: 4.7335\n",
      "Epoch [41/50] Batch [250/1600] LossG: 5.7826 LossD: 0.0000 L1: 4.7825\n",
      "Epoch [41/50] Batch [300/1600] LossG: 5.8011 LossD: 0.0000 L1: 4.8009\n",
      "Epoch [41/50] Batch [350/1600] LossG: 5.8385 LossD: 0.0000 L1: 4.8383\n",
      "Epoch [41/50] Batch [400/1600] LossG: 5.8357 LossD: 0.0000 L1: 4.8355\n",
      "Epoch [41/50] Batch [450/1600] LossG: 5.8233 LossD: 0.0000 L1: 4.8230\n",
      "Epoch [41/50] Batch [500/1600] LossG: 5.8243 LossD: 0.0001 L1: 4.8240\n",
      "Epoch [41/50] Batch [550/1600] LossG: 5.8335 LossD: 0.0001 L1: 4.8332\n",
      "Epoch [41/50] Batch [600/1600] LossG: 5.8261 LossD: 0.0001 L1: 4.8259\n",
      "Epoch [41/50] Batch [650/1600] LossG: 5.8312 LossD: 0.0001 L1: 4.8309\n",
      "Epoch [41/50] Batch [700/1600] LossG: 5.8403 LossD: 0.0001 L1: 4.8401\n",
      "Epoch [41/50] Batch [750/1600] LossG: 5.8479 LossD: 0.0017 L1: 4.8501\n",
      "Epoch [41/50] Batch [800/1600] LossG: 5.8412 LossD: 0.0018 L1: 4.8458\n",
      "Epoch [41/50] Batch [850/1600] LossG: 5.8458 LossD: 0.0018 L1: 4.8500\n",
      "Epoch [41/50] Batch [900/1600] LossG: 5.8446 LossD: 0.0017 L1: 4.8481\n",
      "Epoch [41/50] Batch [950/1600] LossG: 5.8494 LossD: 0.0017 L1: 4.8524\n",
      "Epoch [41/50] Batch [1000/1600] LossG: 5.8495 LossD: 0.0016 L1: 4.8523\n",
      "Epoch [41/50] Batch [1050/1600] LossG: 5.8520 LossD: 0.0015 L1: 4.8547\n",
      "Epoch [41/50] Batch [1100/1600] LossG: 5.8580 LossD: 0.0015 L1: 4.8605\n",
      "Epoch [41/50] Batch [1150/1600] LossG: 5.8662 LossD: 0.0014 L1: 4.8686\n",
      "Epoch [41/50] Batch [1200/1600] LossG: 5.8607 LossD: 0.0014 L1: 4.8629\n",
      "Epoch [41/50] Batch [1250/1600] LossG: 5.8575 LossD: 0.0013 L1: 4.8596\n",
      "Epoch [41/50] Batch [1300/1600] LossG: 5.8626 LossD: 0.0013 L1: 4.8646\n",
      "Epoch [41/50] Batch [1350/1600] LossG: 5.8646 LossD: 0.0012 L1: 4.8665\n",
      "Epoch [41/50] Batch [1400/1600] LossG: 5.8672 LossD: 0.0012 L1: 4.8690\n",
      "Epoch [41/50] Batch [1450/1600] LossG: 5.8657 LossD: 0.0011 L1: 4.8675\n",
      "Epoch [41/50] Batch [1500/1600] LossG: 5.8689 LossD: 0.0011 L1: 4.8707\n",
      "Epoch [41/50] Batch [1550/1600] LossG: 5.8695 LossD: 0.0011 L1: 4.8712\n",
      "Epoch [41/50] Batch [1600/1600] LossG: 5.8708 LossD: 0.0010 L1: 4.8725\n",
      "Epoch 41 finished in 334.7s â€” LossG: 5.8708 LossD: 0.0010\n",
      "Epoch [42/50] Batch [50/1600] LossG: 6.0122 LossD: 0.0000 L1: 5.0124\n",
      "Epoch [42/50] Batch [100/1600] LossG: 5.8731 LossD: 0.0000 L1: 4.8733\n",
      "Epoch [42/50] Batch [150/1600] LossG: 5.8087 LossD: 0.0000 L1: 4.8089\n",
      "Epoch [42/50] Batch [200/1600] LossG: 5.7817 LossD: 0.0000 L1: 4.7818\n",
      "Epoch [42/50] Batch [250/1600] LossG: 5.8339 LossD: 0.0000 L1: 4.8340\n",
      "Epoch [42/50] Batch [300/1600] LossG: 5.9490 LossD: 0.0003 L1: 4.9490\n",
      "Epoch [42/50] Batch [350/1600] LossG: 5.9698 LossD: 0.0002 L1: 4.9698\n",
      "Epoch [42/50] Batch [400/1600] LossG: 5.9434 LossD: 0.0002 L1: 4.9433\n",
      "Epoch [42/50] Batch [450/1600] LossG: 5.9374 LossD: 0.0002 L1: 4.9373\n",
      "Epoch [42/50] Batch [500/1600] LossG: 5.9120 LossD: 0.0002 L1: 4.9119\n",
      "Epoch [42/50] Batch [550/1600] LossG: 5.9061 LossD: 0.0002 L1: 4.9060\n",
      "Epoch [42/50] Batch [600/1600] LossG: 5.8918 LossD: 0.0002 L1: 4.8917\n",
      "Epoch [42/50] Batch [650/1600] LossG: 5.8754 LossD: 0.0002 L1: 4.8753\n",
      "Epoch [42/50] Batch [700/1600] LossG: 5.8638 LossD: 0.0002 L1: 4.8636\n",
      "Epoch [42/50] Batch [750/1600] LossG: 5.8625 LossD: 0.0001 L1: 4.8623\n",
      "Epoch [42/50] Batch [800/1600] LossG: 5.8636 LossD: 0.0002 L1: 4.8632\n",
      "Epoch [42/50] Batch [850/1600] LossG: 5.8661 LossD: 0.0001 L1: 4.8658\n",
      "Epoch [42/50] Batch [900/1600] LossG: 5.8711 LossD: 0.0001 L1: 4.8708\n",
      "Epoch [42/50] Batch [950/1600] LossG: 5.8718 LossD: 0.0001 L1: 4.8716\n",
      "Epoch [42/50] Batch [1000/1600] LossG: 5.8752 LossD: 0.0001 L1: 4.8750\n",
      "Epoch [42/50] Batch [1050/1600] LossG: 5.8757 LossD: 0.0001 L1: 4.8755\n",
      "Epoch [42/50] Batch [1100/1600] LossG: 5.8716 LossD: 0.0001 L1: 4.8714\n",
      "Epoch [42/50] Batch [1150/1600] LossG: 5.8698 LossD: 0.0001 L1: 4.8696\n",
      "Epoch [42/50] Batch [1200/1600] LossG: 5.8700 LossD: 0.0001 L1: 4.8699\n",
      "Epoch [42/50] Batch [1250/1600] LossG: 5.8725 LossD: 0.0001 L1: 4.8723\n",
      "Epoch [42/50] Batch [1300/1600] LossG: 5.8649 LossD: 0.0001 L1: 4.8648\n",
      "Epoch [42/50] Batch [1350/1600] LossG: 5.8630 LossD: 0.0001 L1: 4.8628\n",
      "Epoch [42/50] Batch [1400/1600] LossG: 5.8626 LossD: 0.0001 L1: 4.8625\n",
      "Epoch [42/50] Batch [1450/1600] LossG: 5.8591 LossD: 0.0001 L1: 4.8590\n",
      "Epoch [42/50] Batch [1500/1600] LossG: 5.8599 LossD: 0.0001 L1: 4.8597\n",
      "Epoch [42/50] Batch [1550/1600] LossG: 5.8614 LossD: 0.0001 L1: 4.8612\n",
      "Epoch [42/50] Batch [1600/1600] LossG: 5.8554 LossD: 0.0001 L1: 4.8553\n",
      "Epoch 42 finished in 334.0s â€” LossG: 5.8554 LossD: 0.0001\n",
      "Epoch [43/50] Batch [50/1600] LossG: 6.0752 LossD: 0.0000 L1: 5.0752\n",
      "Epoch [43/50] Batch [100/1600] LossG: 5.8710 LossD: 0.0000 L1: 4.8710\n",
      "Epoch [43/50] Batch [150/1600] LossG: 5.8448 LossD: 0.0000 L1: 4.8449\n",
      "Epoch [43/50] Batch [200/1600] LossG: 5.8752 LossD: 0.0000 L1: 4.8753\n",
      "Epoch [43/50] Batch [250/1600] LossG: 5.8523 LossD: 0.0000 L1: 4.8523\n",
      "Epoch [43/50] Batch [300/1600] LossG: 5.8459 LossD: 0.0000 L1: 4.8460\n",
      "Epoch [43/50] Batch [350/1600] LossG: 5.8494 LossD: 0.0000 L1: 4.8491\n",
      "Epoch [43/50] Batch [400/1600] LossG: 5.8578 LossD: 0.0000 L1: 4.8576\n",
      "Epoch [43/50] Batch [450/1600] LossG: 5.8528 LossD: 0.0000 L1: 4.8526\n",
      "Epoch [43/50] Batch [500/1600] LossG: 5.8576 LossD: 0.0000 L1: 4.8573\n",
      "Epoch [43/50] Batch [550/1600] LossG: 5.8491 LossD: 0.0000 L1: 4.8489\n",
      "Epoch [43/50] Batch [600/1600] LossG: 5.8410 LossD: 0.0000 L1: 4.8408\n",
      "Epoch [43/50] Batch [650/1600] LossG: 5.8470 LossD: 0.0000 L1: 4.8468\n",
      "Epoch [43/50] Batch [700/1600] LossG: 5.8477 LossD: 0.0000 L1: 4.8476\n",
      "Epoch [43/50] Batch [750/1600] LossG: 5.8610 LossD: 0.0000 L1: 4.8609\n",
      "Epoch [43/50] Batch [800/1600] LossG: 5.8603 LossD: 0.0000 L1: 4.8601\n",
      "Epoch [43/50] Batch [850/1600] LossG: 5.8573 LossD: 0.0000 L1: 4.8572\n",
      "Epoch [43/50] Batch [900/1600] LossG: 5.8537 LossD: 0.0000 L1: 4.8536\n",
      "Epoch [43/50] Batch [950/1600] LossG: 5.8563 LossD: 0.0000 L1: 4.8561\n",
      "Epoch [43/50] Batch [1000/1600] LossG: 5.8478 LossD: 0.0000 L1: 4.8476\n",
      "Epoch [43/50] Batch [1050/1600] LossG: 5.8470 LossD: 0.0000 L1: 4.8468\n",
      "Epoch [43/50] Batch [1100/1600] LossG: 5.8480 LossD: 0.0000 L1: 4.8479\n",
      "Epoch [43/50] Batch [1150/1600] LossG: 5.8393 LossD: 0.0000 L1: 4.8392\n",
      "Epoch [43/50] Batch [1200/1600] LossG: 5.8337 LossD: 0.0000 L1: 4.8336\n",
      "Epoch [43/50] Batch [1250/1600] LossG: 5.8342 LossD: 0.0000 L1: 4.8341\n",
      "Epoch [43/50] Batch [1300/1600] LossG: 5.8316 LossD: 0.0000 L1: 4.8315\n",
      "Epoch [43/50] Batch [1350/1600] LossG: 5.8292 LossD: 0.0000 L1: 4.8290\n",
      "Epoch [43/50] Batch [1400/1600] LossG: 5.8274 LossD: 0.0000 L1: 4.8273\n",
      "Epoch [43/50] Batch [1450/1600] LossG: 5.8263 LossD: 0.0000 L1: 4.8262\n",
      "Epoch [43/50] Batch [1500/1600] LossG: 5.8224 LossD: 0.0000 L1: 4.8223\n",
      "Epoch [43/50] Batch [1550/1600] LossG: 5.8216 LossD: 0.0000 L1: 4.8215\n",
      "Epoch [43/50] Batch [1600/1600] LossG: 5.8295 LossD: 0.0000 L1: 4.8294\n",
      "Epoch 43 finished in 339.3s â€” LossG: 5.8295 LossD: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 163 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/50] Batch [50/1600] LossG: 5.6254 LossD: 0.0000 L1: 4.6255\n",
      "Epoch [44/50] Batch [100/1600] LossG: 5.7232 LossD: 0.0000 L1: 4.7233\n",
      "Epoch [44/50] Batch [150/1600] LossG: 5.6856 LossD: 0.0000 L1: 4.6854\n",
      "Epoch [44/50] Batch [200/1600] LossG: 5.7399 LossD: 0.0000 L1: 4.7397\n",
      "Epoch [44/50] Batch [250/1600] LossG: 5.7637 LossD: 0.0000 L1: 4.7636\n",
      "Epoch [44/50] Batch [300/1600] LossG: 5.7905 LossD: 0.0000 L1: 4.7903\n",
      "Epoch [44/50] Batch [350/1600] LossG: 5.7746 LossD: 0.0000 L1: 4.7745\n",
      "Epoch [44/50] Batch [400/1600] LossG: 5.7922 LossD: 0.0034 L1: 4.7979\n",
      "Epoch [44/50] Batch [450/1600] LossG: 5.7989 LossD: 0.0031 L1: 4.8034\n",
      "Epoch [44/50] Batch [500/1600] LossG: 5.7970 LossD: 0.0028 L1: 4.8009\n",
      "Epoch [44/50] Batch [550/1600] LossG: 5.8048 LossD: 0.0026 L1: 4.8082\n",
      "Epoch [44/50] Batch [600/1600] LossG: 5.7985 LossD: 0.0024 L1: 4.8017\n",
      "Epoch [44/50] Batch [650/1600] LossG: 5.7960 LossD: 0.0022 L1: 4.7989\n",
      "Epoch [44/50] Batch [700/1600] LossG: 5.8041 LossD: 0.0021 L1: 4.8069\n",
      "Epoch [44/50] Batch [750/1600] LossG: 5.8091 LossD: 0.0020 L1: 4.8116\n",
      "Epoch [44/50] Batch [800/1600] LossG: 5.8116 LossD: 0.0018 L1: 4.8140\n",
      "Epoch [44/50] Batch [850/1600] LossG: 5.8137 LossD: 0.0017 L1: 4.8159\n",
      "Epoch [44/50] Batch [900/1600] LossG: 5.8181 LossD: 0.0016 L1: 4.8202\n",
      "Epoch [44/50] Batch [950/1600] LossG: 5.8188 LossD: 0.0016 L1: 4.8208\n",
      "Epoch [44/50] Batch [1000/1600] LossG: 5.8278 LossD: 0.0015 L1: 4.8297\n",
      "Epoch [44/50] Batch [1050/1600] LossG: 5.8280 LossD: 0.0014 L1: 4.8298\n",
      "Epoch [44/50] Batch [1100/1600] LossG: 5.8252 LossD: 0.0014 L1: 4.8269\n",
      "Epoch [44/50] Batch [1150/1600] LossG: 5.8328 LossD: 0.0013 L1: 4.8345\n",
      "Epoch [44/50] Batch [1200/1600] LossG: 5.8338 LossD: 0.0013 L1: 4.8355\n",
      "Epoch [44/50] Batch [1250/1600] LossG: 5.8269 LossD: 0.0012 L1: 4.8285\n",
      "Epoch [44/50] Batch [1300/1600] LossG: 5.8272 LossD: 0.0012 L1: 4.8287\n",
      "Epoch [44/50] Batch [1350/1600] LossG: 5.8257 LossD: 0.0011 L1: 4.8271\n",
      "Epoch [44/50] Batch [1400/1600] LossG: 5.8288 LossD: 0.0011 L1: 4.8302\n",
      "Epoch [44/50] Batch [1450/1600] LossG: 5.8256 LossD: 0.0011 L1: 4.8269\n",
      "Epoch [44/50] Batch [1500/1600] LossG: 5.8308 LossD: 0.0010 L1: 4.8320\n",
      "Epoch [44/50] Batch [1550/1600] LossG: 5.8333 LossD: 0.0010 L1: 4.8346\n",
      "Epoch [44/50] Batch [1600/1600] LossG: 5.8360 LossD: 0.0010 L1: 4.8371\n",
      "Epoch 44 finished in 342.0s â€” LossG: 5.8360 LossD: 0.0010\n",
      "Epoch [45/50] Batch [50/1600] LossG: 5.7374 LossD: 0.0000 L1: 4.7376\n",
      "Epoch [45/50] Batch [100/1600] LossG: 5.7878 LossD: 0.0000 L1: 4.7880\n",
      "Epoch [45/50] Batch [150/1600] LossG: 5.7741 LossD: 0.0000 L1: 4.7743\n",
      "Epoch [45/50] Batch [200/1600] LossG: 5.8078 LossD: 0.0000 L1: 4.8080\n",
      "Epoch [45/50] Batch [250/1600] LossG: 5.7853 LossD: 0.0000 L1: 4.7855\n",
      "Epoch [45/50] Batch [300/1600] LossG: 5.7889 LossD: 0.0000 L1: 4.7889\n",
      "Epoch [45/50] Batch [350/1600] LossG: 5.7802 LossD: 0.0000 L1: 4.7803\n",
      "Epoch [45/50] Batch [400/1600] LossG: 5.8027 LossD: 0.0000 L1: 4.8028\n",
      "Epoch [45/50] Batch [450/1600] LossG: 5.8128 LossD: 0.0000 L1: 4.8129\n",
      "Epoch [45/50] Batch [500/1600] LossG: 5.8102 LossD: 0.0000 L1: 4.8104\n",
      "Epoch [45/50] Batch [550/1600] LossG: 5.8171 LossD: 0.0000 L1: 4.8172\n",
      "Epoch [45/50] Batch [600/1600] LossG: 5.8243 LossD: 0.0000 L1: 4.8244\n",
      "Epoch [45/50] Batch [650/1600] LossG: 5.8241 LossD: 0.0000 L1: 4.8242\n",
      "Epoch [45/50] Batch [700/1600] LossG: 5.8271 LossD: 0.0000 L1: 4.8272\n",
      "Epoch [45/50] Batch [750/1600] LossG: 5.8344 LossD: 0.0000 L1: 4.8344\n",
      "Epoch [45/50] Batch [800/1600] LossG: 5.8408 LossD: 0.0000 L1: 4.8409\n",
      "Epoch [45/50] Batch [850/1600] LossG: 5.8499 LossD: 0.0000 L1: 4.8500\n",
      "Epoch [45/50] Batch [900/1600] LossG: 5.8463 LossD: 0.0000 L1: 4.8464\n",
      "Epoch [45/50] Batch [950/1600] LossG: 5.8491 LossD: 0.0000 L1: 4.8492\n",
      "Epoch [45/50] Batch [1000/1600] LossG: 5.8482 LossD: 0.0000 L1: 4.8483\n",
      "Epoch [45/50] Batch [1050/1600] LossG: 5.8432 LossD: 0.0000 L1: 4.8432\n",
      "Epoch [45/50] Batch [1100/1600] LossG: 5.8490 LossD: 0.0000 L1: 4.8490\n",
      "Epoch [45/50] Batch [1150/1600] LossG: 5.8494 LossD: 0.0000 L1: 4.8494\n",
      "Epoch [45/50] Batch [1200/1600] LossG: 5.8451 LossD: 0.0000 L1: 4.8451\n",
      "Epoch [45/50] Batch [1250/1600] LossG: 5.8473 LossD: 0.0000 L1: 4.8473\n",
      "Epoch [45/50] Batch [1300/1600] LossG: 5.8487 LossD: 0.0000 L1: 4.8487\n",
      "Epoch [45/50] Batch [1350/1600] LossG: 5.8464 LossD: 0.0000 L1: 4.8465\n",
      "Epoch [45/50] Batch [1400/1600] LossG: 5.8467 LossD: 0.0000 L1: 4.8467\n",
      "Epoch [45/50] Batch [1450/1600] LossG: 5.8521 LossD: 0.0000 L1: 4.8521\n",
      "Epoch [45/50] Batch [1500/1600] LossG: 5.8474 LossD: 0.0000 L1: 4.8475\n",
      "Epoch [45/50] Batch [1550/1600] LossG: 5.8451 LossD: 0.0000 L1: 4.8452\n",
      "Epoch [45/50] Batch [1600/1600] LossG: 5.8495 LossD: 0.0000 L1: 4.8495\n",
      "Epoch 45 finished in 340.6s â€” LossG: 5.8495 LossD: 0.0000\n",
      "Epoch [46/50] Batch [50/1600] LossG: 5.7791 LossD: 0.0000 L1: 4.7792\n",
      "Epoch [46/50] Batch [100/1600] LossG: 5.7449 LossD: 0.0000 L1: 4.7448\n",
      "Epoch [46/50] Batch [150/1600] LossG: 5.7696 LossD: 0.0000 L1: 4.7696\n",
      "Epoch [46/50] Batch [200/1600] LossG: 5.7408 LossD: 0.0003 L1: 4.7423\n",
      "Epoch [46/50] Batch [250/1600] LossG: 5.7530 LossD: 0.0003 L1: 4.7533\n",
      "Epoch [46/50] Batch [300/1600] LossG: 5.7767 LossD: 0.0003 L1: 4.7769\n",
      "Epoch [46/50] Batch [350/1600] LossG: 5.7971 LossD: 0.0003 L1: 4.7981\n",
      "Epoch [46/50] Batch [400/1600] LossG: 5.7985 LossD: 0.0006 L1: 4.7997\n",
      "Epoch [46/50] Batch [450/1600] LossG: 5.8250 LossD: 0.0006 L1: 4.8258\n",
      "Epoch [46/50] Batch [500/1600] LossG: 5.8298 LossD: 0.0006 L1: 4.8306\n",
      "Epoch [46/50] Batch [550/1600] LossG: 5.8364 LossD: 0.0005 L1: 4.8369\n",
      "Epoch [46/50] Batch [600/1600] LossG: 5.8312 LossD: 0.0005 L1: 4.8317\n",
      "Epoch [46/50] Batch [650/1600] LossG: 5.8336 LossD: 0.0004 L1: 4.8340\n",
      "Epoch [46/50] Batch [700/1600] LossG: 5.8392 LossD: 0.0004 L1: 4.8396\n",
      "Epoch [46/50] Batch [750/1600] LossG: 5.8412 LossD: 0.0004 L1: 4.8416\n",
      "Epoch [46/50] Batch [800/1600] LossG: 5.8435 LossD: 0.0004 L1: 4.8440\n",
      "Epoch [46/50] Batch [850/1600] LossG: 5.8372 LossD: 0.0004 L1: 4.8376\n",
      "Epoch [46/50] Batch [900/1600] LossG: 5.8408 LossD: 0.0003 L1: 4.8412\n",
      "Epoch [46/50] Batch [950/1600] LossG: 5.8434 LossD: 0.0003 L1: 4.8438\n",
      "Epoch [46/50] Batch [1000/1600] LossG: 5.8363 LossD: 0.0003 L1: 4.8366\n",
      "Epoch [46/50] Batch [1050/1600] LossG: 5.8332 LossD: 0.0003 L1: 4.8335\n",
      "Epoch [46/50] Batch [1100/1600] LossG: 5.8300 LossD: 0.0003 L1: 4.8303\n",
      "Epoch [46/50] Batch [1150/1600] LossG: 5.8344 LossD: 0.0003 L1: 4.8347\n",
      "Epoch [46/50] Batch [1200/1600] LossG: 5.8382 LossD: 0.0003 L1: 4.8385\n",
      "Epoch [46/50] Batch [1250/1600] LossG: 5.8434 LossD: 0.0002 L1: 4.8436\n",
      "Epoch [46/50] Batch [1300/1600] LossG: 5.8338 LossD: 0.0002 L1: 4.8340\n",
      "Epoch [46/50] Batch [1350/1600] LossG: 5.8326 LossD: 0.0002 L1: 4.8328\n",
      "Epoch [46/50] Batch [1400/1600] LossG: 5.8316 LossD: 0.0002 L1: 4.8318\n",
      "Epoch [46/50] Batch [1450/1600] LossG: 5.8344 LossD: 0.0002 L1: 4.8345\n",
      "Epoch [46/50] Batch [1500/1600] LossG: 5.8381 LossD: 0.0002 L1: 4.8383\n",
      "Epoch [46/50] Batch [1550/1600] LossG: 5.8385 LossD: 0.0002 L1: 4.8387\n",
      "Epoch [46/50] Batch [1600/1600] LossG: 5.8341 LossD: 0.0002 L1: 4.8343\n",
      "Epoch 46 finished in 334.5s â€” LossG: 5.8341 LossD: 0.0002\n",
      "Epoch [47/50] Batch [50/1600] LossG: 5.8561 LossD: 0.0000 L1: 4.8562\n",
      "Epoch [47/50] Batch [100/1600] LossG: 5.9114 LossD: 0.0000 L1: 4.9115\n",
      "Epoch [47/50] Batch [150/1600] LossG: 5.9202 LossD: 0.0000 L1: 4.9201\n",
      "Epoch [47/50] Batch [200/1600] LossG: 5.8724 LossD: 0.0000 L1: 4.8725\n",
      "Epoch [47/50] Batch [250/1600] LossG: 5.8424 LossD: 0.0000 L1: 4.8426\n",
      "Epoch [47/50] Batch [300/1600] LossG: 5.8385 LossD: 0.0000 L1: 4.8387\n",
      "Epoch [47/50] Batch [350/1600] LossG: 5.8188 LossD: 0.0000 L1: 4.8190\n",
      "Epoch [47/50] Batch [400/1600] LossG: 5.8001 LossD: 0.0000 L1: 4.8003\n",
      "Epoch [47/50] Batch [450/1600] LossG: 5.8232 LossD: 0.0000 L1: 4.8234\n",
      "Epoch [47/50] Batch [500/1600] LossG: 5.8253 LossD: 0.0000 L1: 4.8255\n",
      "Epoch [47/50] Batch [550/1600] LossG: 5.8221 LossD: 0.0000 L1: 4.8223\n",
      "Epoch [47/50] Batch [600/1600] LossG: 5.8202 LossD: 0.0000 L1: 4.8204\n",
      "Epoch [47/50] Batch [650/1600] LossG: 5.8225 LossD: 0.0000 L1: 4.8226\n",
      "Epoch [47/50] Batch [700/1600] LossG: 5.8195 LossD: 0.0000 L1: 4.8197\n",
      "Epoch [47/50] Batch [750/1600] LossG: 5.8245 LossD: 0.0000 L1: 4.8246\n",
      "Epoch [47/50] Batch [800/1600] LossG: 5.8182 LossD: 0.0000 L1: 4.8183\n",
      "Epoch [47/50] Batch [850/1600] LossG: 5.8170 LossD: 0.0000 L1: 4.8171\n",
      "Epoch [47/50] Batch [900/1600] LossG: 5.8172 LossD: 0.0000 L1: 4.8173\n",
      "Epoch [47/50] Batch [950/1600] LossG: 5.8241 LossD: 0.0000 L1: 4.8241\n",
      "Epoch [47/50] Batch [1000/1600] LossG: 5.8230 LossD: 0.0000 L1: 4.8231\n",
      "Epoch [47/50] Batch [1050/1600] LossG: 5.8234 LossD: 0.0000 L1: 4.8234\n",
      "Epoch [47/50] Batch [1100/1600] LossG: 5.8248 LossD: 0.0000 L1: 4.8248\n",
      "Epoch [47/50] Batch [1150/1600] LossG: 5.8212 LossD: 0.0001 L1: 4.8212\n",
      "Epoch [47/50] Batch [1200/1600] LossG: 5.8184 LossD: 0.0001 L1: 4.8184\n",
      "Epoch [47/50] Batch [1250/1600] LossG: 5.8147 LossD: 0.0000 L1: 4.8147\n",
      "Epoch [47/50] Batch [1300/1600] LossG: 5.8209 LossD: 0.0001 L1: 4.8209\n",
      "Epoch [47/50] Batch [1350/1600] LossG: 5.8207 LossD: 0.0001 L1: 4.8207\n",
      "Epoch [47/50] Batch [1400/1600] LossG: 5.8210 LossD: 0.0001 L1: 4.8209\n",
      "Epoch [47/50] Batch [1450/1600] LossG: 5.8202 LossD: 0.0001 L1: 4.8201\n",
      "Epoch [47/50] Batch [1500/1600] LossG: 5.8205 LossD: 0.0001 L1: 4.8204\n",
      "Epoch [47/50] Batch [1550/1600] LossG: 5.8210 LossD: 0.0001 L1: 4.8209\n",
      "Epoch [47/50] Batch [1600/1600] LossG: 5.8206 LossD: 0.0001 L1: 4.8205\n",
      "Epoch 47 finished in 333.8s â€” LossG: 5.8206 LossD: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 69 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50] Batch [50/1600] LossG: 5.8653 LossD: 0.0001 L1: 4.8658\n",
      "Epoch [48/50] Batch [100/1600] LossG: 5.8865 LossD: 0.0000 L1: 4.8867\n",
      "Epoch [48/50] Batch [150/1600] LossG: 5.8416 LossD: 0.0000 L1: 4.8418\n",
      "Epoch [48/50] Batch [200/1600] LossG: 5.8471 LossD: 0.0000 L1: 4.8472\n",
      "Epoch [48/50] Batch [250/1600] LossG: 5.8485 LossD: 0.0000 L1: 4.8486\n",
      "Epoch [48/50] Batch [300/1600] LossG: 5.8171 LossD: 0.0000 L1: 4.8172\n",
      "Epoch [48/50] Batch [350/1600] LossG: 5.8111 LossD: 0.0000 L1: 4.8112\n",
      "Epoch [48/50] Batch [400/1600] LossG: 5.8193 LossD: 0.0000 L1: 4.8194\n",
      "Epoch [48/50] Batch [450/1600] LossG: 5.8224 LossD: 0.0000 L1: 4.8225\n",
      "Epoch [48/50] Batch [500/1600] LossG: 5.8411 LossD: 0.0000 L1: 4.8409\n",
      "Epoch [48/50] Batch [550/1600] LossG: 5.8331 LossD: 0.0000 L1: 4.8329\n",
      "Epoch [48/50] Batch [600/1600] LossG: 5.8433 LossD: 0.0000 L1: 4.8431\n",
      "Epoch [48/50] Batch [650/1600] LossG: 5.8313 LossD: 0.0000 L1: 4.8311\n",
      "Epoch [48/50] Batch [700/1600] LossG: 5.8158 LossD: 0.0000 L1: 4.8156\n",
      "Epoch [48/50] Batch [750/1600] LossG: 5.8185 LossD: 0.0000 L1: 4.8184\n",
      "Epoch [48/50] Batch [800/1600] LossG: 5.8149 LossD: 0.0000 L1: 4.8148\n",
      "Epoch [48/50] Batch [850/1600] LossG: 5.8144 LossD: 0.0000 L1: 4.8142\n",
      "Epoch [48/50] Batch [900/1600] LossG: 5.8180 LossD: 0.0000 L1: 4.8178\n",
      "Epoch [48/50] Batch [950/1600] LossG: 5.8179 LossD: 0.0000 L1: 4.8176\n",
      "Epoch [48/50] Batch [1000/1600] LossG: 5.8182 LossD: 0.0000 L1: 4.8180\n",
      "Epoch [48/50] Batch [1050/1600] LossG: 5.8193 LossD: 0.0000 L1: 4.8191\n",
      "Epoch [48/50] Batch [1100/1600] LossG: 5.8123 LossD: 0.0018 L1: 4.8161\n",
      "Epoch [48/50] Batch [1150/1600] LossG: 5.8108 LossD: 0.0017 L1: 4.8147\n",
      "Epoch [48/50] Batch [1200/1600] LossG: 5.8085 LossD: 0.0017 L1: 4.8135\n",
      "Epoch [48/50] Batch [1250/1600] LossG: 5.8089 LossD: 0.0017 L1: 4.8140\n",
      "Epoch [48/50] Batch [1300/1600] LossG: 5.8149 LossD: 0.0017 L1: 4.8197\n",
      "Epoch [48/50] Batch [1350/1600] LossG: 5.8147 LossD: 0.0016 L1: 4.8192\n",
      "Epoch [48/50] Batch [1400/1600] LossG: 5.8121 LossD: 0.0016 L1: 4.8164\n",
      "Epoch [48/50] Batch [1450/1600] LossG: 5.8116 LossD: 0.0015 L1: 4.8158\n",
      "Epoch [48/50] Batch [1500/1600] LossG: 5.8100 LossD: 0.0015 L1: 4.8139\n",
      "Epoch [48/50] Batch [1550/1600] LossG: 5.8094 LossD: 0.0015 L1: 4.8133\n",
      "Epoch [48/50] Batch [1600/1600] LossG: 5.7989 LossD: 0.0014 L1: 4.8027\n",
      "Epoch 48 finished in 337.7s â€” LossG: 5.7989 LossD: 0.0014\n",
      "Epoch [49/50] Batch [50/1600] LossG: 5.8009 LossD: 0.0001 L1: 4.8011\n",
      "Epoch [49/50] Batch [100/1600] LossG: 5.7246 LossD: 0.0001 L1: 4.7249\n",
      "Epoch [49/50] Batch [150/1600] LossG: 5.7228 LossD: 0.0001 L1: 4.7231\n",
      "Epoch [49/50] Batch [200/1600] LossG: 5.7421 LossD: 0.0001 L1: 4.7424\n",
      "Epoch [49/50] Batch [250/1600] LossG: 5.7376 LossD: 0.0001 L1: 4.7379\n",
      "Epoch [49/50] Batch [300/1600] LossG: 5.7576 LossD: 0.0001 L1: 4.7578\n",
      "Epoch [49/50] Batch [350/1600] LossG: 5.7523 LossD: 0.0001 L1: 4.7525\n",
      "Epoch [49/50] Batch [400/1600] LossG: 5.7482 LossD: 0.0001 L1: 4.7484\n",
      "Epoch [49/50] Batch [450/1600] LossG: 5.7832 LossD: 0.0001 L1: 4.7834\n",
      "Epoch [49/50] Batch [500/1600] LossG: 5.7947 LossD: 0.0001 L1: 4.7947\n",
      "Epoch [49/50] Batch [550/1600] LossG: 5.7987 LossD: 0.0001 L1: 4.7985\n",
      "Epoch [49/50] Batch [600/1600] LossG: 5.8123 LossD: 0.0001 L1: 4.8120\n",
      "Epoch [49/50] Batch [650/1600] LossG: 5.8137 LossD: 0.0001 L1: 4.8133\n",
      "Epoch [49/50] Batch [700/1600] LossG: 5.8146 LossD: 0.0001 L1: 4.8143\n",
      "Epoch [49/50] Batch [750/1600] LossG: 5.8097 LossD: 0.0001 L1: 4.8094\n",
      "Epoch [49/50] Batch [800/1600] LossG: 5.8004 LossD: 0.0001 L1: 4.8000\n",
      "Epoch [49/50] Batch [850/1600] LossG: 5.7984 LossD: 0.0001 L1: 4.7980\n",
      "Epoch [49/50] Batch [900/1600] LossG: 5.7954 LossD: 0.0001 L1: 4.7950\n",
      "Epoch [49/50] Batch [950/1600] LossG: 5.7942 LossD: 0.0001 L1: 4.7938\n",
      "Epoch [49/50] Batch [1000/1600] LossG: 5.8006 LossD: 0.0001 L1: 4.8003\n",
      "Epoch [49/50] Batch [1050/1600] LossG: 5.7903 LossD: 0.0001 L1: 4.7900\n",
      "Epoch [49/50] Batch [1100/1600] LossG: 5.7917 LossD: 0.0001 L1: 4.7914\n",
      "Epoch [49/50] Batch [1150/1600] LossG: 5.7808 LossD: 0.0001 L1: 4.7805\n",
      "Epoch [49/50] Batch [1200/1600] LossG: 5.7801 LossD: 0.0001 L1: 4.7798\n",
      "Epoch [49/50] Batch [1250/1600] LossG: 5.7813 LossD: 0.0001 L1: 4.7810\n",
      "Epoch [49/50] Batch [1300/1600] LossG: 5.7838 LossD: 0.0001 L1: 4.7835\n",
      "Epoch [49/50] Batch [1350/1600] LossG: 5.7886 LossD: 0.0001 L1: 4.7884\n",
      "Epoch [49/50] Batch [1400/1600] LossG: 5.7928 LossD: 0.0001 L1: 4.7926\n",
      "Epoch [49/50] Batch [1450/1600] LossG: 5.7916 LossD: 0.0001 L1: 4.7914\n",
      "Epoch [49/50] Batch [1500/1600] LossG: 5.7943 LossD: 0.0001 L1: 4.7941\n",
      "Epoch [49/50] Batch [1550/1600] LossG: 5.7974 LossD: 0.0001 L1: 4.7972\n",
      "Epoch [49/50] Batch [1600/1600] LossG: 5.7953 LossD: 0.0001 L1: 4.7951\n",
      "Epoch 49 finished in 333.6s â€” LossG: 5.7953 LossD: 0.0001\n",
      "Epoch [50/50] Batch [50/1600] LossG: 5.7108 LossD: 0.0000 L1: 4.7108\n",
      "Epoch [50/50] Batch [100/1600] LossG: 5.6211 LossD: 0.0000 L1: 4.6211\n",
      "Epoch [50/50] Batch [150/1600] LossG: 5.6450 LossD: 0.0000 L1: 4.6450\n",
      "Epoch [50/50] Batch [200/1600] LossG: 5.6310 LossD: 0.0000 L1: 4.6307\n",
      "Epoch [50/50] Batch [250/1600] LossG: 5.6663 LossD: 0.0000 L1: 4.6661\n",
      "Epoch [50/50] Batch [300/1600] LossG: 5.6995 LossD: 0.0000 L1: 4.6993\n",
      "Epoch [50/50] Batch [350/1600] LossG: 5.6982 LossD: 0.0000 L1: 4.6980\n",
      "Epoch [50/50] Batch [400/1600] LossG: 5.7045 LossD: 0.0000 L1: 4.7043\n",
      "Epoch [50/50] Batch [450/1600] LossG: 5.7075 LossD: 0.0000 L1: 4.7074\n",
      "Epoch [50/50] Batch [500/1600] LossG: 5.7134 LossD: 0.0000 L1: 4.7133\n",
      "Epoch [50/50] Batch [550/1600] LossG: 5.7232 LossD: 0.0000 L1: 4.7230\n",
      "Epoch [50/50] Batch [600/1600] LossG: 5.7302 LossD: 0.0000 L1: 4.7301\n",
      "Epoch [50/50] Batch [650/1600] LossG: 5.7455 LossD: 0.0000 L1: 4.7457\n",
      "Epoch [50/50] Batch [700/1600] LossG: 5.7607 LossD: 0.0026 L1: 4.7673\n",
      "Epoch [50/50] Batch [750/1600] LossG: 5.7683 LossD: 0.0027 L1: 4.7753\n",
      "Epoch [50/50] Batch [800/1600] LossG: 5.7823 LossD: 0.0026 L1: 4.7890\n",
      "Epoch [50/50] Batch [850/1600] LossG: 5.7869 LossD: 0.0025 L1: 4.7930\n",
      "Epoch [50/50] Batch [900/1600] LossG: 5.7902 LossD: 0.0024 L1: 4.7959\n",
      "Epoch [50/50] Batch [950/1600] LossG: 5.7972 LossD: 0.0023 L1: 4.8030\n",
      "Epoch [50/50] Batch [1000/1600] LossG: 5.8019 LossD: 0.0022 L1: 4.8077\n",
      "Epoch [50/50] Batch [1050/1600] LossG: 5.8012 LossD: 0.0021 L1: 4.8072\n",
      "Epoch [50/50] Batch [1100/1600] LossG: 5.7994 LossD: 0.0020 L1: 4.8053\n",
      "Epoch [50/50] Batch [1150/1600] LossG: 5.7938 LossD: 0.0019 L1: 4.7996\n",
      "Epoch [50/50] Batch [1200/1600] LossG: 5.7993 LossD: 0.0019 L1: 4.8051\n",
      "Epoch [50/50] Batch [1250/1600] LossG: 5.7967 LossD: 0.0018 L1: 4.8023\n",
      "Epoch [50/50] Batch [1300/1600] LossG: 5.7944 LossD: 0.0017 L1: 4.7998\n",
      "Epoch [50/50] Batch [1350/1600] LossG: 5.7959 LossD: 0.0017 L1: 4.8012\n",
      "Epoch [50/50] Batch [1400/1600] LossG: 5.7934 LossD: 0.0016 L1: 4.7985\n",
      "Epoch [50/50] Batch [1450/1600] LossG: 5.7934 LossD: 0.0016 L1: 4.7984\n",
      "Epoch [50/50] Batch [1500/1600] LossG: 5.8009 LossD: 0.0015 L1: 4.8057\n",
      "Epoch [50/50] Batch [1550/1600] LossG: 5.8073 LossD: 0.0015 L1: 4.8125\n",
      "Epoch [50/50] Batch [1600/1600] LossG: 5.8120 LossD: 0.0015 L1: 4.8170\n",
      "Epoch 50 finished in 332.1s â€” LossG: 5.8120 LossD: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 62 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n",
      "C:\\Users\\len\\AppData\\Local\\Temp\\ipykernel_4696\\1628243315.py:12: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 21 negative Z values that have been clipped to zero\n",
      "  rgb = color.lab2rgb(lab)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set multiprocessing start method (important for Windows)\n",
    "    import multiprocessing\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "    \n",
    "    # Run the training loop\n",
    "    train_loop(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eefb95",
   "metadata": {},
   "source": [
    "Notes & Next steps\n",
    " - This notebook is a starting point. You should verify dataset pairing (names) and adjust the path logic in `PairedSARDataset` if needed.\n",
    " - Experiment with L1 weight, perceptual losses (VGG), and SSIM/LPIPS metrics for better visual fidelity.\n",
    " - For large images, use tiled inference with overlap and blending to remove seam artifacts.\n",
    " - If you have dual-polarization SAR (VV+VH), change `in_channels=2` in the UNet and modify the dataset to return two-channel inputs.\n",
    " - To fine-tune per-terrain, train the general model first then reinitialize the dataset to only that terrain and continue training from the saved best checkpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00b37b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarpyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
