{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'  # Prevents OMP conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU active: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_gpu():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"⚠️ CUDA not available - Falling back to CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "    try:\n",
    "        # Get first available GPU\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        \n",
    "        # Test communication\n",
    "        test_tensor = torch.tensor([1.0]).to(device)\n",
    "        if test_tensor.item() == 1.0:\n",
    "            print(f\"✅ GPU active: {torch.cuda.get_device_name(0)}\")\n",
    "            return device\n",
    "    except RuntimeError as e:\n",
    "        print(f\"❌ GPU test failed: {str(e)}\")\n",
    "    \n",
    "    print(\"⚠️ Falling back to CPU\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = check_gpu()\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()  # Try freeing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "PyTorch: 2.7.0+cu118\n",
      "CUDA Version: 11.8\n",
      "CUDA Available: True\n",
      "Device Count: 1\n",
      "--------------------------------------------------\n",
      "nvidia-smi output:\n",
      " Thu May 29 12:13:28 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.97                 Driver Version: 555.97         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   37C    P8              1W /  115W |     795MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3040      C   C:\\Users\\shash\\anaconda3\\python.exe         N/A      |\n",
      "|    0   N/A  N/A     20448    C+G   ...m Files (x86)\\Overwolf\\Overwolf.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda if hasattr(torch.version, 'cuda') else 'N/A'}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device Count: {torch.cuda.device_count()}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "try:\n",
    "    nvidia_smi = subprocess.check_output('nvidia-smi', shell=True).decode()\n",
    "    print(\"nvidia-smi output:\\n\", nvidia_smi)\n",
    "except Exception as e:\n",
    "    print(f\"nvidia-smi error: {str(e)}\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "JwtXHtB-vw1G"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be used when defining the model \n",
    "# model.to(device)\n",
    "# tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9s5wm77EW0w",
    "outputId": "9d65b29a-71ca-4196-c7d6-3a964128b8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\shash\\anaconda3\\lib\\site-packages (2.7.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: kaggle in c:\\users\\shash\\anaconda3\\lib\\site-packages (1.7.4.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (3.7)\n",
      "Requirement already satisfied: protobuf in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (4.25.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (75.1.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (4.66.5)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (2.2.3)\n",
      "Requirement already satisfied: webencodings in c:\\users\\shash\\anaconda3\\lib\\site-packages (from kaggle) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\shash\\anaconda3\\lib\\site-packages (from bleach->kaggle) (24.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shash\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in c:\\users\\shash\\anaconda3\\lib\\site-packages (0.22.0+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.7.0+cu118 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torchvision) (2.7.0+cu118)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shash\\anaconda3\\lib\\site-packages (from torch==2.7.0+cu118->torchvision) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.0+cu118->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from jinja2->torch==2.7.0+cu118->torchvision) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-image in c:\\users\\shash\\anaconda3\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.11.4 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (1.13.1)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (3.3)\n",
      "Requirement already satisfied: pillow>=10.1 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (10.4.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\shash\\anaconda3\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\shash\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install kaggle\n",
    "%pip install torchvision\n",
    "%pip install scikit-image\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Z_xvYy58KD0"
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1L0oDgbb_27",
    "outputId": "991d6e84-9b34-49fa-d1ce-429718b13aca"
   },
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d requiemonk/sentinel12-image-pairs-segregated-by-terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "FeK8RHgUcEtp"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !unzip sentinel12-image-pairs-segregated-by-terrain.zip\n",
    "# !rm -rf sentinel12-image-pairs-segregated-by-terrain.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1IETAeEcZPJ",
    "outputId": "8b758b94-2645-4c49-a5c6-bb0e7d14701c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 optical images, 16000 SAR images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "opt = []\n",
    "sar = []\n",
    "root_dir = '../v_2'\n",
    "\n",
    "# Verify root directory exists\n",
    "if not os.path.exists(root_dir):\n",
    "    raise FileNotFoundError(f\"Root directory {root_dir} not found!\")\n",
    "\n",
    "for category in os.listdir(root_dir):\n",
    "    # Skip hidden files/folders\n",
    "    if category.startswith('.') or category == '.ipynb_checkpoints':\n",
    "        continue\n",
    "        \n",
    "    category_path = os.path.join(root_dir, category)\n",
    "    \n",
    "    # Skip non-directories\n",
    "    if not os.path.isdir(category_path):\n",
    "        continue\n",
    "        \n",
    "    # Find s1 and s2 subdirectories\n",
    "    subdirs = [d for d in os.listdir(category_path) \n",
    "              if os.path.isdir(os.path.join(category_path, d))]\n",
    "    \n",
    "    # Process each subdirectory\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(category_path, subdir)\n",
    "        \n",
    "        # Collect SAR images\n",
    "        if subdir == 's1':\n",
    "            sar.extend([\n",
    "                os.path.join(subdir_path, f) \n",
    "                for f in os.listdir(subdir_path) \n",
    "                if f.lower().endswith('.png')\n",
    "            ])\n",
    "            \n",
    "        # Collect Optical images\n",
    "        elif subdir == 's2':\n",
    "            opt.extend([\n",
    "                os.path.join(subdir_path, f) \n",
    "                for f in os.listdir(subdir_path) \n",
    "                if f.lower().endswith('.png')\n",
    "            ])\n",
    "\n",
    "# Final sorting and validation\n",
    "opt = sorted(opt)\n",
    "sar = sorted(sar)\n",
    "print(f\"Found {len(opt)} optical images, {len(sar)} SAR images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample SAR paths:\n",
      "['../v_2\\\\agri\\\\s1\\\\ROIs1868_summer_s1_59_p10.png', '../v_2\\\\agri\\\\s1\\\\ROIs1868_summer_s1_59_p100.png']\n",
      "\n",
      "Sample Optical paths:\n",
      "['../v_2\\\\agri\\\\s2\\\\ROIs1868_summer_s2_59_p10.png', '../v_2\\\\agri\\\\s2\\\\ROIs1868_summer_s2_59_p100.png']\n",
      "\n",
      "Missing files: 0\n"
     ]
    }
   ],
   "source": [
    "# Check first few paths\n",
    "print(\"Sample SAR paths:\")\n",
    "print(sar[:2])\n",
    "print(\"\\nSample Optical paths:\")\n",
    "print(opt[:2])\n",
    "\n",
    "# Verify all paths exist\n",
    "all_paths = sar + opt\n",
    "missing = [p for p in all_paths if not os.path.exists(p)]\n",
    "print(f\"\\nMissing files: {len(missing)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBMA-tmHswmS"
   },
   "source": [
    "# **Implementing Colorization Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "zquuhq-aCtvp"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL8MLdko5vp0"
   },
   "source": [
    "## Preparing dataset for colorization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "K_cdF36mCylJ"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def rgb_to_lab_cv2(pil_img):\n",
    "    # Fix 1: Correct variable names and conversion flags\n",
    "    img_rgb = np.array(pil_img.convert(\"RGB\")).astype(\"float32\") / 255.0\n",
    "    img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)  # Fixed conversion flag\n",
    "    img_lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2Lab)   # Fixed conversion flag\n",
    "    \n",
    "    # Fix 2: Proper tensor conversion\n",
    "    img_lab = torch.from_numpy(img_lab.transpose(2, 0, 1)).float()\n",
    "    \n",
    "    # Fix 3: Correct channel indexing\n",
    "    L = (img_lab[0:1, ...] / 50.0) - 1.0    # [0,100] -> [-1,1]\n",
    "    ab = (img_lab[1:3, ...] - 128.0) / 128.0  # Normalize ab channels\n",
    "    \n",
    "    return L, ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: torch.Size([1, 256, 256]) ab: torch.Size([2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(opt[0]).resize((256, 256))\n",
    "L, ab = rgb_to_lab_cv2(img)\n",
    "print(\"L:\", L.shape, \"ab:\", ab.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "zNPdY1ONCy7u"
   },
   "outputs": [],
   "source": [
    "def create_patches(img_tensor, patch_size=224):\n",
    "    \"\"\"Split tensor into patches (img_tensor: [C, H, W])\"\"\"\n",
    "    patches = []\n",
    "    c, h, w = img_tensor.shape\n",
    "    \n",
    "    for i in range(0, h, patch_size):\n",
    "        for j in range(0, w, patch_size):\n",
    "            if i + patch_size <= h and j + patch_size <= w:\n",
    "                patch = img_tensor[:, i:i+patch_size, j:j+patch_size]\n",
    "                patches.append(patch)\n",
    "    return torch.stack(patches) if patches else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "vVScq3AmC2d_"
   },
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, color_paths, transform=None, img_size=224):\n",
    "        self.color_paths = color_paths\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.color_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Fix 1: Variable name correction\n",
    "        pil_img = Image.open(self.color_paths[idx]).convert(\"RGB\")\n",
    "        \n",
    "        # Apply transformations before conversion to Lab\n",
    "        if self.transform:\n",
    "            pil_img = self.transform(pil_img)\n",
    "        else:\n",
    "            pil_img = pil_img.resize((self.img_size, self.img_size))\n",
    "            \n",
    "        L, ab = rgb_to_lab_cv2(pil_img)\n",
    "        return L, ab\n",
    "\n",
    "# Add this above dataset creation\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "arn55ktfC4Nh"
   },
   "outputs": [],
   "source": [
    "dataset = ColorizationDataset(opt[:1000])\n",
    "\n",
    "# Split dataset into training and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "val_size = int(0.2 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Data loaders\n",
    "# Update your data loaders:\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, \n",
    "                          num_workers=0, pin_memory=True)  # num_workers=0\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False,\n",
    "                        num_workers=0, pin_memory=True)     # num_workers=0\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,\n",
    "                         num_workers=0, drop_last=True)      # num_workers=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FW4ZPmyIMPk_",
    "outputId": "b6e6485b-8376-4b2e-fea1-dd8e8cae6c8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_patches shape: torch.Size([16, 1, 224, 224])\n",
      "ab_patches shape: torch.Size([16, 2, 224, 224])\n",
      "tensor([[[ 0.2850,  0.1689,  0.1709,  ...,  0.8491,  0.7821,  0.6760],\n",
      "         [ 0.3840,  0.4141,  0.4686,  ...,  0.9592,  0.9485,  0.8821],\n",
      "         [ 0.4254,  0.2477,  0.2405,  ...,  0.7600,  0.8180,  0.8152],\n",
      "         ...,\n",
      "         [-0.8616, -0.7687, -0.7786,  ...,  0.7795,  0.6974,  0.5947],\n",
      "         [-0.8596, -0.8247, -0.8203,  ...,  0.5887,  0.7018,  0.7974],\n",
      "         [-0.7771, -0.7858, -0.7433,  ..., -0.0098,  0.0870,  0.2286]]])\n"
     ]
    }
   ],
   "source": [
    "# Load a batch and print the shapes of the patches\n",
    "for L_patches, ab_patches in train_loader:\n",
    "    print(f\"L_patches shape: {L_patches.shape}\")\n",
    "    print(f\"ab_patches shape: {ab_patches.shape}\")\n",
    "    print(L_patches[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgE8GU69HfOB"
   },
   "source": [
    "## Implementing the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "uTQjKo4vK2ME"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights, DenseNet121_Weights\n",
    "\n",
    "class EnsembleEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnsembleEncoder, self).__init__()\n",
    "\n",
    "        # Load pre-trained ResNet50 and DenseNet121\n",
    "        self.resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.densenet121 = models.densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
    "\n",
    "        self.resnet50 = nn.Sequential(*list(self.resnet50.children())[:-2])\n",
    "        # self.densenet121 = nn.Sequential(*list(self.densenet121.children())[:-1])\n",
    "        self.densenet121.classifier = nn.Identity()\n",
    "\n",
    "\n",
    "        # Custom layers for fusion\n",
    "        self.conv1x1_resnet50 = nn.ModuleList([\n",
    "            nn.Conv2d(256, 128, kernel_size=1),\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.Conv2d(1024, 512, kernel_size=1),\n",
    "            nn.Conv2d(2048, 1024, kernel_size=1)\n",
    "        ])\n",
    "\n",
    "        self.conv1x1_densenet121 = nn.ModuleList([\n",
    "            nn.Conv2d(256, 128, kernel_size=1),\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.Conv2d(1024, 512, kernel_size=1),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "        ])\n",
    "\n",
    "        self.fusion_blocks = nn.ModuleList([\n",
    "            self.fusion_block(128, 128),\n",
    "            self.fusion_block(256, 256),\n",
    "            self.fusion_block(512, 512),\n",
    "            self.fusion_block(1024, 1024)\n",
    "        ])\n",
    "\n",
    "    # Fusion block\n",
    "    def fusion_block(self, in_channels_resnet, in_channels_densenet):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels_resnet + in_channels_densenet, in_channels_resnet, kernel_size=1),\n",
    "            nn.BatchNorm2d(in_channels_resnet),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through ResNet50\n",
    "        resnet_features = []\n",
    "        resnet_input = x\n",
    "        for i, layer in enumerate(self.resnet50.children()):\n",
    "            resnet_input = layer(resnet_input)\n",
    "            if i in [4, 5, 6, 7]:  # Extract features after specific layers\n",
    "                resnet_features.append(self.conv1x1_resnet50[i-4](resnet_input))\n",
    "\n",
    "        # Forward pass through DenseNet121\n",
    "        densenet_features = []\n",
    "        idx = 0\n",
    "        densenet_input = x\n",
    "        for i, layer in enumerate(self.densenet121.features):\n",
    "            densenet_input = layer(densenet_input)\n",
    "            if i in [ 4, 6, 8, 11]:\n",
    "                densenet_features.append(self.conv1x1_densenet121[idx](densenet_input))\n",
    "                idx += 1\n",
    "\n",
    "\n",
    "        fused_features = []\n",
    "        for i in range(4):\n",
    "            fused = torch.cat((resnet_features[i], densenet_features[i]), dim=1)\n",
    "            fused = self.fusion_blocks[i](fused)\n",
    "            fused_features.append(fused)\n",
    "\n",
    "        return fused_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Test the fixed encoder\n",
    "# encoder = EnsembleEncoder()\n",
    "# dummy_input = torch.randn(1, 1, 224, 224)\n",
    "\n",
    "# # Should output 3 feature maps with proper shapes\n",
    "# features = encoder(dummy_input)\n",
    "# print(\"\\nFinal Feature Shapes:\")\n",
    "# for i, f in enumerate(features):\n",
    "#     print(f\"Level {i}: {f.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZAa-Gw6Jzfx"
   },
   "source": [
    "## Implementing the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "mZJRvn2VHnrj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Decoder block 1: Takes input from Fusion Block 4\n",
    "        self.decode1 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 7x7 -> 14x14\n",
    "        )\n",
    "\n",
    "        # Decoder block 2: Takes input from Decoder Block 1 + Fusion Block 3 (512 + 512 channels)\n",
    "        self.decode2 = nn.Sequential(\n",
    "            nn.Conv2d(512 + 512, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 14x14 -> 28x28\n",
    "        )\n",
    "\n",
    "        # Decoder block 3: Takes input from Decoder Block 2 + Fusion Block 2 (256 + 256 channels)\n",
    "        self.decode3 = nn.Sequential(\n",
    "            nn.Conv2d(256 + 256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 28x28 -> 56x56\n",
    "        )\n",
    "\n",
    "        # Decoder block 4: Takes input from Decoder Block 3 + Fusion Block 1 (128 + 128 channels)\n",
    "        self.decode4 = nn.Sequential(\n",
    "            nn.Conv2d(128 + 128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 56x56 -> 112x112\n",
    "        )\n",
    "\n",
    "        # Final decoder block: Reduce to 2 channels (ab channels)\n",
    "        self.decode5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 2, kernel_size=3, stride=1, padding=1),            \n",
    "            nn.BatchNorm2d(2),            \n",
    "            nn.Tanh(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 112x112 -> 224x224\n",
    "        )\n",
    "\n",
    "    def forward(self, features_7x7, features_14x14, features_28x28, features_56x56):\n",
    "        x = self.decode1(features_7x7)\n",
    "        x = torch.cat([x, features_14x14], dim=1)\n",
    "        x = self.decode2(x)\n",
    "\n",
    "        x = torch.cat([x, features_28x28], dim=1)\n",
    "        x = self.decode3(x)\n",
    "\n",
    "        x = torch.cat([x, features_56x56], dim=1)\n",
    "        x = self.decode4(x)\n",
    "\n",
    "        output = self.decode5(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNHSbR3Q6egy"
   },
   "source": [
    "## Checking our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VW-Tg5UhrVvM",
    "outputId": "a2ebc8fb-790c-4c33-e106-3c523ee72f5f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 2, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ColorizationModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ColorizationModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        features_56x56, features_28x28, features_14x14, features_7x7 = self.encoder(x)\n",
    "\n",
    "        output = self.decoder(features_7x7, features_14x14, features_28x28, features_56x56)\n",
    "\n",
    "        return output\n",
    "\n",
    "encoder = EnsembleEncoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "\n",
    "model = ColorizationModel(encoder, decoder)\n",
    "\n",
    "# input data\n",
    "L_patches = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "output = model(L_patches)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # output shape should be [1, 2, 224, 224]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9glT9b9vRFIC"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tgmjEBEzs0aG",
    "outputId": "bd1100df-240b-47d6-9049-5b37cce38c01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 (Training): 100%|██████████████████████████████████████████████| 40/40 [00:20<00:00,  1.98it/s, loss=0.8593]\n",
      "Epoch 1/20 (Validation): 100%|████████████████████████████████████████████| 10/10 [00:04<00:00,  2.15it/s, loss=0.9242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 0.8593, Validation Loss: 0.9242\n",
      "Model saved with validation loss: 0.9242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 (Training): 100%|██████████████████████████████████████████████| 40/40 [00:19<00:00,  2.01it/s, loss=0.7602]\n",
      "Epoch 2/20 (Validation): 100%|████████████████████████████████████████████| 10/10 [00:04<00:00,  2.04it/s, loss=0.7754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Training Loss: 0.7602, Validation Loss: 0.7754\n",
      "Model saved with validation loss: 0.7754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 (Training): 100%|██████████████████████████████████████████████| 40/40 [00:20<00:00,  1.94it/s, loss=0.6969]\n",
      "Epoch 3/20 (Validation): 100%|████████████████████████████████████████████| 10/10 [00:04<00:00,  2.31it/s, loss=0.7475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Training Loss: 0.6969, Validation Loss: 0.7475\n",
      "Model saved with validation loss: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 (Training): 100%|██████████████████████████████████████████████| 40/40 [00:20<00:00,  1.98it/s, loss=0.6398]\n",
      "Epoch 4/20 (Validation): 100%|████████████████████████████████████████████| 10/10 [00:04<00:00,  2.02it/s, loss=0.5258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Training Loss: 0.6398, Validation Loss: 0.5258\n",
      "Model saved with validation loss: 0.5258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 (Training): 100%|██████████████████████████████████████████████| 40/40 [00:20<00:00,  1.92it/s, loss=0.5912]\n",
      "Epoch 5/20 (Validation): 100%|████████████████████████████████████████████| 10/10 [00:05<00:00,  1.98it/s, loss=0.5705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Training Loss: 0.5912, Validation Loss: 0.5705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 (Training): 100%|██████████████████████████████████████████████| 40/40 [00:20<00:00,  1.94it/s, loss=0.5396]\n",
      "Epoch 6/20 (Validation): 100%|████████████████████████████████████████████| 10/10 [00:04<00:00,  2.01it/s, loss=0.5202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Training Loss: 0.5396, Validation Loss: 0.5202\n",
      "Model saved with validation loss: 0.5202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 (Training): 100%|██████████████████████████████████████████████| 40/40 [00:21<00:00,  1.90it/s, loss=0.4989]\n",
      "Epoch 7/20 (Validation): 100%|████████████████████████████████████████████| 10/10 [00:05<00:00,  2.00it/s, loss=0.4004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Training Loss: 0.4989, Validation Loss: 0.4004\n",
      "Model saved with validation loss: 0.4004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 (Training): 100%|██████████████████████████████████████████████| 40/40 [00:21<00:00,  1.90it/s, loss=0.4564]\n",
      "Epoch 8/20 (Validation): 100%|████████████████████████████████████████████| 10/10 [00:04<00:00,  2.02it/s, loss=0.3989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Training Loss: 0.4564, Validation Loss: 0.3989\n",
      "Model saved with validation loss: 0.3989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 (Training): 100%|██████████████████████████████████████████████| 40/40 [00:20<00:00,  1.93it/s, loss=0.4328]\n",
      "Epoch 9/20 (Validation): 100%|████████████████████████████████████████████| 10/10 [00:05<00:00,  1.96it/s, loss=0.3395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Training Loss: 0.4328, Validation Loss: 0.3395\n",
      "Model saved with validation loss: 0.3395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 (Training): 100%|█████████████████████████████████████████████| 40/40 [00:21<00:00,  1.90it/s, loss=0.3964]\n",
      "Epoch 10/20 (Validation): 100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.96it/s, loss=0.3355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Training Loss: 0.3964, Validation Loss: 0.3355\n",
      "Model saved with validation loss: 0.3355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 (Training): 100%|█████████████████████████████████████████████| 40/40 [00:20<00:00,  1.97it/s, loss=0.3753]\n",
      "Epoch 11/20 (Validation): 100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.96it/s, loss=0.3331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Training Loss: 0.3753, Validation Loss: 0.3331\n",
      "Model saved with validation loss: 0.3331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 (Training): 100%|█████████████████████████████████████████████| 40/40 [00:20<00:00,  1.91it/s, loss=0.3578]\n",
      "Epoch 12/20 (Validation): 100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.98it/s, loss=0.5091]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Training Loss: 0.3578, Validation Loss: 0.5091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 (Training): 100%|█████████████████████████████████████████████| 40/40 [00:20<00:00,  1.92it/s, loss=0.3308]\n",
      "Epoch 13/20 (Validation): 100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.06it/s, loss=0.2376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Training Loss: 0.3308, Validation Loss: 0.2376\n",
      "Model saved with validation loss: 0.2376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 (Training): 100%|█████████████████████████████████████████████| 40/40 [00:20<00:00,  1.92it/s, loss=0.3139]\n",
      "Epoch 14/20 (Validation): 100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.98it/s, loss=0.2680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Training Loss: 0.3139, Validation Loss: 0.2680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 (Training): 100%|█████████████████████████████████████████████| 40/40 [00:20<00:00,  1.91it/s, loss=0.2881]\n",
      "Epoch 15/20 (Validation): 100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.96it/s, loss=0.2029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Training Loss: 0.2881, Validation Loss: 0.2029\n",
      "Model saved with validation loss: 0.2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 (Training): 100%|█████████████████████████████████████████████| 40/40 [00:20<00:00,  1.94it/s, loss=0.3316]\n",
      "Epoch 16/20 (Validation): 100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.98it/s, loss=0.2753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Training Loss: 0.3316, Validation Loss: 0.2753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 (Training): 100%|█████████████████████████████████████████████| 40/40 [00:21<00:00,  1.90it/s, loss=0.2919]\n",
      "Epoch 17/20 (Validation): 100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.99it/s, loss=0.2165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Training Loss: 0.2919, Validation Loss: 0.2165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 (Training): 100%|█████████████████████████████████████████████| 40/40 [00:20<00:00,  1.94it/s, loss=0.2564]\n",
      "Epoch 18/20 (Validation): 100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.98it/s, loss=0.2039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Training Loss: 0.2564, Validation Loss: 0.2039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 (Training):  32%|██████████████▋                              | 13/40 [00:06<00:14,  1.93it/s, loss=0.2425]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize encoder and decoder\n",
    "encoder = EnsembleEncoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "\n",
    "# Freeze the encoder parameters as they are pre-trained\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    encoder.eval()\n",
    "    decoder.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Training)\")\n",
    "    for i, (L_batch, ab_batch) in enumerate(train_bar):\n",
    "        L, ab = L_batch.to(device), ab_batch.to(device)\n",
    "        L = L.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        features_56x56, features_28x28, features_14x14, features_7x7 = encoder(L)\n",
    "        output = decoder(features_7x7, features_14x14, features_28x28, features_56x56)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, ab)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        train_bar.set_postfix(loss=f\"{running_loss/(i+1):.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    decoder.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Validation)\")\n",
    "    with torch.no_grad():\n",
    "        for i, (L_batch, ab_batch) in enumerate(val_bar):\n",
    "            L, ab = L_batch.to(device), ab_batch.to(device)\n",
    "            L = L.repeat(1, 3, 1, 1)  \n",
    "\n",
    "            # Forward pass\n",
    "            features_56x56, features_28x28, features_14x14, features_7x7 = encoder(L)\n",
    "            output = decoder(features_7x7, features_14x14, features_28x28, features_56x56)\n",
    "\n",
    "            # Compute validation loss\n",
    "            loss = criterion(output, ab)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_bar.set_postfix(loss=f\"{val_loss/(i+1):.4f}\")\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(decoder.state_dict(), 'model_1.pth')\n",
    "        print(f\"Model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuS-OiImQhNh"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XH7J21ViEW00"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2x0ynNOEW00",
    "outputId": "c8cfa0aa-85a8-44bf-ed44-ab84548456f0"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder().to(device)\n",
    "decoder.load_state_dict(torch.load('model_1.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHdW9qXQ7R6Z"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6SynNupcP_h"
   },
   "outputs": [],
   "source": [
    "# Get a batch from the test loader\n",
    "dataiter = iter(test_loader)\n",
    "L_batch, ab_batch = next(dataiter)\n",
    "L_batch, ab_batch = next(dataiter)\n",
    "L_batch, ab_batch = L_batch.to(device), ab_batch.to(device)\n",
    "L_batch = L_batch.repeat(1, 3, 1, 1)\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    features_56x56, features_28x28, features_14x14, features_7x7 = encoder(L_batch)\n",
    "\n",
    "    predicted_ab = decoder(features_7x7, features_14x14, features_28x28, features_56x56)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYBqgJftcUTx"
   },
   "outputs": [],
   "source": [
    "L_batch = L_batch[:, 0, :, :]\n",
    "L_batch = L_batch.unsqueeze(1)\n",
    "\n",
    "L_batch = (L_batch + 1) * 0.5 * 100\n",
    "predicted_ab = ((predicted_ab + 1) * 0.5 * (127 + 128)) - 128\n",
    "ab_batch = ((ab_batch + 1) * 0.5 * (127 + 128)) - 128\n",
    "\n",
    "# Combine L and ab channels\n",
    "predicted_lab = torch.cat([L_batch, predicted_ab], dim=1)\n",
    "real_lab = torch.cat([L_batch, ab_batch], dim=1)\n",
    "\n",
    "\n",
    "predicted_lab = predicted_lab.cpu().numpy()\n",
    "real_lab = real_lab.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.color import lab2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Iterate over the batch\n",
    "for i in range(8):\n",
    "\n",
    "    lab_image = predicted_lab[i]\n",
    "    real_img = real_lab[i]\n",
    "\n",
    "    # Transpose to (height, width, 3) for skimage\n",
    "    lab_image = lab_image.transpose(1, 2, 0).astype(np.float64)\n",
    "    real_img = real_img.transpose(1, 2, 0).astype(np.float64)\n",
    "\n",
    "    rgb_image = lab2rgb(lab_image)\n",
    "    real_rgb = lab2rgb(real_img)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(real_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title('Real Color Image')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(rgb_image)\n",
    "    plt.title('Predicted Color Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_rgb = to_rgb_safe(L, ab_real)\n",
    "# plt.imshow(real_rgb)\n",
    "# plt.title(\"Ground Truth Check\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual dummy LAB: mid-gray with blue tint\n",
    "# lab = np.zeros((224, 224, 3), dtype=np.float32)\n",
    "# lab[:, :, 0] = 50         # L: mid-brightness\n",
    "# lab[:, :, 1] = 0          # a: neutral\n",
    "# lab[:, :, 2] = -50        # b: blue tint\n",
    "\n",
    "# rgb = lab2rgb(lab)\n",
    "\n",
    "# plt.imshow(rgb)\n",
    "# plt.title(\"Manual LAB to RGB Test\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2gHPLZMRFDKd",
    "outputId": "30e5281b-8dfb-42dd-b3bc-8842be941246"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from skimage.color import lab2rgb\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "\n",
    "# def to_rgb_safe(L_tensor, ab_tensor):\n",
    "#     # Remove batch dim\n",
    "#     L = L_tensor.squeeze(0).cpu().numpy()\n",
    "#     ab = ab_tensor.squeeze(0).cpu().numpy()\n",
    "\n",
    "#     # Denormalize properly\n",
    "#     L = (L + 1) * 50                # [0, 100]\n",
    "#     ab = ab * 127.5                 # [-128, 127]\n",
    "\n",
    "#     # Stack to (H, W, 3)\n",
    "#     lab = np.zeros((224, 224, 3), dtype=np.float32)\n",
    "#     lab[:, :, 0] = L[0]            # L channel\n",
    "#     lab[:, :, 1] = ab[0]           # a channel\n",
    "#     lab[:, :, 2] = ab[1]           # b channel\n",
    "\n",
    "#     # LAB to RGB\n",
    "#     rgb = lab2rgb(lab)\n",
    "#     return rgb\n",
    "\n",
    "# # Choose a sample\n",
    "# i = 0\n",
    "# L = L_batch[i].unsqueeze(0)\n",
    "# ab_real = ab_batch[i].unsqueeze(0)\n",
    "# ab_pred = predicted_ab[i].unsqueeze(0)\n",
    "\n",
    "# # Convert\n",
    "# real_rgb = to_rgb_safe(L, ab_real)\n",
    "# pred_rgb = to_rgb_safe(L, ab_pred)\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(real_rgb)\n",
    "# plt.title(\"Real Color Image\")\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(pred_rgb)\n",
    "# plt.title(\"Predicted Color Image\")\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_blYq4v7ohb"
   },
   "source": [
    "## Evaulting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_XlyLCqjYNj",
    "outputId": "fd280005-9ae4-4401-a0ad-d38be8296b31"
   },
   "outputs": [],
   "source": [
    "def prediction(model, test_loader):\n",
    "    encoder.eval()\n",
    "    model.eval()\n",
    "    original_images = []\n",
    "    predicted_images = []\n",
    "\n",
    "    for L_batch, ab_batch in tqdm(test_loader):\n",
    "        L_batch, ab_batch = L_batch.to(device), ab_batch.to(device)\n",
    "        input = L_batch.repeat(1, 3, 1, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features_56x56, features_28x28, features_14x14, features_7x7 = encoder(input)\n",
    "\n",
    "            predicted_ab = model(features_7x7, features_14x14, features_28x28, features_56x56)            \n",
    "\n",
    "        L_batch = (L_batch + 1) * 0.5 * 100        \n",
    "        predicted_ab = ((predicted_ab + 1) * 0.5 * (127 + 128)) - 128\n",
    "        ab_batch = ((ab_batch + 1) * 0.5 * (127 + 128)) - 128\n",
    "\n",
    "        # Combine L and ab channels\n",
    "        predicted_lab = torch.cat([L_batch, predicted_ab], dim=1)\n",
    "        actual_lab = torch.cat([L_batch, ab_batch], dim=1)\n",
    "\n",
    "        predicted_lab = predicted_lab.cpu().numpy()\n",
    "        actual_lab = actual_lab.cpu().numpy()\n",
    "\n",
    "        predicted_images.extend(predicted_lab)\n",
    "        original_images.extend(actual_lab)\n",
    "\n",
    "    return original_images, predicted_images\n",
    "\n",
    "original_images, predicted_images = prediction(decoder, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4wxztv2mDFI",
    "outputId": "1f6739ac-2930-4315-93c3-44779a57443d"
   },
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.color import lab2rgb\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(original_images, predicted_images):\n",
    "\n",
    "    total_ssim = 0.0\n",
    "    total_psnr = 0.0\n",
    "    total_samples = 0\n",
    "    for original_img, predicted_img in zip(original_images, predicted_images):\n",
    "        original_img = lab2rgb(original_img.transpose(1, 2, 0))\n",
    "        predicted_img = lab2rgb(predicted_img.transpose(1, 2, 0))\n",
    "\n",
    "        ssim_value = ssim(original_img, predicted_img, multichannel=True, channel_axis=2, data_range=1.0)\n",
    "        psnr_value = psnr(original_img, predicted_img, data_range=1.0)\n",
    "        total_ssim += ssim_value\n",
    "        total_psnr += psnr_value\n",
    "        total_samples += 1\n",
    "\n",
    "    average_ssim = total_ssim / total_samples\n",
    "    average_psnr = total_psnr / total_samples\n",
    "\n",
    "    return average_ssim, average_psnr\n",
    "\n",
    "ssim_value, psnr_value = evaluate_model(original_images, predicted_images)\n",
    "print(f\"Average SSIM: {ssim_value:.4f}\")\n",
    "print(f\"Average PSNR: {psnr_value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCnnUQv2EW00"
   },
   "source": [
    "# Results\n",
    "\n",
    "We are getting quite good results but after observing carefully we saw that the model is learning green color more than the other colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
